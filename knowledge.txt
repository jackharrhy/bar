Abstract—A denotational semantics of quantum Turing machines is defined in the strongly compact closed category of finite
dimensional Hilbert spaces. Using the Moore-Penrose generalized
inverse, a new additive trace is introduced on the restriction of
this category to isometries, which trace is carried over to directed
quantum Turing machines as monoidal automata. The resulting
traced monoidal category is further transformed into the indexed
monoidal algebra of undirected quantum Turing automata.
I. INTRODUCTION
In recent years, following the endeavors of Abramsky and
Coecke to express some of the basic quantum-mechanical
concepts in an abstract axiomatic category theory setting,
several models have been worked out to capture the semantics of quantum information protocols [1] and programming
languages [11], [15], [23]. Concerning quantum hardware, an
algebra of automata which include both classical and quantum
entities has been studied in [12].
The objective of the present paper is to provide a denotational style semantics for quantum Turing machines as
hardware devices. At the same time, the rigid topological
layout of Turing machines as a linear array of tape cells
is replaced by a flexible graph structure, giving rise to the
concept of Turing automata and graph machines as introduced
in [6]. By denotational semantics we mean that the changing
of the tape contents caused by the entire computation process
is specified directly as a linear operator, rather than just one
step of this process.
Our presentation will use the language of [1], [16], but
it will be specific to the concrete strongly compact closed
category (FdHilb, ⊗) of finite dimensional Hilbert spaces
at this time. One can actually read Section 4 separately as
an interesting study in linear algebra, introducing a novel
application of the Moore-Penrose generalized inverse of rangeHermitian operators by taking their Schur complement in
certain block matrix operators. This is the main technical contribution of the paper. We believe, however, that the category
theory contributions are much more interesting and relevant.
All of these results are around the well-known Geometry
of Interaction (GoI) concept introduced originally by Girard
[13] in the late 1980’s as an interpretation of linear logic.
The ideas, however, originate from and are directly related to
a yet earlier work [2] by the author on the axiomatization
of flowchart schemes, where the traced monoidal category
axioms first appeared in an algebraic context. Our category
theory contributions are as follows.
(i) We introduce a total trace on the monoidal subcategory
of (FdHilb, ⊕) defined by isometries, which has previously
been sought by others [14], [21].
(ii) We explain the role of the Int construction for traced
monoidal categories [16] in turning a computation process
bidirectional or reversible.
(iii) We capture the phenomenon in (ii) above by our
own concept “indexed monoidal algebra” [7], an equivalent
formalism for compact closed categories.
Due to space limitations we have to assume familiarity with
some advanced concepts in category theory, namely traced
monoidal categories [16], compact closed categories [18], and
the Int construction that links these two types of symmetric
monoidal categories [20] to each other. For brevity, by a
monoidal category we shall mean a symmetric monoidal one
throughout the paper.
II. MONOIDAL CATEGORIES AND INDEXED MONOIDAL
ALGEBRAS
The following definition of (strict) traced monoidal categories uses the terminology of [16]. Trace (called feedback in
[2]) in a monoidal category C with unit object I, tensor ⊗,
and symmetries cA,B : A ⊗ B → B ⊗ A is introduced as a
left trace, i.e., an operation C(U ⊗ A, U ⊗ B) → C(A, B).
Definition 2.1. A trace for a monoidal category C is a natural
family of functions
T rU
A,B : C(U ⊗ A, U ⊗ B) → C(A, B)
satisfying the following three axioms:
vanishing:
T rI
A,B(f) = f , T rU⊗V
A,B (g) = T rV
A,B(T rU
V ⊗A,V ⊗B(g));
superposing:
T rU
A,B(f) ⊗ g = T rU
A⊗C,B⊗D(f ⊗ g), where g : C → D;
yanking:
T rU
U,U (cU,U ) = 1U .
Naturality of trace is meant in all three variables A, B, U.
The word sliding is used as a synonym for (di-)naturality in
U. When using the term feedback for trace, the notation T r
changes to ↑ or ⇑, and we simply write T rU (↑
U , ⇑
U ) for
T rU
A,B whenever A and B are understood from the context.
One further axiom will be of interest for us in Section 4.
derived composition:
f ◦ g = T rB
A,C(cB,A ◦ (f ⊗ g)) for f : A → B, g : B → C.
It is known, cf. [2, Axiom X3], that this identity is a consequence of the traced monoidal category axioms. Moreover,
in the presence of derived composition, it is sufficient to
impose/check naturality with respect to permutations only.
Notice that we write composition of morphisms (◦) in a
left-to-right manner, avoiding the use of “;”, which some
may find more appropriate. Accordingly, when working in
a Hilbert space H, we shall think of a vector v ∈ H as a
“row vector”, that is, a morphism v : C → H. Consequently,
we apply an operator (matrix) T on v as vT , and not T v.
A “column vector” (v ↓) is a morphism H → C, which
is naturally isomorphic to a (row) vector in the dual space
H∗
. We shall use the symbols I and 0 as “generic” identity
(respectively, zero) operators, provided that the underlying
Hilbert space is understood from the context. As a further
technical simplification we shall be working with the strict
monoidal formalism, even though the monoidal category of
Hilbert spaces with the usual tensor product is not strict.
Definition 2.2. A monoidal category C is compact closed
(CC, for short) if every object A has a left adjoint A∗
in the
sense that there exist morphisms dA : I → A ⊗ A∗
(the unit
map) and eA : A∗ ⊗ A → I (the counit map) for which the
two composites below result in the identity morphisms 1A and
1A∗ , respectively.
A = I ⊗ A →dA⊗1A (A ⊗ A
∗
) ⊗ A
= A ⊗ (A
∗ ⊗ A) →1A⊗eA A ⊗ I = A,
A
∗ = A
∗ ⊗ I →1A∗⊗dA A
∗ ⊗ (A ⊗ A
∗
)
= (A
∗ ⊗ A) ⊗ A
∗ →eA⊗1A∗ I ⊗ A
∗ = A
∗
.
Category C is self-dual compact closed (SDCC, for short) if
A = A∗
for each object A. The category SDCC has as objects
all SDCC categories, and as morphisms monoidal functors
preserving the given self-adjunctions. As it is well-known,
every CC category admits a so called canonical trace [16]
defined by the formula
T rU
A,Bf = (dU∗ ⊗ 1A) ◦ (1U∗ ⊗ f) ◦ (eU ⊗ 1B).
See Fig. 1.
U *
U *
A
B
f
U
U
U
U
A
B
f =
e
d
Fig. 1. Canonical trace
On the analogy of enriched categories [19] indexed
monoidal algebras have been introduced recently in [6]. An
indexed monoidal algebra (or half-category) M consists of
objects A, B, . . ., morphisms f, g, . . ., and an operation rank,
which assigns to each morphism f an object A. We write
f : A to indicate the rank of f. There is an associative binary
operation ⊗ (tensor) on objects and a unit object I, defining a
monoid structure M. On morphisms, the following operations
are defined.
– A binary operation, also called tensor, which assigns to each
pair of morphisms f : A and g : B a morphism f ⊗g : A⊗B.
– A unary operation trace, by which every morphism f :
A ⊗ A ⊗ B is assigned a morphism lA,B f : B. We shall
write lA f if B is understood.
– For each object A, an identity morphism 1A : A ⊗ A.
Intuitively, a (“half”-)morphism f : A stands for a real
morphism f : I → A in a corresponding hypothetical SDCC
category C. Tensor in M is essentially ⊗ in C, and lA f
for f : A ⊗ A ⊗ B captures the canonical trace of the
morphism fA : A → A ⊗ B in C that corresponds to f
by compact closure. There is also an indexing mechanism in
M, which employs permutation symbols as a key instrument.
Using Mac Lane’s coherence theorem for monoidal categories
[20], a permutation symbol ρ : A ⇒ B is a free symbolic
representation of a permutation A → B in C, independently
of any concrete C sharing the given object structure M with
M.
Permutation symbols do not form a category over the objects
of M, though. They do form a monoidal category in which the
objects are object terms (words) over M’s objects as object
variables. Composition and tensor in this free category will
be denoted by • and ⊘ in the axioms I1-I9 below. Since
each object term evaluates to an object according to the given
monoid M, every permutation symbol ρ : A ⇒ B has a
unique canonical interpretation as a permutation A → B in
any monoidal category having M as its object structure. In
our algebraic language, each permutation symbol ρ : A ⇒ B
serves as a unary operation in M, which takes a morphism
f : A to a morphism f · ρ : B. Two permutation symbols
ρ, ρ′
: A ⇒ B are said to be equivalent, ρ ≡ ρ
′
, if they denote
the same permutation in every monoidal category having the
object structure M.
Composition (◦) is the following derived operation in M.
– For f : A ⊗ B and g : B ⊗ C,
f ◦ g =lB ((f ⊗ g) · (cA,BB ⊘ 1C )).
See Fig. 2.
f g
A B B
B C B A
C
Fig. 2. Composition as a derived operation
Operations in M are subject to the following nine equational axioms, which postulate that the resulting indexed
monoidal algebra (IMA, for short) be indeed equivalent to
an SDCC category.
I1. Functoriality of indexing
f · (ρ1 • ρ2) = (f · ρ1) · ρ2
for f : A and composable ρ1 : A ⇒ B, ρ2 : B ⇒ C;
f · 1A = f for f : A.
I2. Naturality of indexing
(f ⊗ g) · (ρ1 ⊘ ρ2) = f · ρ1 ⊗ g · ρ2
for f : A, g : B, ρ1 : A ⇒ C, ρ2 : B ⇒ D;
(lA f) · ρ =lA (f · (1AA ⊘ ρ))
for f : A ⊗ A ⊗ B, ρ : B ⇒ C.
I3. Coherence
f · ρ1 = f · ρ2 for f : A, ρi
: A ⇒ B, whenever ρ1 ≡ ρ2.
I4. Associativity and symmetry of tensor
(f ⊗ g) ⊗ h = f ⊗ (g ⊗ h) for f : A, g : B, h : C;
f ⊗ g = (g ⊗ f) · cA,B for f : A, g : B.
I5. Right identity
f ◦ 1B = f and f ⊗ 1I = f for f : A → B.
I6. Symmetry of identity
1A · cA,A = 1A.
I7. Vanishing
lI f = f for f : A;
lA⊗B f =lB (lA f · (1A ⊘ cB,A ⊘ 1BC))
for f : A ⊗ B ⊗ A ⊗ B ⊗ C.
I8. Superposing
lA (f ⊗ g) =lA f ⊗ g for f : A ⊗ A ⊗ B, g : C.
I9. Trace swapping
lB (lA f) =lA (lB (f · (cAA,BB ⊘ 1C)))
for f : A ⊗ A ⊗ B ⊗ B ⊗ C.
Analogously to a functor, an indexed monoidal homomorphism h : M → M′ between IMA’s M and M′
consists
of a pair of functions. The object function assigns to each
object A in M an object hA, so that h preserves the monoid
structure, and the morphism function assigns to each morphism
f : A a morphism hf : hA in such a way that h defines a
homomorphism in the algebraic sense.
Let IMA denote the category of indexed monoidal algebras
with indexed algebraic homomorphisms between them. The
following result was proved in [6].
Theorem 2.1. The categories SDCC and IMA are equivalent.
Given an arbitrary traced monoidal category C, one can
turn it functorially into an IMA Alg(C) by the help of the
Int construction [16]. The trick is to simply restrict the CC
category Int(C) to its self-dual objects (A, A), and then use
Theorem 2.1 to obtain an equivalent IMA. See Theorem 5.1
below for details.
III. MONOIDAL VS TURING AUTOMATA
Circuits and automata over an arbitrary monoidal category
M have been studied in [3], [4], [5], [17]. It was shown that the
collection of such machines has the structure of a monoidal
category equipped with a natural feedback operation, which
satisfies the traced monoidal axioms, except for yanking.
Moreover, sliding holds in a weak sense, for isomorphisms
only.
Let A and B be objects in M. An M-automaton (circuit)
A → B is a pair (U, α), where U is a further object and
α : U ⊗ A → U ⊗ B is a morphism in M. If, for example,
M = (Set, ×), then the pair (U, α) represents a deterministic
Mealy automaton with states U, input A, and output B.
The structure of M-automata/circuits has been described as
a monoidal category Circ(M) with feedback in [17]. This
category was also shown to be freely generated by M.
In this paper we take a different approach to the study
of monoidal automata. We follow the method of [6] with
the aim of constructing a traced monoidal category as an
adequate semantical structure for these automata. One must
not confuse this type of semantics with the meaning normally
associated with the category Circ(M) above, as they have
seemingly very little in common. A traced monoidal category
indicates a delay-free semantics, as opposed to the step-bystep delayed semantics suggested by Circ(M). Arguing at an
intuitive level, the difference is the following. In a delayed
model, the “combinational logic” α : U ⊗ A → U ⊗ B
describes one primitive step of the automaton, and the stepwise
behavior is derived naturally as a kind of operational semantics
in terms of sequences. In contrast, a delay-free model is like
an asynchronous automaton (e.g. a flip-flop constructed from
two NAND gates), which must first stabilize on a given input,
keeping it steady over an indefinite number of steps, before
the next input can even be considered.
Even though the analogy above is quite appropriate, the
category that we are going to construct is not meant to be the
quotient of Circ(M) by the yanking identity, so as to turn it
into a traced monoidal category in the straightforward manner.
Rather, we define a brand new tensor and feedback (trace) on
our M-automata, which are analogous to the basic operations
in iteration theories [10]. Regarding the base category M, we
shall assume an additional, so called additive tensor ⊕, so that
⊗ distributes over ⊕. These two tensors will then be “mixed
and matched” in the definition of tensor (⊠) for M-automata,
providing them with an intrinsic Turing machine behavior.
The “prototype” of this construction, resulting in the indexed monoidal algebra of conventional Turing automata, has
been elaborated in [7] using M = (Rel, ×, +) as the base
category. This category was ideal as a template for the kind
of construction we have in mind, since it is biproduct by
+ and self-dual compact closed according to ×. Below we
present the quantum counterpart of this construction, working
in the biproduct strongly compact closed category [1] of finite
dimensional Hilbert spaces (FdHilb, ⊗, ⊕). More precisely,
the category M above will be the restriction of FdHilb to
isometries as morphisms, which subcategory is no longer
compact closed or biproduct. We shall only use the inner
product feature of FdHilb, completeness of the metric space
induced is irrelevant.
IV. DIRECTED QUANTUM TURING AUTOMATA
In this section we present the construction outlined above, to
obtain a strange asymmetric model which does not yet qualify
as a recognizable quantum computing device in its own right.
The model represents a Turing machine in which cells are
interconnected in a directed way, so that the control (tape
head) always moves along interconnections in the given fixed
direction, should it be left or right. In other words, direction is
incorporated in the scheme-like graphical syntax, rather than
the semantics. We use this model only as a stepping stone
towards our real objective, the (undirected) quantum Turing
automaton described in Section 5.
Definition 4.1. A directed quantum Turing automaton is a
quadruple
T = (H, K,L, τ),
where H, K, and L are finite dimensional Hilbert spaces over
the complex field C, and τ : H ⊗ K → H ⊗ L is an isometry
in FdHilb.
Recall that an isometry between Hilbert spaces H1 and H2
is a linear map σ : H1 → H2 such that σ ◦ σ
† = I, where
σ
†
is the (Hilbert space) adjoint of σ. Following the notation
of general monoidal automata we write T : K → L, and call
the isometry τ the transition operator of T . Thus, T is the
monoidal automaton (H, τ) : K → L. Sometimes we simply
identify T with τ, provided that the other parameters of T are
understood from the context.
a) b)
Fig. 3. Two simple DQTA
The reader can obtain an intuitive understanding of the
automaton T from Fig. 3a. The state space H is represented by
a finite number of qubits, while the control is a moving particle
that moves from one of the input interfaces (space K) to one
of the output ones (space L). It can only move in the input →
output direction, as specified by the operator τ. The number of
input and output interfaces is finite. The control itself does not
carry any information, it is just moving around and changes the
state of T . In comparison with conventional Turing machines,
the state of T is the tape contents of the corresponding Turing
machine, and the current state of the Turing machine is just an
interface identifier for T . For example, one can consider the
DQTA in Fig. 3b as one tape cell of a Turing machine TM
having 2
3
symbols in its tape alphabet and only 2 states (2 leftmoving and 2 right-moving interfaces, both input and output).
Correspondingly, H is 8-dimensional, while the dimension of
both K and L is 4. In motion, if the control particle of T
resides on the input interface labeled (L, i) ((R, i)), then TM
is in state i moving to the left (respectively, right). The point
is, however, that the automaton T need not represent just one
cell, it could stand for any finite segment of a Turing machine,
in fact a Turing graph machine in the sense of [6]. In our
concrete example, a segment of TM with n tape cells would
have 3n qubits inside the circle of Fig. 3b, but still the same
4 + 4 interfaces.
An isometric isomorphism σ : H1 → H2 (unitary map, if
H1 = H2) is a linear operator such that both σ and σ
†
are
isometries. Two automata Ti
: (Hi
, τi) : K → L, i = 1, 2,
are isomorphic, notation T1
∼= T2, if there exists an isometric
isomorphism σ : H1 → H2 for which
τ2 = (σ
† ⊗ IK) ◦ τ1 ◦ (σ ⊗ IL).
For simplicity, though, we shall work with representatives,
rather than equivalence classes of automata.
Turing automata can be composed by the standard cascade
product of monoidal automata, cf. [4], [5], [17]. If T1 =
(H1, τ1) : L → M and T2 = (H2, τ2) : M → N are directed
quantum Turing automata (DQTA, for short), then
T1 ◦ T2 = (H1 ⊗ H2,L, N , τ)
is the automaton whose transition operator τ is
(πH1,H2 ⊗ IL) ◦ (IH2 ⊗ τ1) ◦ (πH2,H1 ⊗ IM) ◦ (IH1 ⊗ τ2),
where πH,K is the symmetry H⊗K → K⊗H in (FdHilb, ⊗).
As known from [17], the cascade product of automata is
compatible with isomorphism, so that it is well-defined on
isomorphism classes of DQTA. The identity Turing automaton
1K : K → K has the unit space C as its state space,
and its transition operator is simply IK. The results in [17]
imply that these data define a category DQT over finite
dimensional Hilbert spaces as objects, in which the morphisms
are isomorphism classes of DQTA.
Now let
T1 = (H1, τ1) : K1 → L1 and T2 = (H2, τ2) : K2 → L2
be DQTA, and define T1 ⊠ T2 to be the automaton over the
state space H1 ⊗ H2 whose transition operator
τ = τ1⊠τ2 : (H1⊗H2)⊗(K1⊕K2) → (H1⊗H2)⊗(L1⊕L2)
acts as follows: τ ≃ σ1 ⊕ σ2, where the morphisms
σi
: (H1 ⊗ H2) ⊗ Ki → (H1 ⊗ H2) ⊗ Li
, i = 1, 2 are:
σ1 = (πH1,H2 ⊗ IK1
) ◦ (IH2 ⊗ τ1) ◦ (πH2,H1 ⊗ IL1
), and
σ2 = IH1 ⊗ τ2.
In the above equations, ⊕ denotes the orthogonal sum of
Hilbert spaces. Intuitively, τ is the selective performance of
either τ1 or τ2 on the tensor space H1 ⊗ H2. The natural
isomorphism ≃ is distributivity in the sense of [1, Proposition 5.3], which is meaningful in all biproduct compact closed
categories. It is clear that the operator τ1 ⊠ τ2 is an isometry,
so that the operation ⊠ is well-defined. We call this operation
the Turing tensor. The Turing tensor is also associative, up to
natural isomorphism, of course.
The symmetries K ⊠ L → L ⊠ K associated with ⊠ are
the “single-state” Turing automata whose transition operator
is the permutation
κK,L =
L K
K
L

0 I
I 0

: (C⊗)(K ⊕ L) → (C⊗)(L ⊕ K).
Along the lines of [17] it is routine to check that ⊠ is also
compatible with isomorphism of automata, and (DQT, ⊠)
becomes a monoidal category in this way.
Our third basic operation on DQTA is feedback. Feedback
follows the scheme of iteration in Conway matrix theories
[10], using an appropriate star operation. Let T : U ⊕ K →
U ⊕ L be a DQTA having
τ : H ⊗ (U ⊕ K) → H ⊗ (U ⊕ L)
as its transition operator. Then ↑
U T : K → L is the automaton
over (the same space) H specified as follows. Consider the
matrix of τ:
H ⊗ U H ⊗ L
H ⊗ U
H ⊗ K 
τA τB
τC τD

according to the biproduct decomposition
τ = h[τA, τC ], [τB, τD]i,
where [ , ] stands for coproduct and h , i for product. The
transition operator of ↑
U T is defined by the Kleene formula:
↑
U
τ = limn→∞
(τD + τC ◦ τ
∗n
A ◦ τB). (1)
In the Kleene formula, τ
∗n
A =
Pn
i=0 τ
i
A, where τ
0
A = I and
τ
i+1
A = τ
i
A ◦τA. In other words, τ
∗n
A is the n-th approximation
of τA’s Neumann series well-known in operator theory. The
correctness of the above definition is contingent upon the
existence of the limit and also on the resulting operator being
an isometry. For these two conditions we need to make a
short digression, which will also clarify the linear algebraic
background.
Let Iso denote the subcategory of FdHilb having only
isometries as its morphisms. Notice that (Iso, ⊗) is no longer
compact closed, even though the multiplicative tensor ⊗ is
still intact in it. (The duals are gone.) This tensor, however,
does not concern us at the moment. Consider ⊕ as an additive
tensor in Iso:
τ1 ⊕ τ2 = h[τ1, 0], [0, τ2]i
for all isometries τi
: Hi → Ki
, i = 1, 2. Clearly, τ1 ⊕τ2 is an
isometry. The new additive unit (zero) object is the zero space
Z. With the additive symmetries κH,K : H ⊕ K → K ⊕ H,
(Iso, ⊕) again qualifies as a monoidal category. The biproduct
property of ⊕ is lost, however. Nevertheless, one may attempt
to define a trace operation ↑
U τ in Iso by the Kleene formula
(1), where τ : U ⊕ K → U ⊕ L. (Cut H⊗ in the matrix of τ.)
Since the Kleene formula does not appear to be manageable,
we first redefine ↑
U τ and prove the equivalence of the two
definitions later. Let
⇑
U
τ = τD + τC ◦ (I − τA)
+ ◦ τB, (2)
where ()+ denotes the Moore-Penrose generalized inverse of
linear operators. Recall e.g. from [8] that the Moore-Penrose
inverse (MP inverse, for short) of an arbitrary operator σ :
H → K is the unique operator σ
+ : K → H satisfying the
following two conditions:
(i) σ ◦ σ
+ ◦ σ = σ, and σ
+ ◦ σ ◦ σ
+ = σ
+;
(ii) σ ◦ σ
+ and σ
+ ◦ σ are Hermitian.
The connection between formulas (1) and (2) is the following. If the Neumann series τ
∗
A converges, then (I − τA) is
invertible and
τ
∗
A = (I − τA)
−1 = (I − τA)
+.
We know that kτAk ≤ 1, where k k denotes the operator
norm. (Remember that τ is an isometry.) Therefore the Kleene
formula needs an explanation only if kτAk = 1. In that case,
even if (I − τA) is invertible, τ
∗
A may not converge.
Just as the Kleene formula in computer science, the expression on the right-hand side of equation (2) is well-known and
frequently used in linear algebra. For a block matrix
M =

A B
C D 
,
where A is square, the matrix D −CA+B is called the Schur
complement of A on M, denoted A/M. See e.g. [8]. Observe
that, under the assumption K = L,
⇑
U
τ = I − (I − τA)/(I − τ).
For this reason we call ⇑
U τ the Schur I-complement of τA
on τ, and write ⇑
U τ = τA\τ.
Theorem 4.1. The operator τA\τ is an isometry.
Proof. Isolate the kernel N of (I − τA), and let U0 be the
orthogonal complement [22] of N on U. The matrix of (I−τA)
in this breakdown is
I − τA =
N U0
N
U0

0 0
−τ
N
A I − τ
0
A

. (3)
Put this matrix in the top left corner of τ:
Since τ is an isometry (regardless of its concrete orthogonal
representation as a matrix operator), all entries in the above
block matrix with superscript N must be 0. Consequently,
(I − τ
0
A) is invertible and τA\τ = τ
0
A\τ0, where
τ0 : U0 ⊕ K → U0 ⊕ L
is the restriction of τ to the bottom right 2×2 corner. Indeed,

0 0
0 I − τ
0
A
+
=

0 0
0 (I − τ
0
A)
−1

,
so that
τC ◦ (I − τA)
+ ◦ τB = τ
0
C ◦ (I − τ
0
A)
−1
◦ τ
0
B.
It turns out from the above discussion that (I − τA) is group
invertible and range-Hermitian, cf. [8], [9]. Therefore the
MP inverse of (I − τA) coincides with its Drazin inverse,
which is the group generalized inverse of this operator. See
again [8], [9]. It follows that we can assume, without loss of
generality, that (I − τA) is invertible. Note that (3) is only
a unitary similarity, therefore the sliding axiom is needed to
make this argument correct. See Theorem 4.4 below. For better
readability, replace the symbols τA, τB, τC , and τD by A, B,
C, and D, respectively. Furthermore, ignore the composition
symbol ◦ as if we were dealing with ordinary matrix product.
Then we have:

A B
C D   A† C
†
B† D†

=

I 0
0 I

.
The following four matrix equations are derived:
AA† + BB† = I, (4)
AC† + BD† = 0, (5)
CA† + DB† = 0, (6)
CC† + DD† = I. (7)
We need to show that
(D + C(I − A)
−1B)(D† + B
†
(I − A
†
)
−1C
†
) = I.
The product on the left-hand side yields:
DD† + DB†
(I − A
†
)
−1C
† + C(I − A)
−1BD†
+ C(I − A)
−1BB†
(I − A
†
)
−1C
†
.
By (5) and (6) this is equal to:
DD† − CA†
(I − A
†
)
−1C
† − C(I − A)
−1AC†
+ C(I − A)
−1BB†
(I − A
†
)
−1C
†
,
which is further equal to DD† + CQC†
, where
Q=(I −A)
−1BB†
(I −A
†
)
−1−A
†
(I −A
†
)
−1−(I −A)
−1A.
According to (7) it is sufficient to prove that Q = I. A couple
of equivalent transformations follow.
1. Multiply both sides of Q = I by (I − A) from the left:
BB†
(I − A
†
)
−1 − (I − A)A
†
(I − A
†
)
−1 − A = I − A,
BB†
(I − A
†
)
−1 − (I − A)A
†
(I − A
†
)
−1 = I.
2. Multiply by (I − A†
) from the right:
BB† − (I − A)A
† = I − A
†
,
BB† + AA† = I.
The result is equation (4), which is given. The proof is now
complete. ✷
Lemma 4.2. Let τ : U ⊕ V ⊕ K → U ⊕ V ⊕ L be an isometry
defined by the matrix


M B1
B2
C1 C2 D

 , where M =

P Q
R S 
.
If I − (P\M) = I − (S + R(I − P)
+Q) is invertible, then
⇑V (⇑U τ) =⇑U⊕V τ.
Proof. Using the kernel-on-top representation of operators as
explained under Theorem 4.1, we can assume (without loss of
generality) that I − P is also invertible. Then the statement
follows from the Banachiewicz block inverse formula [9,
Proposition 2.8.7]:

A B
C D −1
=

A−1+A−1B(D−CA−1B)
−1 −A−1B(D−CA−1B)
−1
−(D − CA−1B)
−1CA−1
(D − CA−1B)
−1

,
using A = I − P, B = −Q, C = −R, and D = I − S.
Computations are left to the reader. ✷
Note that the Banachiewicz formula does not hold true for
the MP or the Drazin inverse of the given block matrix when
A−1
and (D − CA−1B)
−1
are replaced on the right-hand
side by A+ and (D − CA+B)
+, respectively, even if one
of these square matrices is invertible. There are appropriate
block inverse formulas for generalized inverses, cf. [9], but
these formulas are extremely complicated and are of no use
for us.
Lemma 4.3. Let τ : U ⊕ V ⊕ K → U ⊕ V ⊕ L be an isometry
as in Lemma 4.2. If P\M = I, then
⇑V (⇑U τ) =⇑U⊕V τ.
Proof. Again, we can assume that I−P is invertible. To keep
the computation simple, let U and V both be 1-dimensional.
This, too, can in fact be assumed without loss of generality,
if one uses an appropriate induction argument. The induction,
however, can be avoided at the expense of a more advanced
matrix computation. Thus,
τ =


p q u1
r s u2
v1 ↓ v2 ↓ D

 ,
where ui and (vi ↓), i = 1, 2 are row and column vectors,
respectively. To simplify the computation even further, let the
numbers p, q, r, s be real. The 2 × 2 matrix I − M is singular
and range-Hermitian, therefore it is Hermitian (only because
the numbers are real, see [9, Corollary 5.4.4]), so that it must
be of the form
I − M =

a b
b b2/a 
for some real numbers a, b with a = 1 − p 6= 0. Then
⇑U τ =

c u
v ↓ D′

,
where c = (1 − b
2/a) + b
2/a = 1,
u = u2 − (b/a) · u1,
(v ↓) = (v2 ↓) − (b/a) · (v1 ↓), and
D
′ = D + (1/a) · (v1 ↓)u1.
Since c = 1, u and (v ↓) must be 0. Consequently,
a · u2 = b · u1 and a · (v2 ↓) = b · (v1 ↓). (8)
In order to calculate (I − M)
+, let M′ = S(I − M)S
−1
,
where S = S
−1
is the unitary matrix
S =
1
d
·

−b a
a b 
, d2 = a
2 + b
2
.
After a short computation,
M′ =

0 0
0 d
2/a 
.
It follows that:
(I − M)
+ = S

0 0
0 a/d2

S, and
⇑U⊕V τ = D + (v1 ↓, v2 ↓)S

0 0
0 a/d2

S

u1
u2

.
Comparing this expression with
⇑V (⇑U τ) = D′ = D + (1/a) · (v1 ↓)u1,
we need to prove that
(v1 ↓, v2 ↓)S

0 0
0 a/d2

S

u1
u2

=
1
a
· (v1 ↓)u1.
On the left-hand side we have:
a
d
4
· (a · v1 ↓ +b · v2 ↓)(a · u1 + b · u2)
=
a
d
4
· (a · v1 ↓ +
b
2
a
· v1 ↓)(a · u1 +
b
2
a
· u1) (by (8))
=
a
d
4
· (
a
2 + b
2
a
· v1 ↓)(a
2 + b
2
a
· u1)
=
1
a
· (v1 ↓)u1.
The proof is complete.
Theorem 4.4. The operation ⇑U defines a trace for the
monoidal category (Iso, ⊕).
Proof. Naturality of trace with respect to permutations is easy
to see. For example, the sliding axiom (dinaturality in U) can
be shown for an arbitrary permutation σ : V → U as follows.
Let τ : U ⊕ K → U ⊕ L be an isometry with h[A, B], [C, D]i
being the biproduct decomposition (matrix) of τ. Then, for the
“matrix” S of σ:
⇑V ((σ ⊕ I) ◦ τ ◦ (σ
−1 ⊕ I))
= D + CS−1
(I − SAS−1
)
+SB
= D + CS−1
(SS−1 − SAS−1
)
+SB
= D + CS−1
(S(I − A)S
−1
)
+SB
= D + CS−1S(I − A)
+S
−1SB
= D + C(I − A)
+B =⇑U τ.
In the above derivation we have used the obvious property
(SMS−1
)
+ = SM+S
−1
of the MP inverse. Remember that σ is a permutation, so that
σ
−1 = σ
†
. Superposing, yanking, and the derived composition
axiom are trivial. Therefore the only challenging axiom is
vanishing.
Let τ : U ⊕ V ⊕ K → U ⊕ V ⊕ L be an isometry given by
the matrix

M B
C D 
, where M =

P Q
R S 
.
We need to prove that ⇑V (⇑U τ) =⇑U⊕V τ. Again, without
loss of generality, we can assume that (I − P) is invertible
and
I − P\M =

0 0
0 S0

,
where V = N ⊕ V0 and S0 : V0 → V0 is invertible. If N is
the zero space, so that I − P\M itself is invertible, then the
statement follows from Lemma 4.2. Otherwise
⇑V (⇑U τ) =⇑V0
(⇑N (⇑U τ)).
By Lemma 4.3, ⇑N (⇑U τ) =⇑U⊕N τ, and by Lemma 4.1,
⇑V0
(⇑U⊕N τ) =⇑U⊕N⊕V0
τ =⇑U⊕V τ.
The proof is now complete. ✷
At this point the reader may want to check the validity of
the Conway semiring axioms
(ab)
∗ = a(ba)
∗
b + 1, (a + b)
∗ = (a
∗
b)
∗
a
for all a, b ∈ C, where
c
∗ = (1 − c)
+ =

(1 − c)
−1
if c 6= 1
0 if c = 1.
See [10]. Obviously, they do not hold, but they come very
close. It may also occur to the reader that the Schur Icomplement defines a trace in the whole category (FdHilb, ⊕).
Of course this is not true either, because the Banachiewicz
formula does not work for the MP inverse.
In a recent paper [21], Malherbe et al. introduced the so
called kernel-image trace as a partial trace [14] on any additive
category C. Given a morphism τ : U ⊕ K → U ⊕ L in C
with a block matrix τ = h[τA, τC ], [τB, τD]i as above, the
kernel-image trace ↑
U
k−i
τ is defined if both τB and τC factor
through (I − τA), that is, there exist morphisms i : K → U
and k : U → L such that
τC = i ◦ (I − τA) and τB = (I − τA) ◦ k.
See Fig. 4.
τB
τC
τA
I−
U L
K U
i
k
Fig. 4. The kernel-image trace
In this case
↑
U
k−i
τ = τD + τC ◦ k = τD + i ◦ τB.
It is easy to see that ↑
U
k−i
τ is always defined if τ is
an isometry, and ↑
U
k−i
τ =⇑
U τ. (Use the kernel-on-top
transformation of (I −τA) as in Theorem 4.1.) Therefore ↑
U
k−i
is totally defined on (Iso, ⊕) and it coincides with ⇑
U . Using
[21, Remark 3.3] we thus have an alternative proof of our
Theorem 4.4 above.
Now we turn back to the original definition of trace in
(Iso, ⊕) by (1).
Theorem 4.5. For every isometry τ : U ⊕ K → U ⊕ L, ↑
U τ
is well defined as an isometry K → L. Moreover,
↑
U
τ =⇑
U
τ.
Proof. This is in fact a simple formal language theory exercise. Take a concrete representation of τ as an (n+k)×(n+l)
complex matrix (aij ), where n, k, and l are the dimensions of
U, K, and L, respectively. For a corresponding set of variables
X = {xij}, consider the matrix iteration theory Mat L(X∗)
determined by the iteration semiring of all formal power series
over the ω-complete Boolean semiring B with variables X as
described in Chapter 9 of [10]. The fundamental observation
is that ↑
n (aij ) is the evaluation of the series matrix ↑
n (xij )
under the assignment xij = aij , provided that each entry in
this matrix is convergent. In our case, since |a11| ≤ 1, this matrix is definitely convergent if n = 1, and ↑
1
(aij ) =⇑
1
(aij ).
A straightforward induction on the basis of Theorem 4.4 then
yields ↑
n (aij ) =⇑
n (aij ), knowing that every iteration theory
is a traced monoidal category. ✷
Corollary 4.6. The monoidal category (DQT, ⊠) is traced
by the feedback ↑.
Proof. Now the key observation is that, for every isometry
τ : U ⊕ K → U ⊕ L and object M,
(⇑
U
τ) ⊗ IM =⇑
U⊗M (τ ⊗ IM).
This equation is an immediate consequence of
(σ ⊗ I)
+ = σ
+ ⊗ I,
which is an obvious property of the MP inverse. (See the defining equations (i)-(ii) of σ
+.) In the light of this observation,
each traced monoidal category axiom is essentially the same
in (DQT, ⊠) as it is in (Iso, ⊕). Thus, the statement follows
from Theorems 4.4 and 4.5. ✷
V. MAKING TURING AUTOMATA BIDIRECTIONAL
Now we are ready to introduce the model of quantum Turing
automata as a real quantum computing device.
Definition 5.1. A quantum Turing automaton of rank K is a
triple T = (H, K, τ), where H and K are finite dimensional
Hilbert spaces and τ : H⊗K → H⊗K is a unitary morphism
in FdHilb.
a) b)
Fig. 5. One cell of a Turing machine as a QTA
Again, two automata Ti
: (Hi
, K, τi), i = 1, 2 are called
isomorphic if there exists an isometric isomorphism σ : H1 →
H2 for which τ2 = (σ
† ⊗ IK) ◦ τ1 ◦ (σ ⊗ IK).
Example In Fig. 5a, consider the abstract representation
of one tape cell drawn from a hypothetical Turing machine
having two states: 1 and 2. The tape alphabet {0, 1} is also
binary, which means that there is a single qubit sitting in the
cell. Thus, H is 2-dimensional. The control particle c can
reside on any of the given four interfaces. For example, if
c is on the top left interface, then control is coming from the
left in state 1. After one move, c can again be on any of
these four interfaces, so that the dimension of K is 4. Notice
the undirected nature of one move, as opposed to the rigid
input→output orientation forced on DQTA. The situation is,
however, analogous to having a separate input and dual output
interface for each undirected one in a corresponding DQTA.
See Fig. 5b. Let T0 denote the quantum Turing automaton
(QTA, for short) so obtained, equipped with an appropriate
transition operator τ as an 8 × 8 unitary matrix.
We are going to describe the structure of QTA directly as
an indexed monoidal algebra QT . The object monoid for QT
is the monoid of objects in (Iso, ⊕). Morphisms QT of rank
K are all isomorphism classes of QTA of rank K. Tensor of
morphisms will be denoted by ⊞, for the symbol ⊗ is heavily
overloaded.
With a slight ambiguity we identify each permutation symbol ρ with its interpretation in (Iso, ⊕) as a permutation
isometry. Using the algebraic language, by an automaton T : K
(transition operator τ : H ⊗ K) we mean one with state space
H and interfaces K. Thus, our example automaton is T0 : 4
with τ0 : 2 × 4. The algebra QT is defined as follows.
— For T = (H, K, τ) and ρ : K ⇒ L, T · ρ comes with the
transition operator
(IH ⊗ ρ
−1
) ◦ τ ◦ (IH ⊗ ρ). (9)
— The identity automaton 1K : K ⊕K is the single-state QTA
having the transition operator:
κK,K = h[0, I], [I, 0]i (10)
— For automata
T1 = (H1, K1, τ1) and T2 = (H2, K2, τ2),
T1 ⊞ T2 = (H, K1 ⊕ K2, τ1 ⊞ τ2),
where H = H1 ⊗ H2 and
τ1 ⊞ τ2 = τ1 ⊠ τ2 : (11)
H ⊗ (K1 ⊕ K2) → H ⊗ (K1 ⊕ K2)
— For T = (H, U ⊕ U ⊕ K, τ),
lU T = (H, K, lU τ),
where
lU τ =↑
U⊕U (τ ◦ (IH ⊗ (κU,U ⊕ IK))). (12)
Notice the “alternating twist” κ in the definition of 1 and l,
which is characteristic of the Int construction. For a better
intuitive understanding, see also the corresponding analogous
definitions in [7] with respect to conventional Turing automata.
Recall from Section 2 that the functor Alg takes an arbitrary
traced monoidal category and turns it into an indexed monoidal
algebra through the Int construction.
Theorem 5.1. QT ∼= Alg(DQT) is an indexed monoidal
algebra.
Proof. (Sketch) First we review the definition of the functor
I : SDCC → IMA from [6], which turns an SDCC category
into an equivalent IMA. Let C be an SDCC category over
a monoid M as objects. Then, for each object A in M,
the morphisms of M = IC with rank A are all morphisms
I → A in C. Indexing in M is essentially the restriction of
the covariant hom functor C(I, ) to permutations, and
1A = dA : I → A ⊗ A.
For f : A and g : B,
f ⊗M g = f ⊗C g : I → A ⊗ B,
and for f : A ⊗ A ⊗ B, lA f is defined as the canonical trace
of the morphism fA : A → A ⊗ B in C that corresponds to f
according to compact closure.
Now let C be the restriction of Int(DQT) to its selfdual objects (K, K). That is, a morphism K → L in C is a
morphism (K, K) → (L,L) in Int(DQT). The morphisms of
rank K in Alg(DQT), being the morphisms (Z, Z) → (K, K)
in Int(DQT), are therefore isometries H ⊗ K → H ⊗ K
for some H. In FdHilb, these are exactly the unitary maps
H ⊗ K → H ⊗ K. Consequently, the correspondence between
the morphisms of Alg(DQT) and QT is one-to-one and onto.
It is easy to verify that the unit map dK in C is κK,K, so that
the definition (10) of 1K is correct. Also, indexing becomes the
combination of the covariant and contravariant hom functors
DQT(I, ) and DQT( , I) as specified in (9). Concerning
(11), let
T1 : (Z, Z) → (K1, K1) and T2 : (Z, Z) → (K2, K2)
a
b
. . .
. . .
Fig. 6. A segment of a Turing machine as a QTA
be morphisms in Int(DQT). By definition, T1 = (H1, τ1) :
K1 → K1 and T2 = (H2, τ2) : K2 → K2 in DQT. Again, by
the very definition of ⊗ in Int(DQT),
T1 ⊗ T2 = T1 ⊠ T2 : K1 ⊕ K2 → K1 ⊕ K2.
Similarly, as to (12), if
T : (Z, Z) → (U ⊕ U ⊕ K, U ⊕ U ⊕ K), that is
T = (H, τ) : U ⊕ U ⊕ K → U ⊕ U ⊕ K,
then the morphism TU : U → U ⊕ K in C (that is,
T(U,U)
: (U, U) → (U ⊕ K, U ⊕ K)
in Int(DQT)) that corresponds to T by compact closure is
the automaton
(H, τU ) : U ⊕ U ⊕ K → U ⊕ K ⊕ U,
where τU = τ ◦ (IH ⊗ κU,U⊕K). Therefore
T rU TU = T r(U,U)T(U,U) = (H, σ),
where
σ = ↑
U⊕U (τU ◦ (IH ⊗ (IU ⊕ κK,U )))
= ↑
U⊕U (τ ◦ (IH ⊗ (κU,U ⊕ IK))) =lU τ.
The proof is complete. ✷
Example (Continued) In Fig. 6a, consider a segment of our
hypothetical Turing machine consisting of n cells. As shown
in Fig. 6b, the semantics of this segment as a QTA is:
l2(n−1) ((⊞
n
i=1T0) · ρ) : 4,
where ρ : 4n → 4n is the permutation that sends k = 1, 2 and
l = 4n − 1, 4n to 4(n − 1) + k and l, respectively, and
ρ(4i+k)=
2i+(k−2) if 0 ≤ i< n−1, k=3, 4
2(n−1)+2(i−1)+k if 1 ≤ i < n, k = 1, 2.
VI. CONCLUSION
We have provided a theoretical foundation for the study of
quantum Turing machines. The biproduct strongly compact
closed category FdHilb of finite dimensional Hilbert spaces
served as the underlying structure for this foundation. We
narrowed down the scope of this category to isometries,
switched from multiplicative to additive tensor, and defined
a new additive trace operation by the help of the MoorePenrose generalized inverse. This trace was then carried over
to the monoidal category of directed quantum Turing automata.
Finally, we applied the Int construction to obtain a compact
closed category, which we further transformed into the indexed
monoidal algebra of undirected quantum Turing automata.
Under consideration for publication in Math. Struct. in Comp. Science
The monoidal structure of Turing machines
MIKLOS BARTHA ´ †
Department of Computer Science, Memorial University of Newfoundland
St. John’s, NL, Canada
Received 10 September 2011
Indexed monoidal algebras are introduced as an equivalent structure for self-dual
compact closed categories, and a coherence theorem is proved for the category of such
algebras. Turing automata and Turing graph machines are defined by generalizing the
classical Turing machine concept, so that the collection of such machines becomes an
indexed monoidal algebra. On the analogy of the von Neumann data-flow computer
architecture, Turing graph machines are proposed as potentially reversible low-level
universal computational devices, and a truly reversible molecular size hardware model
is presented as an example.
1. Introduction
The importance of reversibility in computation has been argued at several platforms in
connection with the speed and efficiency of modern-day computers. As stated originally
by Landauer (Landauer 1961) and re-emphasized by Abramsky (Abramsky 2005a): “it is
only the logically irreversible operations in a physical computer that necessarily dissipate
energy by generating a corresponding amount of entropy for every bit of information that
gets irreversibly erased”. Abramsky’s remedy for this situation in (Abramsky 2005a) is
to translate high level functional programs in a syntax directed way into a simple kind
of automata which are immediately seen to be reversible. The concept strong compact
closed category (Abramsky 2005b) has been introduced and advocated as a theoretical
foundation for this type of reversibility.
The problem of reversibility, however, does not manifest itself at the software level.
Even if we manage to perform our programs in reverse, it is not guaranteed that information will not be lost during the concrete physical computation process. To the contrary,
it may get lost twice, once in each direction. The solution must therefore be found at the
lowest hardware level. Our model of Turing graph machines is being presented as a possible hardware solution for the problem of reversibility, but follows Abramsky’s structural
approach. We even go one step further by showing how computations can be done in a
virtually undirected fashion under the theoretical umbrella of self-dual compact closed
† Work partially supported by the Natural Science and Engineering Research Council of Canada.
M. Bartha 2
categories. In practical terms we mean that, unlike in synchronous systems (e.g. sequential circuits), where the information is propagated through the interconnections (wires)
between the functional elements (logical gates) always in the same direction, in a Turing
graph machine the flow of information along these interconnections takes a direction that
is determined dynamically by the current input and state of the machine. We are going
to reconsider self-dual compact closed categories as indexed monoidal algebras and prove
a coherence theorem to establish undirected graphs – constituting the basic underlying
structure for Turing graph machines – as free indexed monoidal algebras generated by
the ranked alphabet consisting of the star graphs.
Different parts of this paper need not be read in a strict sequential order. In-depth
knowledge of algebra and category theory is only required in Sections 2, 3, and 4. The
reader less familiar with categories could still understand the concept of Turing automata
and Turing graph machines in Section 6, and appreciate the main practical contribution
of this study. Then, gradually moving backwards to previous sections, one can also understand the categorical background through the numerous examples and diagrams provided
in the text to help the intuition.
Our work relates to several significant theoretical endeavors to capture the geometry of
the operations tensor and trace (feedback, iteration) in different models. The most important of these are linear logic (Girard 1987), game semantics (Hyland 1997), semantics of
quantum protocols and programming languages (Abramsky and Coecke 2004; D’Hondt
and Panangaden 2006), communicating concurrent processes (Milner 2009), interaction
nets (Lafont 1989), and Girard’s Geometry of Interaction program (Girard 1989; Girard
1990) in general. The future generalization of our coherence result to ribbon categories
(balanced monoidal categories (Joyal and Street 1991)) will also make a connection to
knot theory (Freyd and Yetter 1992), and other directions specified in (Joyal et al. 1996;
Joyal and Street 1991).
The idea of Turing automata and graph machines, however, was inspired solely by
C. C. Elgot’s work on flowchart schemes and their semantics (Elgot 1975), which work
has later been developed by S. L. Bloom and Z. Esik to a theory highlighted by the ´
concept iteration theories (Bloom and Esik 1993). In a sense the present paper provides ´
a synthesis of finite automata and Turing machines in the framework of category theory,
and introduces an indexing mechanism (Burstall et al. 1989) for iteration theories.
This paper is an extension of a talk given by the author at DCM 2010, Edinburgh.
2. Traced monoidal and compact closed categories
In this section we shall assume familiarity with the concept of symmetric monoidal categories (Mac Lane 1971). It is known, cf. (Mac Lane and Par´e 1985), that every monoidal
category is equivalent to a strict one. Repeating an argument from (Joyal and Street
1991), most results obtained with the hypothesis that a monoidal category is strict can,
in principle, be reformulated and proved without that condition. Even though our results in this paper are no exceptions, we shall not assume that our monoidal categories
are strict, unless this assumption is clearly technical and simplifies the discussion significantly. Since the concept of indexed monoidal algebras is being introduced in the
Structure of Turing machines 3
present paper, it is appropriate that its definition be given under the general non-strict
conditions. As another argument, the principal example of Turing automata presented
in Section 6 is not strict as an indexed monoidal algebra or traced monoidal category. A
further generalization of this concept for braidings, rather than symmetries, will follow.
Thus, a monoidal category consists of a category C, a bifunctor ⊗ : C × C → C, a
unit object I of C, and natural isomorphisms aX,Y,Z : (X ⊗ Y ) ⊗ Z → X ⊗ (Y ⊗ Z)
as associators, lX : I ⊗ X → X and rX : X ⊗ I → X as left and right unitors, and
cU,V : U ⊗ V → V ⊗ U as symmetries, subject to the well-known coherence axioms
specified in (Mac Lane 1971).
Traced monoidal categories (with one additional axiom) and their graphical language
first appeared in (Bartha 1987a) in an algebraic setting, using the name “scheme algebra”
for these structures. The operation trace was called feedback. Essentially the same axiomatization and graphical language was published in (C˘az˘anescu and S¸tef˘anescu 1990)
under the name “biflow”. Joyal, Street, and Verity then rediscovered this notion, generalized it to balanced monoidal categories, and presented the fundamental Int construction
on the embedding of an arbitrary balanced traced monoidal category into a tortile one
(Joyal et al. 1996). In case braiding is symmetry, as it is in our present study, the Int
construction transforms an arbitrary traced monoidal category into a compact closed
category.
The following definition of traced monoidal categories uses the terminology of (Joyal et
al. 1996). Trace in a monoidal category C is introduced as left trace, that is, an operation
C(U + A,U + B) → C(A,B), rather than C(A + U,B + U) → C(A,B) (i.e., right trace)
as it appears in (Joyal et al. 1996), to be in accordance with the author’s own work. Let
C be a monoidal category with tensor ⊗ and unit object I.
Definition 2.1. A trace for a symmetric monoidal category C is a natural family of
functions
TrU
A,B : C(U ⊗ A,U ⊗ B) → C(A,B)
satisfying the following three axioms:
vanishing:
TrI
A,B(f) = f for f : A → B,
TrU⊗V
A,B (aU,V,A ◦ g) = TrV
A,B(TrU
V ⊗A,V ⊗B(g ◦ aU,V,B)),
where g : U ⊗ (V ⊗ A) → (U ⊗ V ) ⊗ B;
superposing:
TrU
A,B(f) ⊗ g = TrU
A⊗C,B⊗D(a
−1
U,A,C ◦ (f ⊗ g) ◦ aU,B,D),
where f : U ⊗ A → U ⊗ B, and g : C → D;
yanking:
TrU
U,U (cU,U ) = 1U .
Naturality of trace is meant in all three variables A,B,U. Naturality in A and B is selfexplanatory, while (di-)naturality in U is expressed by the following separate axiom.
M. Bartha 4
sliding:
TrU
A,B((g ⊗ 1A) ◦ f) = TrV
A,B(f ◦ (g ⊗ 1B)) for f : V ⊗ A → U ⊗ B, g : U → V.
See Fig. 1, and notice that we write composition of morphisms (◦) in a left-to-right
manner. When using the term feedback for trace, the notation Tr changes to ↑.
g
f g
f
U
U B V B
A V A
V
U
=
Fig. 1. The sliding axiom
On a separate line, compact closed categories were introduced in (Kelly and Laplaza
1980).
Definition 2.2. A symmetric monoidal category C is compact closed (CC, for short)
if every object A has a left adjoint A∗
in the sense that there exist morphisms dA :
I → A ⊗ A∗
(the unit map) and eA : A∗ ⊗ A → I (the counit map) for which the two
composites below result in the identity morphisms 1A and 1A∗ , respectively.
A ≃l I ⊗ A →dA⊗1A (A ⊗ A
∗
) ⊗ A ≃a A ⊗ (A
∗ ⊗ A) →1A⊗eA A ⊗ I ≃r A,
A
∗ ≃l A
∗ ⊗ I →1A∗⊗dA A
∗ ⊗ (A ⊗ A
∗
) ≃a (A
∗ ⊗ A) ⊗ A
∗ →eA⊗1A∗ I ⊗ A
∗ ≃r A
∗
,
where l, r, and a stand for appropriate left unitor, right unitor, and associator morphisms,
respectively.
By virtue of the adjunctions A ⊣ A∗
there is a natural isomorphism between the homsets C(B ⊗ A,C) and C(B,C ⊗ A∗
) for every objects B,C, hence the name “compact
closed” category. Category C is self-dual compact closed (SDCC, for short) if A = A∗
for each object A. The category SDCC has as objects all SDCC categories, and as
morphisms strict monoidal functors preserving the given self-adjunctions.
Every CC category admits a so called canonical trace (Joyal et al. 1996) defined by
the formula
TrU
A,Bf = (dU∗ ⊗ 1A) ◦ (1U∗ ⊗ f) ◦ (eU ⊗ 1B)
in the strict setting. See Fig. 2. A well-known SDCC category is the category (Rel, ×) of
sets and relations with tensor being the cartesian product ×. We shall use this category
as an example to explain the idea of indexing on it.
According to (Burstall et al. 1989; Bartha and J¨urgensen 1989), an indexed family of
sets is a functor I : Ind → Set, where Ind is the index category. In our example, Ind
is the monoidal category (Set, ×) as a subcategory of (Rel, ×) and I is the covariant
powerset functor P, which is of course not monoidal. Relations A → B are, however, still
Structure of Turing machines 5
U *
U *
A
B
f
U
U
U
U
A
B
f =
e
d
Fig. 2. Canonical trace in CC categories.
subsets of A×B, and as such they can be indexed by morphisms (functions) A×B → C
in Set. For any two objects (sets) one can then consider the binary operation tensor,
⊗A,B(= ×) : P(A) × P(B) → P(A × B), and the unary operation trace, lA,B: P(A ×
A × B) → P(B) for which b ∈lA,B R iff ∃a ∈ A ((a,a),b) ∈ R. The concept indexed
monoidal algebra arises from observing the equational algebraic laws satisfied by these
operations and their relationship to indexing. The reader will soon be reassured that the
word “trace” is not being abused in this context.
3. Association and permutation symbols
Following up on our example SDCC category (Rel, ×) as an indexed family of sets
equipped with the operations ⊗ and l, observe that the index category Ind can be chosen
in many different meaningful ways without affecting the outcome of indexing, as long as
Ind remains a sub-(monoidal) category of (Rel, ×) containing all permutations. For
example Ind could be chosen as the restriction of (Set, ×) to bijections, or, on the other
extreme, it could even be the whole category (Rel, ×). In any case, the index functor I is
essentially an appropriate restriction of the covariant hom-functor Hom(I, ) in (Rel, ×).
Clearly, the larger the index category, the less is hidden from the SDCC category to
be recaptured, that is, the less interesting the whole description. Therefore we take a
minimalistic approach with regard to indexing, and abolish the index category altogether.
Instead, we introduce permutation symbols, which can be interpreted as permutations
in any suitable choice of a hypothetical index monoidal category. The problem is that
the symbols themselves do not form a category over the given objects; in our example,
sets. By Mac Lane’s coherence theorem for monoidal categories, permutation symbols
form a category over object terms as objects, which terms are built up freely from the
given objects as object variables. Eventually, permutation symbols will be used as unary
operations in the algebraic structure already containing ⊗ and l.
3.1. Association symbols
Let τ = (⊗,I) be the algebra type – fixed for the rest of the paper – consisting of a
binary operation symbol ⊗ (tensor) and a constant symbol I (unit). Let, furthermore,
X = {x1,x2,... ,xn,...} be a countably infinite set of variable symbols, and Xn =
M. Bartha 6
{x1,... ,xn} for each n ∈ N. The carrier set of the free τ -algebra generated by X is
denoted by Tτ (X), as usual. This set consists of all τ -terms (trees) over the variables X.
For a tree t ∈ Tτ (X), yield(t) ∈ X∗
is the string of variables appearing on the leaves
of t in a left-to-right order. The set T˜
τ (X) ⊆ Tτ (X) consists of those trees t for which
yield(t) = x1x2 ... xn for some n ∈ N. The number n is called the length of t, denoted l(t).
Observe that a tree t ∈ T˜
τ (X) is just an alternative representation of a binary word in Mac
Lane’s (Mac Lane 1971) coherence theorem, so that every occurrence of a variable is one
of the place holder symbol . Note that, even though we have infinitely many variables,
there is only one place holder in t, since xi always marks the i-th occurrence of the place
holder from left to right. In the concrete representation of trees we shall adopt the usual
prefix notation for the operation ⊗. For example, t = ⊗(x1, ⊗(⊗(I,x2), ⊗(x3,I))) is a
tree in T˜
τ (X) of length 3.
Redefine the operation ⊗ over T˜
τ (X) in such a way that t1⊗t2 is obtained from ⊗(t1,t2)
by incrementing the index of each variable occurring in the subtree t2 by l(t1). Notice
that the notation for ⊗ changes from prefix to infix in the resulting τ -algebra. Clearly,
l(t1 ⊗ t2) = l(t1) + l(t2). According to Mac Lane’s coherence theorem, (T˜
τ (X), ⊗,I)
then becomes the object structure of a monoidal category Λ in which there is a single
morphism between t and t
′
iff l(t) = l(t
′
). This is the free single-sorted monoidal category,
i.e., the one freely generated by a single object, in our representation the tree x1.
Proposition 3.1. (Mac Lane 1971, Theorem VII.1) For every monoidal category C and
any object C of C there exists a unique strict monoidal functor Λ → C sending x1 to C.
To switch from single-sorted to many-sorted, let O be a class of object variables. Define
Tτ (O) as the collection of pairs (t,α), where t ∈ T˜
τ (X) is a tree of length n and α :
Xn → O is a mapping. The pair (t,α) is called an object term. The mapping α defines a
sequence of objects (A1,... ,An) (also denoted α), so that we can identify the pair (t,α)
with a single tree u the leaves of which are labeled by the object variables in O rather
than the symbols in X. Roughly speaking, O takes over the role of X, except that O is
not necessarily a set.
For pairs (trees) u1 = (t1,α1) and u2 = (t2,α2), let u1 ⊗ u2 = (t1 ⊗ t2,α1 ⊗ α2), where
(α1 ⊗ α2)(xi) = ½
α1(xi) if 1 ≤ i ≤ l(t1)
α2(xi−l(t1)) if l(t1) < i ≤ l(t1) + l(t2).
Define ΛO to be the monoidal category having (Tτ (O), ⊗,(I, ∅)) as its object structure,
so that there is a unique morphism (t1,α) → (t2,α) in ΛO iff l(t1) = l(t2). This unique
morphism will be called the O-association symbol (a-symbol, for short) (t1,α) → (t2,α),
denoted a(t1,t2,α). On the analogy of Proposition 3.1 it is easy to see that ΛO is freely
generated by the object variables (sorts) O.
Proposition 3.2. For every monoidal category C and any mapping φ from O to the
objects of C there exists a unique strict monoidal functor ΛO → C, also denoted φ,
sending (x1,A) to φ(A) for each object variable A.
Now let O = (O, ⊗,I) be a τ -algebra, that is, a class O of objects with an interpretation
of the τ -operations on O. Considering objects as object variables, we can still speak of
Structure of Turing machines 7
object terms over O. For an object term u = (t,α) in Tτ (O), let |u|O (or simply |u|, if
O is understood) be the evaluation of u in O as an object in O. If ρ = a(t1,t2,α) is any
O-association symbol u1 → u2, then we also say that ρ is an O-association symbol from
|u1| to |u2|, and use the somewhat abusing notation ρ : |u1| ⇒ |u2|. Keep in mind that
two O-association symbols may not be composed as “morphisms” suggested by this way
of writing. In general, to speak of the category ΛO of O-association symbols does not
make sense. Nevertheless, by Proposition 3.2, each a-symbol ρ : A ⇒ B has a unique
canonical interpretation as a real associator morphism ρ : A → B in every monoidal
category C having O as its object structure.
Consider now Tτ (O) as the base collection of object variables, and concentrate on the
monoidal category ΛTτ (O)
. The objects in this category are pairs (t, α), where t is a
term in T˜
τ (X) and α : Xn → Tτ (O) is a mapping. By Proposition 3.2 there exists a
unique “syntactical” strict monoidal functor ΛO : ΛTτ (O) → ΛO extending the “identity”
injection (x1,u) 7→ u. Technically, ΛOa(t1,t2, α) is obtained by substituting the sequence
of trees u1,... ,un determined by α into t1 and t2. Given the algebra O, the result is
viewed as an O-association symbol A ⇒ B for appropriate objects A and B. On the other
hand, the a-symbol a(t1,t2, α) gives rise “semantically” to the O-association symbol
a(t1,t2, α)/O = a(t1,t2,γ) : A′ ⇒ B′
, where γ = (|u1|,... , |un|). By the algebraic laws
of tree substitution, A = A′ and B = B′
. We say that the a-symbols ρ1 = ΛOa(t1,t2, α)
and ρ2 = a(t1,t2, α)/O are equivalent, and write ρ1 ≡ ρ2. We call ρ1 a refinement of
ρ2. The point is that ρ1 and ρ2 define the same associator morphism A → B in every
monoidal category having the object structure O. In general, it is not easy to give a
concrete characterization of the equivalence ≡ as the symmetric and transitive closure
of refinement. Fortunately, however, we shall not need any such characterization in our
coherence axiom I3 in Section 4.1 below, for simple refinement will do as ≡ in that axiom.
3.2. Permutation symbols
Let O and O = (O, ⊗,I) be as in the previous subsection.
Definition 3.1. An O-permutation symbol is a quadruple (t1,t2,α,χ), where t1,t2 ∈
T˜
τ (X) are of the same length n, α : Xn → O is a mapping, and χ : n → n is a
permutation.
As a concrete permutation symbol (p-symbol, for short), the quadruple above will be
identified as p(t1,t2,α,χ). Extending the monoidal category ΛO of O-association symbols,
the symmetric monoidal category ΠO of O-permutation symbols is constructed as follows.
Objects in ΠO are the same as in ΛO, whereas morphisms (t1,α) → (t2,α′
) are p-symbols
p(t1,t2,α,χ) such that α(i) = α
′
(χ(i)) for all i ∈ [n] = {1,... ,n}, where n is the common
length of t1 and t2. Technically speaking, the object variables appearing on the leaves of
the tree u1 = (t1,α) are relabeled according to χ, and the tree structure itself is replaced
by t2. Composition and tensor in ΠO are defined by:
— p(t1,t2,α,χ) ◦ p(t2,t3,α′
,η) = p(t1,t3,α,χ ◦ η);
— p(t1,t2,α,χ) ⊗ p(q1,q2,β,η) = p(t1 ⊗ q1,t2 ⊗ q2,α ⊗ β,χ ⊗ η).
M. Bartha 8
Remember that α
′ = χ
−1 ◦ α and χ ⊗ η is the obvious tensor of χ and η. The identity on
object u = (t,α) is 1u = p(t,t,α,idn), and the symmetry cu,v for u = (t,α) and v = (q,β)
is p(t⊗q,q⊗t,α⊗β,πn,m), where n = l(t), m = l(q), and πn,m is the block transposition
n + m → m + n. Notice that the uniqueness of morphisms between two given objects no
longer holds in ΠO. By Mac Lane’s coherence theorem (Mac Lane 1963) for symmetric
monoidal categories, however, we still have the counterpart of Proposition 3.2.
Proposition 3.3. For every symmetric monoidal category C and any mapping φ from
O to the objects of C there is a unique strict symmetric monoidal functor φ : ΠO → C
sending (x1,A) to φ(A) for each object variable A in O.
Given the algebra O one can again classify an O-permutation symbol p(t1,t2,α,χ) : u1 →
u2 as an O-one |u1| ⇒ |u2|, based on the evaluation of the trees u1 and u2. Permutation
symbols ρ1 : A ⇒ B and ρ2 : B ⇒ C are called composable if they are such as morphisms
in the category ΠO. The concept of refinement and equivalence is also adopted in the
straightforward manner: for every Tτ (O)-permutation symbol ρ = p(t1,t2, α,χ) with
α = (u1,... ,un), the p-symbol ΠOρ is a refinement of ρ/O = p(t1,t2,γ,χ), where
γ = (|u1|,... , |un|). Remember that ΠO is the syntactical strict symmetric monoidal
functor ΠTτ (O) → ΠO extending the injection (x1,u) 7→ u. The meaning of ρ1 ≡ ρ2 is
again that ρ1 and ρ2 define the same permutation in every symmetric monoidal category
C having the object structure O.
3.3. Strict permutation symbols
On the ground of strict monoidal categories the form of permutation symbols becomes
considerably simpler. Since the ⊗-tree structure need not be indicated, the objects of
the category ΠO are simply finite sequences (strings) of object variables. Accordingly,
a strict O-permutation symbol is a pair (u,χ), where u is a string of object variables
having length n and χ is a permutation n → n. In the presence of a monoid O over O,
the empty string as an object term is identified with I.
For notational convenience we shall use a few instances of the symbols 1u and cu,v in
the non-strict setting as if they were strict, i.e., as if the object terms u,v were strings
and not trees. This can be done safely if the length of u and v is not more than 2, and I
does not occur on the leaves of these trees. For example, 1A, 1AB, and cA,AB will do as
p-symbols
(x1,A) → (x1,A),
(⊗(x1,x2),(A,B)) → (⊗(x1,x2),(A,B)), and
(⊗(x1, ⊗(x2,x3)),(A,A,B)) → (⊗(⊗(x1,x2),x3),(A,B,A)),
respectively, but 1ABC is already ambiguous as a non-strict identity. Taking this idea one
step further, an object u = (t,α) of ΠO can be represented by a string of pairs
(A1,p1)...(An,pn),
where each Ai
is either an object variable or an occurrence of the symbol I (which is not
supposed to be present in O). The second component pi of (Ai
,pi) is the path identifier
Structure of Turing machines 9
for leaf Ai
in the tree u as a string of 1’s and 2’s. (E.g., 1 ... 1 identifies the leftmost leaf,
and 2 ... 2 the rightmost one.) See Fig. 3. The set of all such paths in u will be denoted
by path(u), and obju(p) will refer to the object variable (or I) sitting at leaf p of u. Path
p is called variable if it points to an object variable, and constant if obju(p) = I. Then
every p-symbol p(t1,t2,α,χ) : u1 → u2 can be written in a unique way as a “quasi”-strict
p-symbol
(A1,p1)...(An,pn) → (B1,q1)...(Bm,qm),
where pi and qj are appropriate paths in path(u1) and path(u2). In this way of writing
the permutation χ indeed appears as one in the corresponding strict permutation symbol
between the yields of u1 and u2. See again Fig. 3. We shall need this representation of
p-symbols in Section 5.
Ä
Ä
Ä
A
A
B
I B
(A,11) (I,12) (B,2)
(B,1) (A,2)
=
Fig. 3. Quasi-strict representation of p-symbols.
4. Indexed monoidal algebras
In this section we introduce the category IMA of indexed monoidal algebras along the
lines of the report (Bartha and J¨urgensen 1989), and establish an equivalence between the
categories IMA and SDCC. From this point on, throughout the paper, by a monoidal
category we mean a symmetric one.
Beyond the idea of indexing, the concept of indexed monoidal algebras is also very
closely related to that of enriched categories (Kelly 1982). For an illustration we are
going to recapture our example SDCC category (Rel, ×) as an enriched one. An enriched
category has a base monoidal category C, in our case the category (Rel, ×) itself, from
which the morphisms are adopted. Independently, there is a class R of objects, in our
example, sets. To each pair (A,B) of objects in R one must assign a so called hom-object
A(A,B) of C. We choose A(A,B) = A × B. Then one needs to specify a composition law
M = MA,B,C : A(A,B) ⊗ A(B,C) → A(A,C) as a collection of morphisms in C. This
choice is fairly straightforward in our case as a relation (A×B)×(B×C) → (A×C). The
identity element jA : I → A(A,A) is of course the identity relation on A. These data are
subject to the conditions specified in (Kelly 1982), which conditions are trivially satisfied
in our example. It follows that C = (Rel, ×) can be recaptured as the underlying category
of the C-enriched category A defined in the above way. By definition, the underlying
category has R as objects, and as morphisms A → B the morphisms f : I → A(A,B) in
C. Observe that the idea is to eventually assign a collection of morphisms with each pair
M. Bartha 10
of objects (A,B), but this is done in an implicit way through the hom-functor Hom(I, )
of the category C. In our example this is exactly the covariant powerset functor, which
is perfectly in line with the indexing idea presented in the previous section.
Of course the above way of recapturing an SDCC category C is trivial, not hiding C itself
at all. Therefore we only adopt the idea of assigning a class of “uni”-morphisms to each
object (i.e., the set P(A) to object A). The base category is dropped, and the composition
law is replaced by the direct algebraic operations tensor and trace on morphisms. With
the help of the indexing mechanism facilitated by the permutation symbols, these two
operations will accomplish the same goal as the composition law in the base category.
See also (Bartha and J¨urgensen 1989), where the composition law itself was captured by
a general binary operation composition on morphisms.
4.1. The definition of indexed monoidal algebras
Formally, an indexed monoidal algebra (IMA, for short) consists of a class O of objects
A,B,C,..., a class M of morphisms f,g,h,..., and an operation rank, which assigns to
each morphism f an object A. We write f : A to indicate the rank of f. There is also
a distinguished unit object I and a binary operation ⊗ (tensor) on objects, determining
a τ -algebra O = (O, ⊗,I). As an early indication to the category structure in mind, by
f : A → B we mean a morphism f : A⊗B for each pair A,B of objects. The symbol ⊗ has
become heavily overloaded by now. For this reason we shall use ⊘ for ⊗ in permutation
symbols. Accordingly, composition of p-symbols will be denoted by •. Keep in mind,
however, that e.g. the symmetry O-permutation symbol cA,B : ⊘(A,B) → ⊘(B,A) is an
O-permutation symbol A ⊗ B ⇒ B ⊗ A semantically.
With respect to M, the following operations are imposed.
— For each O-permutation symbol ρ : A ⇒ B a unary operation ρ, which assigns to
each morphism f : A a morphism f · ρ : B.
— A binary operation tensor, which assigns to each pair of morphisms f : A and g : B
a morphism f ⊗ g : A ⊗ B.
— A unary operation trace, which assigns to each morphism f : (A⊗A)⊗B a morphism
lA,B f : B.
— For each object A a constant 1A : A ⊗ A.
Let us agree that we write lA f for lA,B f whenever B is understood. Notice the boldface
notation 1A : A → A as opposed to 1u : u → u for permutation symbols.
We shall use two more “categorical” operations, which are already derived from the
above basic ones.
— A binary operation composition, which assigns to each pair of morphisms f : A → B
and g : B → C the morphism
f ◦A,B,C g =lB ((f ⊗ g) · (a1 • (cA,BB ⊘ 1C ) • a2) : A → C, where
a1 : (A ⊘ B) ⊘ (B ⊘ C) → (A ⊘ (B ⊘ B)) ⊘ C,
a2 : ((B ⊘ B) ⊘ A) ⊘ C → (B ⊘ B) ⊘ (A ⊘ C)
Structure of Turing machines 11
are the obvious a-symbols. Observe that the prefix tree notation would actually be accurate for ⊘, but the infix one is easier to read in the present context. See Fig. 4a.
— A binary operation (general) tensor, assigning to each pair of morphisms f : A → B
and g : C → D, the morphism
f ⊗A,B,C,D g = (f ⊗ g) · (a1 • ((1A ⊘ cB,C ) ⊘ 1D) • a2) : (A ⊗ C) ⊗ (B ⊗ D), where
a1 : (A ⊘ B) ⊘ (C ⊘ D) → (A ⊘ (B ⊘ C)) ⊘ D,
a2 : (A ⊘ (C ⊘ B)) ⊘ D → (A ⊘ C) ⊘ (B ⊘ D).
See Fig. 4b. Again, let us agree that, ambiguous as it is, we shall not indicate the indices
in ◦ and ⊗ unless it is absolutely necessary. Clearly, the basic operation ⊗ is intended
to be the instance ⊗I,A,I,B of the general one. Observe that the above definition of
composition and tensor is in line with the traced monoidal category axioms. Regarding
composition, see also (Bartha 1987a, Identity X3). As we shall point out in Theorem 4.1
below, our trace operation models the canonical trace concept in SDCC categories.
f g f g
A B B
B A B A
A B C D
C C B D
C
a) b)
Fig. 4. Composition (a) and tensor (b) in M.
The above operations are subject to the following equational axioms.
I1. Functoriality of indexing
f · (ρ1 • ρ2) = (f · ρ1) · ρ2 for f : A and composable ρ1 : A ⇒ B, ρ2 : B ⇒ C;
f · 1A = f for f : A.
I2. Naturality of indexing
(f ⊗ g) · (ρ1 ⊘ ρ2) = f · ρ1 ⊗ g · ρ2 for f : A, g : B, ρ1 : A ⇒ C, ρ2 : B ⇒ D;
(lA f) · ρ =lA (f · (1AA ⊘ ρ)) for f : (A ⊗ A) ⊗ B, ρ : B ⇒ C.
I3. Coherence
f · ρ1 = f · ρ2 for f : A, whenever ρ1 ≡ ρ2.
I4. Associativity and symmetry of tensor
((f ⊗ g) ⊗ h) · aA,B,C = f ⊗ (g ⊗ h) for f : A, g : B, h : C;
f ⊗ g = (g ⊗ f) · cB,A for f : A, g : B.
I5. Right identity (yanking)
f ◦ 1B = f for f : A → B.
I6. Symmetry of identity
1A · cA,A = 1A.
M. Bartha 12
I7. Vanishing
lI f = f · a for f : (I ⊗ I) ⊗ A,
where a : (I ⊘ I) ⊘ A → A;
lA⊗B (f · a) =lB (lA f · ((1A ⊘ (cB,A ⊘ 1BC )) • a
′
)) for f : A ⊗ ((B ⊗ A) ⊗ (B ⊗ C)),
where a : A ⊘ ((B ⊘ A) ⊘ (B ⊘ C)) → ((A ⊘ B) ⊘ (A ⊘ B)) ⊘ C,
and a
′
: A ⊘ ((A ⊘ B) ⊘ (B ⊘ C)) → (A ⊘ A) ⊘ ((B ⊘ B) ⊘ C).
I8. Superposing
lA ((f ⊗ g) · a) =lA f ⊗ g for f : (A ⊗ A) ⊗ B, g : C,
where a : ((A ⊘ A) ⊘ B) ⊘ C → (A ⊘ A) ⊘ (B ⊘ C).
I9. Trace swapping
lB (lA (f · a)) =lA (lB (f · ((cAA,BB ⊘ 1C ) • a
′
)) for f : ((A ⊗ A) ⊗ (B ⊗ B)) ⊗ C,
where a : ((A ⊘ A) ⊘ (B ⊘ B)) ⊘ C → (A ⊘ A) ⊘ ((B ⊘ B) ⊘ C),
and a
′
: ((B ⊘ B) ⊘ (A ⊘ A)) ⊘ C → (B ⊘ B) ⊘ ((A ⊘ A) ⊘ C).
The collection of axioms I1, ..., I9 will be denoted by IM.
Let M = (O,M) and M′ = (O′
,M′
) be indexed monoidal algebras. An indexed
monoidal homomorphism h : M → M′ maps each object A in O to an object hA in
O′ and each morphism f : A in M to a morphism hf : hA in M′
, so that h defines
a τ -algebra homomorphism O → O′ between the object structures. Furthermore, the
following homomorphism conditions are met by the morphisms.
(i) h(f · ρ) = (hf) · hρ;
(ii) h(f ⊗ g) = hf ⊗ hg;
(iii) h(lU f) =lhU hf ;
(iv) h1A = 1hA.
Notice that the symbol h has three different meanings in (i)–(iv) above: hA, hf, and
hρ. In hρ, h stands for the unique strict monoidal functor ΠO → ΠO′ determined by h
on objects. The category IMA consists of all indexed monoidal algebras as objects and
indexed monoidal homomorphisms as morphisms.
We now introduce our second motivating example, the single-sorted indexed monoidal
algebra of open undirected graphs.
Example 4.1. An open graph (Bartha and Kr´esz 2003) is an undirected multigraph
(Lov´asz and Plummer 1986) with some of its vertices distinguished and labeled, each
with a different label. In the present discussion we assume that the distinguished vertices
have degree 1. We call these vertices, as well as the edges incident with them, external.
If the number of external vertices in a concrete graph G is n, then we use the numbers
in [n] to label them. Graph G is then of rank n, that is, G : n. Two open graphs are
isomorphic if they are such as ordinary graphs by an isomorphism that preserves the
labeling of the external vertices.
Clearly, the algebra O in this example is the monoid (N, +, 0), which is isomorphic to
Structure of Turing machines 13
{1}
∗
. Thus, the example is single-sorted and strict. The operations are interpreted on
morphisms (open graphs) in the following way.
— For a graph G : n and permutation symbol ρ : n ⇒ n, G · ρ is the graph obtained
from G by relabeling its external vertices according to the ordinary permutation n → n
determined by ρ in the strict single-sorted monoidal category of permutations. This
permutation is the unique maximal refinement of ρ, which exists in the single-sorted
setting.
— For graphs G : n and H : m, G ⊗ H : n + m is the disjoint union of G and H with the
labels of H’s external vertices incremented by n.
— For every n ∈ N, the graph 1n : n + n consists of n edges connecting external vertex
i with n + i for each i ∈ [n].
— For a graph G : n + n + m, ln G is constructed from G by gluing together the pairs
of external edges ending in external vertices i and n + i for each i ∈ [n], discarding
the vertices i and n + i themselves. As part of this procedure, whenever a number of
external edges are glued together in a cycle with no intercepting internal vertices, a new
isolated vertex is added to the graph. Finally, the label of each remaining external vertex
is decremented by 2n. See Fig. 5 for the implementation of l3 on a graph G : 3 + 3 + 1.
1 1
2
3 6
7
5
4
3
( ) =
Fig. 5. Taking the trace of a graph.
The algebra of open graphs defined in this way is denoted by G. The reader can easily
verify that G is an IMA. The algebra of relations described in Section 2 is also indexed
monoidal over sets as objects and pairs of sets (A,R) as morphisms such that R ⊆ A.
Clearly, the rank of (A,R) is A.
4.2. Equivalence of the categories IMA and SDCC
The two examples discussed in the previous subsection served as models for the definition
of indexed monoidal algebras. The graph example reflects a “coproduct” philosophy
regarding the tensor ⊗, whereas in the relation example tensor is clearly product-oriented.
In neither of these examples will, however, tensor become coproduct or product in the
corresponding SDCC category.
Theorem 4.1. The categories IMA and SDCC are equivalent.
Proof. Let M = (O,M) be an IMA, and define the monoidal category C = SM
over the objects O as follows. Morphisms A → B and identities in C are exactly those
M. Bartha 14
in M, while composition and tensor are adopted from M as derived operations. For
a morphism f : C and objects A,B such that C = A ⊗ B, one may want to use the
distinction fA,B : A → B to keep different hom-sets disjoint. We shall return to this
foundational issue shortly. Symmetries cA,B : A ⊗ B → B ⊗ A in C are the morphisms
1A⊗B ·(1AB ⊘cA,B). In general, every permutation symbol ρ : u → v is represented in SM
as 1|u|
·(1u ⊘ρ) : |u| → |v|. For each self-adjunction A ⊣ A, the unit map dA : I → A⊗A
and the counit map eA : A ⊗ A → I are 1A · l
−1 and 1A · r
−1
, respectively, where l and
r are the appropriate left and right unitors.
It is essentially routine to check that SM is an SDCC category. Some of the details,
however, require a careful organization. We start out with a few immediate consequences
of the axioms IM.
J1. Symmetry of trace
lA f =lA (f · (cA,A ⊘ 1B)) for f : (A ⊗ A) ⊗ B.
See Fig. 6.
f
1
↔ ↔
A A B
A A
↔ ↔
↔ ↔
↔ ↔
↔
↔
↔
=
= =
=
Fig. 6. Symmetry of trace.
J2. Canonical trace
lA f = ((1A · l
−1
) ◦I,(A⊗A),B f) · l
′
for f : (A ⊗ A) ⊗ B,
where l and l
′ are appropriate left unitors.
See Fig. 7. Notice that in the diagrams of Figures 6 and 7 we rely heavily on axiom I3
(coherence), taking refinements of permutation symbols whenever this becomes necessary.
For technical simplicity we work in the strict monoidal setting in these diagrams. The
reader can easily fill in the parentheses and the corresponding association symbols to
make these diagrams work in the non-strict case.
J3. Left identity
1A ◦ f = f for f : A → B.
See Fig. 8. Note that the symmetry of trace and that of 1A have both been used in the
Structure of Turing machines 15
f
A A B
1
A A
↔ ↔ ↔
↔ ↔
= = =
=
Fig. 7. Canonical trace. Continue on Fig. 6 bottom.
A A
A B
↔
f
↔ ↔
↔
= = =
= = =
Fig. 8. Left identity.
proof.
J4. Tensor of identity
1A⊗B = 1A ⊗A,A,B,B 1B
See Fig. 9, and take f = 1A⊗B.
With regard to the category C, the monoidal axioms follow trivially from I1, ..., I6,
J3, and J4. Proving the unit and counit property of dA and eA is equally easy. See e.g.
Fig. 10 for the first equation in Definition 2.2. In the understanding of this diagram, the
reader should follow the guiding principle that whenever either endpoint of an instance
of 1A is involved in the trace operation lA, that instance can be eliminated from the
diagram by “yanking”. This rule is a direct consequence of axioms I5, I6, and J3 in the
light of the indexing mechanism. The definition of functor S on (homo-)morphisms is
evident, and left to the reader.
Conversely, let C be an SDCC category having the object structure O. Define the IMA
f
↔
C A B
A A B B
↔ ↔ ↔
= = =
Fig. 9. Tensor of identity.
M. Bartha 16
= =
Fig. 10. Proving SM compact closed.
f
A A B
e
fA
f
e
d
=
Fig. 11. Canonical trace again, cf. Fig. 2.
M = IC as follows. For each object A, the morphisms of rank A are those in C(I,A).
Since C is symmetric, every permutation symbol ρ : A ⇒ B determines a permutation
ρC : A → B in C. Then, for f : A, define f · ρ = f ◦C ρC. Observe that indexing indeed
becomes the restriction of the covariant hom-functor to permutations, as intended. For
f : A and g : B in M, let
f ⊗M g = l
−1
◦ (f ⊗C g) : I → A ⊗ B,
where l is the unitor I ⊗ I → I. As to the identities, let 1A = dA : I → A ⊗ A. For
f : (A⊗A)⊗B, lA f is defined as the canonical trace of the morphism fA : A⊗I → A⊗B
in C that corresponds to f ◦ aA,A,B according to compact closure. That is, lA f is the
morphism
f ◦ (eA ⊗ 1B) = (dA ⊗ 1I ) ◦ a1 ◦ (1A ⊗ fA) ◦ a2 ◦ (eA ⊗ 1B) : I → B
in C with the associators a1 and a2 as appropriate. See Fig. 11.
In the light of this translation, each of the equations in IM is either a standard
monoidal category axiom or has been observed in (Joyal et al. 1996; Kelly and Laplaza
1980) for traced monoidal or compact closed categories. Thus, M is an IMA. The definition of functor I on morphisms (strict monoidal functors) is again straightforward.
By definition, I(SM) ∼= M via indexing by an appropriate unitor. On the other hand,
the only difference between the SDCC categories C and S(IC) is that the hom-sets A → B
in the latter are identified with the ones I → A ⊗ B of the former, using the natural
isomorphisms given by the self-adjunctions A ⊣ A. In other words, morphisms A → B
in S(IC) – as provided for by compact closure – are simply renamed as they appear in
Structure of Turing machines 17
G1
G2 G3
1 1 1
2
2
3
3
4 4
2
4 3
Fig. 12. Three simple graphs.
C(I,A⊗B). Thus, there exists a natural isomorphism between the functors 1SDCC and
SI, so that the categories IMA and SDCC are equivalent as stated.
Example 4.2. There are exactly three non-isomorphic open graphs: G1, G2, and G3 of
rank 4 in G which consist of external vertices only. These graphs are given in Fig. 12.
Clearly, G1 = 11 ⊕11 and G2 = 11 ⊗11 = 12. Graph G3 is simplest to put as c1,1 : 2 → 2
in the SDCC category SG. Along these lines it is instructive to find simple terms for
each graph as morphisms i → j (i + j = 4) in SG. For example, G1 = e1 ⊗ d1 : 2 → 2,
G2 = (11 ⊗ d1) ◦ (c1,1 ⊗ 11) : 1 → 3, and G3 = 11 ⊗ e1 : 3 → 1.
Example 4.2 illustrates the difficulty in interpreting SDCC-terms as open graphs and
vice versa. The graphical language (Joyal and Street 1991; Selinger 2009) for CC categories clarifies this translation procedure to some extent, but the root of the problem
is that the monoidal category language assumes (explicitly or implicitly) that hom-sets
are pairwise disjoint. According to Mac Lane (Mac Lane 1971), however, this is not a
necessary requirement if one uses the hom-set-based definition of categories. The domain
and codomain of a concrete morphism f are only fixed by the standard definition of
categories. Consider, for example, the CC category Int(C) freely generated by a traced
monoidal category C as described in (Joyal et al. 1996). This category has as objects pairs
(A,B) of objects in C. Morphisms (A,B) → (C,D) in Int(C) are the ones A⊗D → C⊗B
in C. Clearly, hom-sets will overlap in Int(C), unless one separates them by force. The
very same idea manifests itself in indexed monoidal algebras, reflecting the philosophy
that morphisms are fixed, while their domain and codomain may vary, depending on what
hom-sets we want to put them in. It is only the rank of a morphism that is uniquely
determined. This is the major difference between our graphical language, which is universal for IMA’s, and the one described in (Joyal and Street 1991; Selinger 2009) for CC
categories as symmetric autonomous categories.
A further remark on traced monoidal categories is also in order at this point, which will
explain the Int construction, too, through an intuitive argument. Consider the traced
monoidal category G~ of open directed graphs (flowcharts) as described in (Bartha 1987a).
In this category, a morphism n → m is an open graph G with n + m external vertices
such that each edge is directed in G. Accordingly, the external vertices are labeled by
either bi
, i ∈ [n] (begin vertices) if their out-degree is 1, or by exj , j ∈ [m] (exit vertices)
if their in-degree is one. As it was proved in (Bartha 1987a), G~ is freely generated by
the directed star graphs (boxes) n → m as a traced monoidal category. Since SG is
also traced with the canonical trace, the mapping that takes each directed star graph
M. Bartha 18
K~
n,m : n → m to the undirected star graph Kn+m : n + m (as a morphism n → m in
SG) can be extended in a unique way to a traced monoidal functor U. Clearly, G2 = U12
and G3 = Uc1,1. There is, however, no morphism in G~ that is mapped to G1 by U. It is
therefore misleading to say, at least in the categorical context, that G is obtained from
G~ by “forgetting” the direction of the edges in graphs.
Let us revise the above argument in the context of the Int construction. For each star
graph K~
n,m : n → m, divide the n in-degrees of the center vertex into n1 “real” in-degrees
and m2 “dual” out-degrees, so that n1+m2 = n. Symmetrically, divide the m out-degrees
into m1 real out-degrees and n2 dual in-degrees. Correspondingly, distinguish between
real and dual begin/exit vertices as appropriate, and say that K~
n,m is a morphism
(n1,n2) → (m1,m2) in the new setting. In general, a morphism (n1,n2) → (m1,m2)
in the arising category Int(G~) is just a morphism n1 + m2 → m1 + n2 in G~. When composing graphs in Int(G~), one will always connect real out-degrees with real in-degrees
and dual in-degrees with dual out-degrees simultaneously. Observe that this philosophy
requires composition in G~ on the real side and trace (feedback) on the dual side. See
(Joyal et al. 1996) for the details.
It follows from the general result in (Joyal et al. 1996) that Int(G~) is a CC category,
in which the dual of each object (n,m) is (m,n). Moreover, Int(G~) is freely generated
by G~. Regarding our SDCC category SG, it can be recaptured from Int(G~) by taking
the sub- (symmetric) monoidal category generated by the star graphs K~
k,k (k ≥ 1) considered as morphisms (n,n) → (m,m) with k = n + m. Clearly, this subcategory stays
within the scope of self-dual objects (n,n), n ∈ N, and it is isomorphic to SG. Heuristically speaking, the Int construction suggests that G is obtained from G~ by making the
edges in directed graphs bidirectional. This seemingly trivial observation has far-reaching
consequences in computer science with respect to the reversibility of computations. For
example, the bus connection between the processor and the memory in a von Neumann
computer is bidirectional. The interconnections between tape cells in a Turing machine
are bidirectional. We shall elaborate further on this point in Sections 6 and 7.
5. Coherence in indexed monoidal algebras
In general, a coherence result for some type µ of monoidal categories is about establishing
a left adjoint for a forgetful functor F from the category of µ-monoidal categories into an
appropriate syntactical category, and providing a graphical characterization of the free
monoidal µ-categories so obtained. For some typical examples, see (Mac Lane 1971; Kelly
and Laplaza 1980; Selinger 2009; Bartha 1987a; Bartha 1987b). We have also presented
two such results in Section 3 by constructing the monoidal categories ΛO and ΠO. In
this section we present a more substantial coherence theorem for SDCC categories, but
phrase it in terms of indexed monoidal algebras. The graphical language arising from
this result will justify our efforts in the previous section to reconsider SDCC categories
in the given algebraic context.
Our way of choosing the forgetful functor F differs from the method followed in (Kelly
and Laplaza 1980), where the category structure was still preserved. We go one step
further in forgetting, and preserve only the alphabet structure of morphisms. For a class
Structure of Turing machines 19
O of object variables, a ranked alphabet (signature) Σ = (O,M) consists of a class M
of morphism variables and a mapping M → Tτ (O) called rank. Again, by writing f : u
we indicate the rank of f. Recall that Tτ (O) is the class of object terms over O. An
alphabet mapping between ranked alphabets Σ = (O,M) and ∆ = (O′
,M′
) is a mapping
φ, which assigns to each object variable A in O an object variable φA and to each
morphism variable f : u in M a morphism variable φf : φu in M′
. Remember that φ also
stands for the unique strict monoidal functor ΠO → ΠO′ determined by φ. We say that
an alphabet mapping preserves the rank of morphism variables.
Observe that our perception of a ranked alphabet is completely in line with the concept of monoidal signatures found in (Joyal and Street 1991). By definition, a monoidal
signature consists of a set Σ0 of object variables, a set Σ1 of morphism variables, and a
pair of functions dom, cod : Σ1 → Mon(Σ0), where Mon(Σ0) is the set of object terms
over Σ0. Object terms in our sense are exactly the same as those in Σ0, and our morphism
variables are “one half” of the morphism variables in Σ1.
Every IMA M = (O,M) can trivially be considered as a ranked alphabet Σ = AM
in which the object variables are O and the morphism variables with rank u are simply
the morphisms in M of rank |u|. We use a subscript to distinguish between instances
of morphism f belonging to different ranks as morphism variables. If h : M → M′
is a
homomorphism, then Ah : AM → AM′
is the alphabet mapping φ by which φA = hA
and φfu = (hf)hu for every morphism f : |u| in M. Our aim is to provide a left adjoint for
the functor A. In algebraic terms this amounts to constructing the IMA freely generated
by Σ.
Let Σ = (O,M) be a ranked alphabet, fixed for the rest of this section. First we augment Σ by the following morphism variables.
— For each object term u, a symbol Hu : u.
— For each object variable A, a symbol ⊥A : I.
These symbols must not originally be present in Σ. Let ΣH denote the augmented alphabet. The IMA G(Σ) freely generated by Σ has as objects Tτ (O) (i.e., object terms over
O), and as morphisms Σ-graphs defined below. The τ -algebraic structure on objects is
the free one.
By a Σ-graph we mean a finite undirected and labeled multigraph G = (V,E,ℓ) with
vertices V , edges E, and vertex labeling ℓ. For each vertex v, the label ℓ(v) of v is a
morphism variable in ΣH. Exactly one vertex, called the host, is labeled by the symbol
Hu for some object term u, which term identifies the rank of G as a morphism in the
prospective IMA G(Σ).
Intuitively, G is a network of satellite machines of some sort, which are interconnected
with each other and the host as indicated by the edges in E. It is required that the label of
each vertex v be consistent with its degree d(v), so that d(v) = l(ℓ(v)). If ℓ(v) = u, then v
has a so called port associated with each path p ∈ path(u). (Recall the definition of paths
from Section 3.3.) Port p is variable/constant if p is such as a path. Each variable port
of vertex v identifies the point at which a unique edge impinges on v. To be meticulously
precise about the term “impinges on”, v is in fact a coherent group of port vertices, each
of which is labeled by a port identifier (i.e., path p). In addition, the whole group of ports
M. Bartha 20
^ ^
Fig. 13. A Σ-graph.
inA1
inAn
A1
A1
An
An
σ
. . .
1 n
. . .
1 n
Fig. 14. An atomic Σ-graph
is labeled by ℓ(v). Edges, too, must be consistent with the labeling in the sense that,
for each edge connecting two different ports p1 and p2 belonging to vertices labeled by
morphism variables of rank u1 and u2, respectively, obju1
(p1) = obju2
(p2).
See Fig. 13 for an example Σ-graph G : AB in the strict setting, where the morphism
variables f and g have ranks BA and ABA, respectively. The host itself is invisible
in the figure, only its ports are indicated as separate boxes, which we call channels or
interfaces. All other vertices, as well as the edges connecting them, are called internal.
Vertices labeled ⊥A are the loop vertices in G. Note that the idea of using a host vertex
originates from (Leiserson 1983).
Morphism variables in Σ are represented as atomic Σ-graphs (star graphs) in the way
depicted by Fig. 14. An isomorphism between Σ-graphs G,G′
: u is a graph isomorphism
that preserves the labeling information of the vertices. We shall not distinguish between
isomorphic graphs. The indexed monoidal algebra operations are defined on Σ-graphs as
follows.
— For a graph G : u, each Tτ (O)-permutation symbol ρ : u ⇒ u
′
is interpreted as the
relabeling of the interfaces (i.e., ports of the host) according to the O-permutation symbol
ΠOρ. Thechnically, this amounts to changing the label of the host together with the paths
associated with the variable ports of the host, and the possible deletion/introduction of
inAn
A1 An
n+1 2n
inA1
inA1
inAn
1 n
. . .
Fig. 15. The identity graph
Structure of Turing machines 21
constant ports within that vertex. See again Fig. 3.
— For graphs G1 : u1 and G2 : u2, G1 ⊗ G2 is obtained by taking the disjoint union of
the subgraphs of G1 and G2 induced by their internal vertices, joining the two hosts into
one labeled by Hu1⊘u2
, adjusting the port labels of the new host in the obvious way, and
connecting these ports to the respective ports in the internal copies of G1 and G2 in the
way these edges exist in G1 and G2 separately.
— The identity graph 1u : u⊘u is shown in Fig. 15 in the strict setting, whereby u is the
string A1 ... An. For a non-strict example, the graph 1I has the host as its sole vertex,
which has two constant ports. In general, port 1w is connected to port 2w within the
host, whenever these ports are variable.
— For a graph G : (u ⊘ u) ⊘ v with l(u) = n, the trace operation lu is defined by gluing
together each edge ending at interface labeled 11w with the edge ending at interface
labeled 12w for all existing variable paths w in the tree u, after detaching these edges
from the host. (The first 1 in 11w and 12w points to the left at the root of the tree
⊘(⊘(u,u),v)).) Whenever this procedure results in a loop of an even number of edges
(but no intercepting internal vertices) glued together, a new loop vertex labeled by ⊥A
is created and added to the graph, where A is the common object variable associated
with the interfaces involved in the loop. (That is, A = obj(u⊘u)⊘v(pi), where pi
is any of
the ports/paths involved.) Finally, the label of the host is changed to Hv and the port
labels are adjusted accordingly.
See again Fig. 5 and (Bartha and J¨urgensen 1989) for single-sorted examples. See also
(Elgot 1975; Bloom and Esik 1985; Bartha 1987a; Bartha 1987b) for the corresponding ´
standard definition of feedback/iteration in (directed) flowcharts. Interestingly, in all of
these works, graphs (flowcharts) are equipped with a single loop vertex, so that loops do
not multiply when taking the feedback. On the other hand, the loop vertex is present
in the graph 1I as well. Regarding the single-sorted case this amounts to imposing the
additional axiom l 11 = 10 (rather, its directed version, e.g. (Bartha 1987a, Axiom S5:
↑ 1 = 0)), which is not a standard traced monoidal category axiom. From the point of
view of axiomatization this is a minor issue. Another issue, namely the assignment of an
individual monoid to each object A is, however, extremely important and interesting. In
terms of flowcharts, this allows one to erase begin vertices and join two incoming edges
at any given port. See e.g. the constants 01 : 0 → 1 and ǫ : 2 → 1 in (Bartha 1987a;
Bartha 1987b). In (Joyal and Street 1991), these morphisms are called the co-unit and
the diagonal, respectively. MacLane (Mac Lane 1971) calls them η and µ. According to
(Selinger 2009) these are the erase and copy maps. These constants (morphisms) were
naturally incorporated in the axiomatization of schemes, both flowchart and synchronous.
Concerning undirected graphs, the presence of such morphisms with a “circularly symmetric” interface (e.g., a fan into three equivalent directions for the diagonal 2 → 1)
allows for an upgrade of ordinary edges to hyperedges, exactly the way it is described
in (Milner 2009) for bigraphs. The axiomatization of undirected hypergraphs as indexed
monoidal algebras will be presented in a forthcoming paper.
It is easy to check that the above interpretation of the indexed monoidal operations
M. Bartha 22
on G(Σ) satisfies the axioms IM. Technically, G(Σ) is the multi-sorted, non-strict, and
labeled counterpart of the trivial IMA G. Thus, G(Σ) is an IMA. It is also clear that G(Σ)
is generated by Σ, that is, by the collection of the atomic Σ-graphs. (See again Fig. 14.)
Indeed, every undirected graph can be reconstructed from its vertices as star graphs by
adding internal edges one by one using the trace operation. Our goal is to show that
G(Σ) is freely generated by Σ. In the proof we are going to follow the standard algebraic
technique of working in the totally free algebra of well-formed Σ-terms in order to take
the quotient of this algebra determined by the given set IM of identities. Observe that
the axioms IM are indeed identities, for each permutation symbol is an individual unary
operation in our framework.
Compared to the simple-looking graphical language for CC categories described in
(Selinger 2009), our concept of Σ-graph appears to be unduly meticulous. We believe it
is not. The distinction of ports and their labeling is necessary in order to clearly indicate
the flow of information in the graph, even in the strict case. Yet, a graph must remain a
graph in the standard combinatorial sense. Consider, for example, the two diagrams in
Fig. 16, both representing the identity morphism 1A∗ in Selinger’s graphical language.
Note that Selinger labels the edges by object variables, rather than distinguishing ports
and labeling those. The graphical language itself originates from (Joyal and Street 1991),
where the geometric aspects of such graphs are better explained. Selinger’s coherence
statement (Selinger 2009, Theorem 4.33) for CC categories says:
“A well-formed equation between morphisms in the language of CC categories
follows from the axioms of CC categories iff it holds, up to isomorphism of diagrams, in the graphical language.”
The isomorphism of the diagrams of Fig. 16 as “graphs” is far from being obvious. This
A* A
Fig. 16. Two diagrams for 1A∗ .
leads to an ambiguity in the understanding of the coherence statement regarding the
direction and labeling of the arrows in diagrams, which is not easy to clarify. For this
reason we do not rely on Selinger’s graphical language for CC categories in our coherence
statement. Another reason is that our concern is with indexed monoidal algebras and
not with SDCC categories in the first instance.
The structure T(Σ) = (T(Σ)u|u ∈ Tτ (O)) of well-formed indexed monoidal Σ-terms
(Σ-terms, for short) is the algebra (not IMA) freely generated from the morphism variables in Σ using the indexed monoidal operations and constants. Thus, T(Σ) is a totally
syntactical many-sorted term algebra in which the operations remain uninterpreted. Interpreting a Σ-term t in the algebra G(Σ) results in a Σ-graph |t|. Our goal is to show that
an equation t = t
′
in T(Σ) is provable from the axioms IM iff |t| = |t
′
|. The proof of this
statement will revisit the normal form construction in (Bartha 1987a) by adjusting it to
the present undirected environment in a straightforward manner. Part of this work has
already been done in (Bartha and J¨urgensen 1989). Without essential loss of generality
we restrict the proof to the strict single-sorted case. Working under this assumption, the
Structure of Turing machines 23
^ ^
Fig. 17. Normal form of Σ-terms.
subscript 1 in 11, l1, and ⊥1 =l1 11 will be understood and omitted. The notation l
l
stands for the l-fold application of l. The alphabet Σ collapses into an ordinary ranked
alphabet (Σn | n ∈ N).
Definition 5.1. (See (Bartha 1987a, Definition 5)) A Σ-term t : n (that is, t ∈ T(Σ)n)
is in normal form (n.f., for short) if
t =l
l
(((⊗
p
i=1σi) ⊗ (⊗
q
j=11)) · ρ) ⊗ (⊗
r
k=1⊥)
for some l,p,q,r ∈ N, σi ∈ Σmi
, and permutation (symbol) ρ such that:
1 n = m + 2q − 2l, where m = Σimi
, and
2 ρ : m + 2q → m + 2q is l-straight in the sense that:
(a) for each j ∈ [q], 2l < ρ(m + 2j − 1) < ρ(m + 2j); furthermore
ρ(m + 2j − 1) < ρ(m + 2j + 1), provided that j < q, and
(b) for each i ∈ [l], ρ
−1
(2i − 1) < ρ−1
(2i); furthermore
ρ
−1
(2i − 1) < ρ−1
(2i + 1), provided that i < l.
See Fig. 17 in the case l = p = q = r = 2, σ1 : 3, and σ2 : 2.
The intuitive meaning of Definition 5.1 should be clear. The only “ugly” technical
issue is the l-straight property of ρ, which is intended to resolve the ambiguity caused
by the symmetry of 1 (condition 2(a)) and that of trace (condition 2(b)). Obviously, no
occurrence of 1 is allowed to be involved in the trace operation, which condition is also
ensured by the first inequality of 2(a).
Now we prove the undirected counterpart of (Bartha 1987a, Lemma 6).
Lemma 5.1. For every Σ-term t : n there exists a Σ-term t
′
in n.f. such that t = t
′
is
provable from IM.
Proof. First we show that t can be transformed into a term t
′′ of the form
t
′′ =l
l
((⊗
m
i=1ai) · ρ),
where for each i ∈ [m], ai ∈ Σ or ai = 1. Such a term is said to be in weak normal form.
The proof of this statement uses a simple induction on the structure of t. The reader can
find the details of this argument in (Bartha 1987a, Lemma 5). Obtaining a n.f. t
′
from
a weak n.f. t
′′ then reduces to an easy exercise. One must rearrange the terms ai
in a
proper order using the symmetry of ⊗, eliminate occurrences of 1 that are involved in
the trace operation by yanking, and adjust ρ to be l-straight relying on the symmetry of
1 and that of trace.
Theorem 5.2. (See (Bartha 1987a, Theorem 1)) Let t and t
′ be Σ-terms such that
|t| = |t
′
|. Then the equation t = t
′
is provable from IM.
M. Bartha 24
Proof. By Lemma 5.1 we can assume that t and t
′ are in n.f. The normal form of
Σ-terms was defined in such a way that the only difference between t and t
′ may appear
in the order of the atomic Σ-terms occurring in ⊗m
i=1σi
. Discrepancies of this nature can,
however, be eliminated by the symmetry of ⊗ and the indexing mechanism.
Corollary 5.3. The algebra G(Σ) is freely generated by Σ.
Proof. Immediate by Theorem 5.2.
Let us return to the discussion following Example 4.2 to extend the remark made
there in connection with the Int construction. On the basis of that discussion one can
characterize the algebra G(Σ) as the IMA corresponding to an appropriate subcategory
C of the CC category Int(Sch(Σ)), where ¯ Sch(Σ) is the traced monoidal category of ¯
Σ-flowchart schemes described in (Bartha 1987a) over the doubly ranked alphabet ¯ Σ in ¯
which ¯σ ∈ Σ¯
n,n iff σ ∈ Σn. Unfortunately, one cannot use this characterization directly
to prove Corollary 5.3. Indeed, let M be an arbitrary (single-sorted) IMA. Then, due
to the universality of the Int construction and that of Sch(Σ) as a traced monoidal ¯
category, there is a unique strict monoidal functor from the CC category Int(Sch(Σ)) ¯
to SM extending any given rank-preserving mapping of Σ into ¯ SM. The problem is,
however, that the ports of the atomic Σ-graphs have been duplicated, so that the image
of a morphism (bidirectional flowchart) G~ from the subcategory C will also be a ⊗-
duplicated version of the morphism fG in SM that we intend to produce as the image
of the corresponding undirected graph G. Since there is no way to retrieve fG from
fG ⊗ fG, the method will eventually fail. This fallacy indicates that the construction of
the SDCC category freely generated by an arbitrary traced monoidal category is not a
simple matter, even with the Int construction in hand.
Definition 5.2. Let M be an arbitrary indexed monoidal algebra. An interpretation of
Σ in M is an alphabet mapping Ω : Σ → AM.
By Corollary 5.3, every interpretation Ω can be extended in a unique way to a homomorphism Ω : G(Σ) → M. Thus, G is indeed a left adjoint for the functor A.
6. Turing automata and Turing graph machines
As an important example of indexed monoidal algebras, in this section we introduce
the algebra of Turing automata and Turing graph machines. We shall use the monoidal
category (Bset, +) (sets and bijections with disjoint union as tensor) as the hypothetical
index category. For a set A, let A⋆ = A + {⋆}, where ⋆ is a fixed symbol, called the
anchor.
Definition 6.1. A Turing automaton (TA, for short) T : A is a triple (A,Q,δ), where
A is a set of interfaces, Q is a nonempty set of states, and δ ⊆ (Q×A⋆)
2
is the transition
relation.
Structure of Turing machines 25
.
.
.
.
.
.
L R
<1,L>
<n,L>
<1,R>
<n,R>
a)
a a
b)
Fig. 18. One tape cell in a Turing machine.
Two automata (A,Q,δ) and (A,Q′
,δ′
) are called isomorphic if there exists a bijection
χ : Q → Q′
such that
(χ × idA⋆
) ◦ δ
′ = δ ◦ (χ × idA⋆
).
Following (Katis et al. 2002), we shall be dealing with isomorphism classes of Turing
automata, while working practically with representatives.
Due to the discrete nature of the semantics involved we shall assume that the cardinality of the sets A and Q is at most countably infinite. The role of the anchor as a
distinguished interface will be explained later. The transition relation δ can either be
considered as a function Q × A⋆ → P(Q × A⋆) or as a function Q × (A⋆ × A⋆) → P(Q),
giving rise to a Mealy or Medvedev type automaton, respectively. We shall favor the
latter interpretation, and call ΘA = A⋆ × A⋆ the input alphabet for T. Then, as it is
customary in automata theory, the extension of δ to input strings in ΘA will be denoted
by δ, too.
By the standard definition, T is deterministic if δ is a partial function in the chosen
Medvedev sense. In contrast, T is strongly deterministic if δ is a partial function in the
Mealy way. Despite the Medvedev style formalism we still say that T has a transition
from a to b in state q, resulting in state r, if ((q,a),(r,b)) ∈ δ (that is, r ∈ δ(q,(a,b))).
According to this philosophy, if A is finite and |A| = n, then δ is an (n + 1) × (n + 1)
matrix, where each entry is a relation over Q. By way of duality, one can also consider
T as an automaton with states A⋆ and inputs Q × Q. We shall reflect on this duality
shortly.
Example 6.1. In Fig. 18a, consider the primitive model of one tape cell in a Turing
machine (TM, for short) M holding a symbol a. If M has tape alphabet Γ and states
{s1,... ,sn}, then we represent this cell by the TA C = (A + A,Q,δ), where A = [n],
Q = Γ, and δ is essentially the transition relation of M. See Fig. 18b. Recall that the
transition relation of M specifies one move of M as a 5-tuple (si
,a,sj ,b,D), where
a,b ∈ Γ and D is a direction, i.e., L or R. Spelling this out, if M is in state si and its
tape head is scanning a cell that holds symbol a, then M changes its state to sj , rewrites
the symbol a to b, and its tape head moves into direction D. Our interpretation δ of
one move also takes into account the direction where the tape head is coming from, but
this is a trivial issue not affecting the computational power of Turing machines. (Indeed,
one might consider states (si
,L) and (si
,R) in a variant of M.) The main point of this
analogy is the duality that states of M are viewed as interfaces (inputs) of C, whereas
tape symbols (inputs) of M are in fact states of C. To avoid confusion, we shall sometimes
refer to states of M as global states and to tape symbols as local states.
M. Bartha 26
According to the automaton C there are no transitions to or from the anchor ⋆. In
order to start the computation from somewhere, however, one must also consider an
extension C0 of C in which there are such transitions as well. The cell C0 will have a
unique instance in the middle of the two-way infinite array of cells representing M.
We now turn to defining the indexed monoidal algebra T of Turing automata. In this
algebra, morphisms are Turing automata T : A. A set-permutation symbol ρ : A ⇒ B is
interpreted as the relabeling of the interfaces according to the unique bijection A → B
determined by ρ in Bset. (Elaboration of details regarding the transition relation is left
to the reader.) Clearly, every bijection χ : A → B can be interpreted as a relabeling of
any TA T : A. It is therefore natural and inviting to write T · χ : B for the automaton so
obtained. Even though not all bijections χ in Bset are permutations, χ is still a trivial
one-state TA A → B (i.e., χ : A + B). Thus, T ◦ χ is going to be meaningful in the
algebra T , and its meaning will be identical to T · χ whenever χ is a permutation.
The tensor of T : A and T
′
: B having states Q and Q′
, respectively, is the automaton
T ⊗ T
′ = (A + B,Q × Q′
,δ ⊗ δ
′
), where δ ⊗ δ
′ ⊆ ((Q × Q′
) × (A + B)⋆)
2
is defined by
((q,q′
),x),((r,r′
),y)) ∈ δ ⊗ δ
′
iff either q
′ = r
′ and ((q,x),(r,y)) ∈ δ, or q = r and ((q
′
,x),(r
′
,y)) ∈ δ
′
. Notice the
ambiguity in writing just x,y rather than hx,Ai,hy,Ai or hx,Bi,hy,Bi. The definition,
however, applies to the case x = ⋆ and/or y = ⋆, too, so that taking the tensor of T
and T
′ amounts to a selective performance of δ or δ
′ on Q × Q′
. By the same token,
self-transitions of ⋆ according to T ⊗ T
′ are the “union” of such transitions in T and
T
′
(i.e., either one by T or one by T
′ nondeterministically on the product state space,
always leaving the other component unchanged).
The operation ⊗ can be naturally generalized to an infinite number of operands, provided that we lift the declared requirement on the cardinality of the set of states, or
make the additional requirement that only a finite number of operand automata have
more than a single state. Let {Ti
: Ai
|i ∈ I} be a family of Turing automata for some
finite or countably infinite set I. Then ⊗i∈ITi
:
P
i∈I Ai
is the automaton having states
×i∈IQi such that its transition function ⊗i∈I δi
is the selective performance of one of the
δi
’s on ×i∈IQi
in the above sense.
The identity Turing automaton 1A : A + A has a single state in which there is a
transition from ha, 1i to ha, 2i and back for every a ∈ A. There are no transitions to or
from the anchor.
The definition of lA T for a TA T : (A + A) + B is complicated but natural, and
it holds the key to understanding the TM-like behavior of this automaton. Intuitively,
the definition models the behavior of loops in flowchart algorithms (Elgot 1975) when
implemented in an undirected environment. That is, control enters lA T at an interface
b ∈ B, then, after alternating between corresponding interfaces in A + A any number
of times, it leaves at another (or the same) interface b
′ ∈ B. State changes are traced
interactively during this process.
Formally, let u = p1 ... pn ∈ Θ∗
(A+A)+B
be a non-empty input string for T, where
pi = (xi
,yi), n ≥ 1. The string u is called A-alternating from x1 to yn (u : x1 → yn, for
Structure of Turing machines 27
short) if for every i ∈ [n − 1] there exist ai ∈ A and j ∈ {1, 2} such that
yi = hai
,ji and xi+1 = hai
, 2 − ji.
Then, for every state q ∈ Q and input p = (b1,b2), the transition function ˆδ of lA T is
defined by
ˆδ(q,p) = ∪(δ(q,u)|u : b1 → b2 is A-alternating).
The specification of ˆδ in terms of matrices is yet more elegant. Let us assume that
A and B are both finite, containing n and m elements, respectively. Write the square
matrix δ : 2n + m + 1 × 2n + m + 1 in the form
µ
L11 L12
L21 L22 ¶
,
where L11 : 2n × 2n, L12 : 2n × m + 1, L21 : m + 1 × 2n, and L2,2 : m + 1 × m + 1. Then
ˆδ = L22 + L21 ⊙ L
⊛
11 ⊙ L12. (1)
In the underlying semiring R = P(Q × Q), addition (+) is ∪ and multiplication (·) is
composition of relations. The symbol ⊙ denotes alternating matrix product, so that for
any two matrices V1,V2 : n ×
U ⊙
µ
V1
V2
¶
= U ·
µ
V2
V1
¶
.
The operation ⊛ is alternating Kleene star in the following sense. For a 2n×2n square
matrix U:
U
⊛ =
X
n≥0
U
n
,
where U
0
is the alternate identity matrix
µ
0 In
In 0
¶
,
and U
i+1 = U
i ⊙ U.
It is easy to see that the two definitions of ˆδ coincide. The matrix L
⊛
11 captures sequences of transitions alternating between corresponding interfaces in A+A. The Kleene
formula (1) for ˆδ is then self-explanatory.
The classical Kleene formula is one of the earliest findings in automata theory. In its
original form it was introduced by Kleene (Kleene 1956) in the construction to convert
a finite automaton to a regular expression. The emergence of this formula in the present
context is quite natural. We are dealing with graph semantics as purely sequential computations, and Kleene derived exactly the same kind of semantics from the transition graph
(directed, though) of a finite automaton in the form of a regular language (expression).
The reader familiar with iteration theories (Bloom and Esik 1993) will immediately ´
notice that our ⊛ is the undirected counterpart of the star operation in matrix iteration
theories. See also the example (Rel, +) in Section 6 of (Joyal et al. 1996), which originates
from (Bloom and Esik 1993), too. The star operation is also used in Girard’s (Girar ´ d 1989)
original formulation of the Geometry of Interaction; the execution formula, in particular.
M. Bartha 28
C 1
C0 C1
. . . . . .
Fig. 19. A TM as a TA.
On the one hand, the connection between matrix iteration theories and Turing automata
is not surprising, since the model of the star operation was also Kleene star for regular
sets. On the other hand, even if we define Turing automata as directed computational
devices by distinguishing between input and output interfaces and requiring that control
follows a strict input-output direction, the resulting traced monoidal category will not
be an algebraic theory, and therefore not a matrix iteration theory. The reason is the
different tensor, which complements the cascade product of automata as composition.
We shall return to this argument in Section 7.
Example 6.2. Consider the TA C = ([n] + [n], Γ,δ) described in Example 6.1 as one
tape cell of a TM M. Construct the TA
C
∞ = ⊗k∈ZCk :
X
k∈Z
([n] + [n]),
where Z is the set of integers and Ck = C whenever k 6= 0. The automaton C0 is an
extension of C by appropriate transitions involving the anchor. Finally, define
T =lZ (C
∞ · ρ) : ∅, (2)
where ρ is the composite of the restructuring bijection
X
k∈Z
([n] + [n]) →
X
k∈Z
[n] + X
k∈Z
[n]
and the left L-shift hhi,ji,Li 7→ hhi,j − 1i,Li, hhi,ji,Ri 7→ hhi,ji,Ri. See Fig. 19.
The automaton T could be viewed as a model of the whole TM M as a TA. In T,
the anchor has taken over the role of an initial global state, and final states in M have
been replaced by the requirement that all meaningful computations must return to the
anchor. The presence of final states in the definition of Turing machines is specific to
these machines being used as language acceptors. With this exclusive goal in mind,
every TA derived from a TM can trivially be made deterministic by our Medvedevstyle definition. Indeed, whenever the automaton wants to accept (or reject), it will
simply erase (blank out) each tape cell before exiting, so rendering the computation
automatically deterministic. This will always be feasible, since there were only a finite
number of non-blank tape cells (the string to be processed) at the beginning of the
computation. Non-halting computations do not matter, for they are not transitions of
the TA. A classical deterministic TM, in our language, is derived from a single-tape-cell
TA C that is strongly deterministic. Obviously, only the strong deterministic property is
closed under the indexed monoidal operations, which explains this anomaly.
Structure of Turing machines 29
There is one caveat, however, in the description of the automaton T in Example 6.2.
It has a continuum number of states. Therefore T cannot be a faithful representation of
the TM M. In order to overcome this problem one must adopt a denotational semantics
philosophy and consider a sequence Tl
, l ≥ 1 of approximations of T in which, for every
|m| ≥ l, Cm : [n] → [n] is the trivial one-state TA having no transitions at all. Intuitively,
the single state of this empty automaton is the blank, and whenever control reaches Cm,
the machine will crash. Then the “good” TA that we have in mind is the colimit of the
TA Tl according to the following vertical structure in T . For automata T = (A,Q,δ),
T
′ = (A,Q′
,δ′
), and relation φ : Q → Q′
, we write φ : T → T
′
if φ is a homomorphism
of monoidal automata (Bartha 2009), that is,
(φ × idA⋆
) ◦ δ
′ = δ ◦ (φ × idA⋆
).
Observe that, by definition, isomorphism of automata follows exactly this vertical structure as a category. Clearly, there are straightforward injective homomorphisms φl
: Tl →
Tl+1 (remember that the blank is present in Γ), and the colimit of the sequence φl exists
as a TA T∞ having a countably infinite number of states. It is this automaton T∞ that
models the TM M in a faithful manner.
On the analogy of our example SDCC category (Rel, ×), the vertical categories TA of
Turing automata of rank A, too, are SDCC categories. Hence they are indexed monoidal
algebras. We have thus “discovered” the concept of 2-indexed monoidal algebras, which
are the algebraic counterparts of 2-SDCC categories. We shall elaborate on this issue in
a separate paper. For now we only observe that the algebra T has much more structure
than what we could exhibit in this introductory paper.
The formula (2) above suggests that it is often convenient to express the operation
trace in terms of a one-to-one correspondence between two arbitrary disjoint subsets of
interfaces. Let T : D be a TA, A,A′ be disjoint equivalent subsets of D with a specified
bijection χ : A → A′
, and B = D \ (A ∪ A′
). Then lχ T is the TA lA T · ρχ, where
ρχ : D → A+A+B is the bijection determined by χ in the obvious way. The reader can
easily verify that lχ is compatible with relabeling, that is, if ω : A → C, ω
′
: A′ → C
′
,
and ψ : C → C
′ are bijections for some disjoint sets C,C′ also disjoint from B such that
ω ◦ ψ = χ ◦ ω
′
, then
lχ T =lψ T · (ω ∪ ω
′ ∪ idB).
In particular, lχ T =lχ−1 T. In this manner, the concept χ-alternating input string for
T is the obvious generalization of the original A-alternating one.
Lemma 6.1. Let T = (A ∪ A′ ∪ B,Q,δ) be a TA, and χ : A → A′ be a one-to-one
correspondence as above. For every b1,b2 ∈ B and q1,q2 ∈ Q,
((q1,b1),(q2,b2)) ∈ δχ iff ((q1,b1),(q2,b2)) ∈ δχ¯,
for a restriction ¯χ of χ to a finite subset of A, where δχ and δχ¯ are the transition relations
of lχ T and lχ¯ T, respectively.
Proof. Evident by the definition of trace in T , which describes a finite process.
M. Bartha 30
In terms of Turing machines, Lemma 6.1 expresses the trivial fact that each concrete
halting run of a TM affects a finite number of tape cells only.
Lemma 6.2. Let T = (A ∪ A′ ∪ B,Q,δ) be a TA and χ : A → A′ be a one-to-one
correspondence as in Lemma 6.1. Furthermore, assume that A = A1 ∪ A2, A′ = A′
1 ∪ A′
2
such that A1 ∩ A2 = ∅ and χ(Ai) = A′
i
. Then
lχ T =lχ2
(lχ1 T)
for the restrictions χi
: Ai → A′
i
of χ to Ai
.
Proof. Let δχ, δχ1
, and δχ2/χ1
denote the transition functions of lχ T, lχ1 T, and
lχ2
(lχ1 T), respectively. By definition,
δχ2/χ1 ⊆ δχ,
therefore we need only prove that for every state q ∈ Q and input (b1,b2) ∈ ΘB,
δχ(q,(b1,b2)) ⊆ δχ2/χ1
(q,(b1,b2)).
By Lemma 6.1 we can assume that A is finite. Then, using an appropriate induction
argument, we can also assume that A2 = {a} is a singleton.
Denote a
′ = χ(a), and let u = p1 ... pn be a χ-alternating input string for T from
b1 to b2 with pi = (xi
,yi). Isolate those indices ij ∈ [n], j ∈ [m] for which yij = a or
yij = a
′
, and concentrate on the substrings uj = pij+1 ... pij+1 , j ∈ [m−1], together with
the prefix u0 = p0 ... pi1
and the suffix um = pim+1 ... pn. Clearly, each uj : ¯xj → y¯j ,
0 ≤ j ≤ m is a χ1-alternating input string for T, where
x¯j =
½
b1 if j = 0
xij+1 if j ≥ 1,
and ¯yj =
½
b2 if j = m
yij+1 if j < m.
Similarly, the string ¯u = (¯x0, y¯0)...(¯xm, y¯m) is χ2-alternating for lχ1 T from b1 to b2.
Now let q
′ ∈ δχ(q,(b1,b2)). By definition there exists a χ-alternating input string
u = p1 ... pn for T from b1 to b2 such that q
′ ∈ δ(q,u). Let u be as above. Breaking this
string down to the substrings u0,... um, there exist states q = q0,q1,... ,qm+1 = q
′
for
which qj ∈ δ(qj−1,uj−1), j ∈ [m + 1]. Again by definition, this implies
qj ∈ δχ1
(qj−1,(¯xi−1, y¯i−1))
for each j ∈ [m + 1], therefore q
′ ∈ δχ2/χ1
(q,(b1,b2)).
Theorem 6.3. The algebra T of Turing automata is indexed monoidal.
Proof. At this point we can capitalize to a great extent on the simplicity of the axioms in IM. Indeed, each of these axioms, except for vanishing (I7) and trace swapping
(I9), holds naturally true in T . These two axioms, however, follow immediately from
Lemma 6.2. Observe that, analogously to the circuit model in (Katis et al. 2002), superposing (I8) holds up to isomorphism of automata only, which is why we had to work
with isomorphism classes of automata in the algebra T .
Finally, we explain the role of the anchor ⋆. We did not want all Turing automata
of rank I = ∅ to have no transitions at all, like the automata ⊥A =lA 1A, which all
Structure of Turing machines 31
coincide, having a unique state. (Consequently, the canonically traced monoidal category
ST of Turing automata also satisfies the additional “scheme” axiom ↑A 1A = 1I , which is
quite natural.) The anchor is a fixed interface that is not supposed to be interconnected
with any other, so that automata of rank I might still have transitions from ⋆ to ⋆.
Now let us return to the traced monoidal category B = (Rel, +) analyzed in Section 6
of (Joyal et al. 1996). Observe that the Kleene formula (1) in its non-alternating form
defines trace in that category, too. This is evident, because the restriction of B to finite
sets is equivalent to the matrix iteration theory of bit matrices. The CC category Int(B)
has been characterized in technical terms in (Joyal et al. 1996). Now we can show the
intuitive meaning of that structure, too. Indeed, it is not difficult to see that the restriction
of Int(B) to its self-dual objects (A,A) is isomorphic to the subcategory of ST consisting
of single-state Turing automata with no transitions to or from the anchor. We shall be
yet more explicit about this observation in Section 7.
The idea of working with a multiplicative and an additive tensor (i.e., × and +) at
the same time leads directly to the model of quantum Turing automata. The underlying
category in this model is changed from sets and relations to finite dimensional Hilbert
spaces with linear maps between them. The additive tensor is orthogonal sum, while
the multiplicative one is tensor product of Hilbert spaces. Linear maps, however, must
be confined to unitary ones in order to make the additive style trace operation work.
Quantum Turing automata have been introduced along these lines in (Bartha 2011).
In this paper we present a simpler, but still interesting model, which targets electronic
switching at the molecular level. This model has been studied for over a decade now on
the grounds of matching theory by the name soliton automaton, cf. (Bartha and Kr´esz
2010).
Example 6.3. The n-ary atomic switch is the Turing automaton An : [n] (n ≥ 1) having
states [n], so that
δ = {((i,j),(j,i))| 1 ≤ i 6= j ≤ n} ∪ {((i,i),(j,j))| 1 ≤ i 6= j ≤ n}
∪{((i, ∗),(i,j)),((i,j),(i, ∗)),((i, ∗),(i, ∗))|i,j ∈ [n]}.
For better readability, states, indicating a selected edge in an n-star graph, are written
in boldface. In addition, if n = 1, then ((1, 1),(1, 1)) ∈ δ.
Heuristically, the n-ary atomic switch captures the behavior of an atom in a molecule
having n chemical bonds to neighboring atoms. Among these bonds exactly one is double, and referred to as the positive edge in the underlying star graph. The mechanism
of switching is then clear by the definition above. The active ingredient (control) in
this process is called the soliton, which is a form of energy traveling in small packets
through chains of alternating single and double bonds within the molecule, causing the
affected bonds to be flipped from single to double and vice versa. See (Davidov 1985)
for the physico-chemical details, and (Dassow and J¨urgensen 1990; Bartha and Kr´esz
2003; Bartha and Kr´esz 2006) for the corresponding mathematical model. Note that, by
our definition above, whenever the soliton enters an atom with a unique chemical bond
(which must be double since n = 1), it bounces back immediately, producing no state
change.
M. Bartha 32
in 1
1 2 1 P M
Fig. 20. The von Neumann machine.
Let D be a non-empty set of data such that |D| is at most countably infinite. The
indexed monoidal algebra D-dilT of D-flow Turing automata is defined in the following
way.
— Morphisms of rank A are Turing automata T : D × A.
— Each permutation symbol ρ : A ⇒ B is interpreted as a bijection (relabeling) D×A →
D × B, which is basically ρT performed on blocks of size D in parallel.
— Tensor is adopted from T (assuming the identification of D × (A + B) with D × A +
D × B), (lA T)D-dilT =lD×A T, and the identities 1A are the identities 1D×A in T .
The notation D-dil originates from (Arnold and Dauchet 1978), where the magmoid
(single-sorted monoidal category) k-dilM was introduced for integer k and magmoid
M along these lines. Intuitively, a D-flow Turing automaton is a data-flow machine in
which data in D are passed along with each transition. Data appear disguised, however,
as “cloned” interfaces. In other words, computation remains unary, since technically the
control still does not carry information. Notice that the anchor does not emit or receive
any data. By virtue of Lemma 6.2 and Theorem 6.3, the structure D-dilT of D-flow
Turing automata is an indexed monoidal algebra.
Consider, for example, the scheme N of the classical von Neumann computer in Fig. 20
as a data-flow architecture. It consists of two interconnected single-sorted D-flow Turing
automata: the processor P : 2, and the memory M : 1. The processor is a real finite state
automaton, having state components like registers, the instruction counter, the PSW,
etc. The transitions of P are very complex. On the other hand, M has (practically)
infinite states, but its transitions are straightforward. The set D consists of all pieces of
information (data, control, and/or address) that can be transmitted along the bus line
between P and M in either direction. The operation of N need not be explained, and
it is clearly that of a D-flow Turing automaton. It is a very important point, however,
that the machine as a TA can do as much as we want in one step, that is, from the time
control enters port 1 of P until it leaves at the same port. For example, it can execute
one machine instruction stored in the memory, or even a whole program stored there.
In other words, semantics is delay-free. In present-day digital computers the desired
semantics is achieved by limiting the scope of what the machine can do in one step
through introducing clock cycles and delay. Theoretically speaking, undirected trace is
turned into directed feedback with delay (or, using an everyday language, recursion is
transformed into a loop), and computations become inevitably directed in a rigid way.
According to the original scheme N, however, they need not be, yet they could remain
universal.
The problem is that we are not able to build up either the processor or the memory
as a deterministic Turing graph machine from concrete physical atomic TA. The logical
gates and flip-flops used in sequential circuits to build up the processor and the memory
Structure of Turing machines 33
are not Turing automata. According to the resulting common digital architecture, the
computer itself is a pair of interconnected Mealy automata, that is, a synchronous system in the sense of (Leiserson 1983). In the characterization of synchronous systems as
monoidal automata (Bartha 1987b; Bartha 1992), the underlying monoidal category is
(a subcategory of) (Set, ×). Tensor of such automata is therefore related to the cartesian
product of sets and functions, as opposed to disjoint union of sets and relations according to the Turing automaton model. Composition of automata is cascade product, and
feedback is delayed. The monoidal structure of synchronous systems obeys exactly the
traced monoidal (alias scheme) axioms, except for yanking, which does not hold in these
categories. The trace (feedback) of the symmetry cU,U is the unit delay, or register ∇U .
See Section 7 for further explanation.
Regarding the speed of computation, the impact of introducing a fixed delay is significant. It is well-known in complexity theory that the time complexity of a TM is
comparable to that of an equivalent RAM (random access machine, i.e., von Neumann
machine) program. The reason is that, even though the processor and the memory are
not TA-like, control will only stay for a constant number of clock cycles in these components every time it enters either of them during the execution of a program. In other
words, complexity can be measured in the number n of times control passes through the
bus, which is a TA-like bidirectional interconnection. We must, however, multiply n by
at least the fixed length t of one clock cycle. No matter how small t is nowadays, it is still
very large in contrast to the speed by which particles could ideally travel in a delay-free
TA-like quantum computer as described in (Bartha 2011).
The soliton automaton model, introduced in Example 6.3 through several continuations, makes an attempt to at least partially solve the problem of implementing electronic
switching by Turing automata built up from atomic components.
Example 6.3 (Continued) The n-ary atomic alternating switch A2
n augments the ordinary n-ary atomic switch by the passing of a digital information in the following way.
Control from a negative interface (i.e., one not covered by the unique positive edge) can
only take 0 for input and emits 1 for output. (Remember that in the meantime the positive edge is switched from the output side to the input side.) Conversely, control from
a positive interface can only take 1 for input and emits 0 for output. Transitions to and
from the anchor are as in the corresponding 2n-ary switch.
For the rest of the paper, the alphabet Σ will be single-sorted, that is, Σ = (Σn | n ≥ 0).
Definition 6.2. A D-flow Turing graph machine over Σ is a triple M = (G,D, Ω),
where G is a Σ-graph, D is a nonempty set (at most countably infinite), and Ω is an
interpretation of Σ in T under which the single sort of Σ is mapped into D. Equivalently,
Ω is an interpretation in D-dilT that maps sort “1” to object {1}.
Intuitively, machine M comes with an underlying graph G that has a D-flow Turing
automaton sitting in each of its internal vertices. The operation of M as a complex Turing
automaton is uniquely determined by the given interpretation according to the homomorphism Ω. The classical (bounded) Turing machine concept can again be recaptured
by taking Σ = Σ2 = {c}, where c stands for “tape cell”. A Turing machine TM is trans-
M. Bartha 34
formed into a D-flow Turing graph machine M whose underlying graph is a linear array
of cell vertices with the following interpretation Tc : 2 of c. The states of Tc are the tape
symbols (local states) of TM, and, by way of duality, elements of D are the global states
of TM. The transition relation of TM translates directly and naturally into that of M,
using duality. The only shortcoming of this analogy is the finiteness of the underlying
graph G. Notice that it is not possible to specify the TA lZ C∞ in equation (2) as a
Turing graph machine, since the graph structure as generated from the ranked alphabet
Σ is necessarily finite. To extend the syntax to infinite graphs, one may consider the
standard subgraph relationship as a partial order and make it closed for taking limits.
Then the well-known denotational technique can be adopted to extend the semantics Ω
to infinite graphs. Of course, one is normally interested in infinite graphs of a certain
“regular” pattern, which graphs could be generated by appropriate grammars. See e.g.
(Bauderon and Courcelle 1987; Engelfriet and Vereijken 1997). Semantics could then be
carried over to such grammars in the algebraic way.
Example 6.3 (Continued) Let Σ be the ranked alphabet consisting of a single symbol
cn for each rank n ≥ 1. A pre-soliton automaton is a Turing graph machine
S = (G, {0, 1}, Ω),
where Ω is the fixed interpretation that sends each symbol cn into the n-ary atomic
alternating switch A2
n
. Since the interpretation is fixed, we shall identify each pre-soliton
automaton with its underlying graph. Moreover, since A2
n
is circularly symmetric, we do
not need to order the ports (degrees) of the internal vertices. Thus, G is an ordinary
open undirected graph as described in Example 4.1.
Let q be a state of graph G. By definition, each internal edge e ∈ E is either consistent
with respect to q, meaning that e has the same sign (positive or negative) viewed from
its two internal endpoints, or inconsistent if this is not the case. (Notice that a looping
edge is always negative if consistent.) A soliton walk from interface i to interface j
(i,j ∈ [n] + {∗}) is a transition of G from i to j in state q according to the standard
behavior of G as a Turing automaton. The reader can now easily verify that this definition
of soliton walks coincides with the original one given in (Dassow and J¨urgensen 1990),
provided that q is a perfect internal matching (Lov´asz and Plummer 1986; Bartha and
Kr´esz 2003) of G. In our language, a perfect internal matching is a state q of G such
that every edge of G is consistent with respect to q, and the positive edges determine
a matching by which the internal vertices are all covered. Indeed, the definition of A2
n
implies that the soliton can only traverse consistent edges in an alternating positivenegative fashion. A new feature of this model is a soliton walk from the anchor, which
must return to the anchor if q is a perfect internal matching, and in that case it defines
a closed alternating walk (e.g. an alternating cycle).
At this point we stop elaborating on soliton automata, referring the reader to the
recent study (Bartha and Kr´esz 2011). The key observation enabling the restriction of
the states of pre-soliton automata to perfect internal matchings is the Gallai-Edmonds
Structure Theorem (Lov´asz and Plummer 1986), well-known in matching theory. On the
basis of this theorem, the Gallai-Edmonds algebra of graphs having a perfect internal
Structure of Turing machines 35
matching has been worked out in (Bartha and Gomb´as 1991) as the homomorphic image
of the indexed monoidal algebra G. The IMA of soliton automata then turns out to be
an appropriate quotient of the Gallai-Edmonds algebra. The reader can find yet more
information on soliton automata in (Kr´esz 2007; Kr´esz 2008).
Finally, we return to our original dilemma of reversible vs. irreversible computations
and explicitly define the “reverse” of a Turing automaton T = (A,Q,δ) simply as T
R =
(A,Q,δ−1
). This definition makes T actually reversible only if δ : A × Q → A × Q is a
partial injection. It is known, cf. (Abramsky et al. 2002; Lutz and Derby 1982; Bennett
1973) that Turing machines with such a restricted capability still have sufficient power
to remain universal. Yet, the effective construction of a reversible universal Turing graph
machine poses an enormous challenge. The soliton automaton model described above is
an interesting try, but unfortunately it falls short of being universal even in terms of
designing individual ad-hoc machines.
7. Related work
The model of Turing automata has grown out of the study of general monoidal automata
(Bartha 1992; Bartha 2008; Bartha 2009), also known as circuits (Katis et al. 2002). The
category structure of circuits comes very close to that of Turing automata, but it is not
a traced monoidal category.
According to (Katis et al. 2002), a category with feedback is a monoidal category C
equipped with a feedback operation ↑
U
A,B: C(U ⊗A,U ⊗B) → C(A,B), which satisfies all
the requirements for TrU
A,B in Definition 2.1, except for the yanking axiom. Furthermore,
sliding holds in a weaker form only, when the morphism g : U → V is an isomorphism.
For an arbitrary monoidal category C a C-automaton A → B is a pair (U,α), where
U is an object and α : U ⊗ A → U ⊗ B is a morphism in C. The pair (U,α) typically
models a Mealy automaton, e.g., a sequential circuit. Reflecting this interpretation, the
object U is called the state component, while α is the combinational logic of (U,α). The
collection of C-automata can be given the structure of a category AutC equipped with a
tensor as follows. Objects, and tensor of them are as in C. Furthermore:
1A = (I, 1A);
(U,α) ◦ (V,β) = (U ⊗ V,(cU,V ⊗ 1A) ◦ (1V ⊗ α) ◦ (cV,U ⊗ 1B) ◦ (1U ⊗ β));
(U,α) ⊗ (V,β) = (U ⊗ V,(1U ⊗ cV,A + 1C ) ◦ (α ⊗ β) ◦ (1U + cB,V + 1D)).
See Fig. 21. The type of composition depicted in Fig. 21 is commonly known as the
cascade product of automata. The category C is embedded into AutC by the functor
A 7→ A, α 7→ (I,α). Feedback is defined in AutC as follows:
↑
V
A,B (U,α) = (U ⊗ V,α), where α : U ⊗ (V ⊗ A) → U ⊗ (V ⊗ B).
Two C-automata (U,α),(V,β) : A → B are isomorphic if there exists an isomorphism
γ : U → V in C such that
(γ ⊗ 1A) ◦ β = α ◦ (γ ⊗ 1B).
Isomorphism classes of C-automata are called circuits, and Circ(C) is the quotient of
M. Bartha 36
U V A
U V A C
B
U V C
U V B D
a
a
b
b
Fig. 21. Composition and tensor in AutC
AutC by this isomorphism. As it was proved in (Katis et al. 2002), tensor (of isomorphism
classes of C-automata) makes Circ(C) a monoidal category with the symmetry adopted
from M. Moreover, Circ(C) is a category with feedback, freely generated by C. In our
intuitive interpretation, feedback in Circ(C) turns interfaces (input/output channels)
into registers (state components). Note that, by the Mealy automaton interpretation of
(U,α), the duality principle with respect to input vs. state is not in effect. Inputs and
outputs are signals 0,1, and states are flip-flops/registers. It is also important to see that
the Int construction does not work for monoidal categories with a delayed feedback. One
cannot reverse the unit delay into a “negative” one. Designing a reversible computation
model strictly on the principle of Mealy automata as input-output devices is therefore
not possible.
As a comparison, we now build up the indexed monoidal algebra of Turing automata
from appropriate monoidal ones, which we call directed Turing automata. Feedback
(trace), as well as tensor, follows an entirely different philosophy in directed Turing
automata, even though composition between them is still the old cascade product. Trace
only cuts interfaces in a Turing automaton without creating registers from them. The
state component of the resulting automaton remains the same object (set), but the transition relation (combinational logic) becomes a lot more complicated. Most importantly,
though, trace complements an additional additive tensor, rather than ⊗.
Formally, let C have a further additive tensor ⊕ such that ⊗ distributes over ⊕. In our
example category of sets and relations, ⊗ = × and ⊕ = +. Then, using the simplified
strict formalism, the Turing tensor of automata (U,α) : A → B and (V,β) : C → D is
(U,α) ⊠ (V,β) = (U ⊗ V,γ) : A ⊕ C → B ⊕ D,
where γ ≃ γ1 ⊕ γ2 is as follows:
γ1 = (cU,V ⊗ 1A) ◦ (1V ⊗ α) ◦ (cV,U ⊗ 1B) : (U ⊗ V ) ⊗ A → (U ⊗ V ) ⊗ B,
γ2 = 1U ⊗ β : (U ⊗ V ) ⊗ C → (U ⊗ V ) ⊗ D.
The natural isomorphism ≃ above is left-distributivity. The reader will recognize the
intuitive idea behind the Turing tensor ⊠: it is the selective performance of either α or β
over U ⊗V . The duality principle with respect to input and state does apply, therefore the
whole automaton concept requires an intuitive understanding that is completely different
Structure of Turing machines 37
from that of Mealy automata. The anchor as a distinguished interface is specific to the
relational Turing automaton model, and is missing from the present general discussion.
It is not difficult to show that the Turing tensor, too, turns the category of (isomorphism classes of) C-automata into a monoidal one Tur(C). Keep in mind, however, that
the object structure in Tur(C) has ⊕ for tensor, rather than ⊗ as in Circ(C). Now let
us assume that C admits an additive trace, too, by which (C, ⊕) is a traced monoidal
category. Then, under certain natural conditions, this trace can be carried over to the
category (Tur(C), ⊠), so that it becomes traced monoidal as well. This is definitely the
case with our example category C = (Rel, ×, +), so that the traced monoidal category T~
of directed Turing automata is well-established. As another example, directed quantum
Turing automata have been built up in (Bartha 2011) along these lines, using the base
category of finite dimensional Hilbert spaces and isometries between them with the two
tensors being tensor product and orthogonal sum. In the quantum example, the additive trace relies on the Moore-Penrose generalized inverse of linear operators, cf. (Roman
2005).
Given the traced monoidal category (Tur(C), ⊠), one can transform it into the CC
category Int(Tur(C)), restrict the scope of this category to self-dual objects (A,A), and
convert the resulting SDCC category to an IMA. It is exactly in this way that the IMA
T of (undirected) Turing automata can be derived from the traced monoidal category
T~ of directed Turing automata. In Section 6 we have deliberately chosen an accelerated
and more intuitive direct way of introducing the algebra T .
There has been a prior attempt by Hines (Hines 2003) to use the Int construction for
the purpose of deriving two-way finite state automata from ordinary ones and bounded
Turing machines from finite state Mealy automata. Regarding finite state automata, the
starting point is that the transition function of such a machine can be modeled as a
homomorphism Σ∗ → End(Q), where Σ is the input alphabet, Q is the set of states,
and End(Q) is the endomorphism monoid of the object Q in the monoidal category
(Rel, +). Actually this was not a new idea, since finite state automata (deterministic
or not) had been described before by the concept of presentations in matrix iteration
theories (Bloom and Esik 1993) in a more general way. The basic observation made ´
by Hines is that the transition function of a two-way automaton can be modeled by a
homomorphism Σ∗ → End(Ql
,Qr) with respect to the CC category Int(Rel, +), where
Ql and Qr are the left-moving and right-moving states, respectively. This idea is indeed
somewhat reminiscent of our construction of undirected Turing automata from directed
ones as outlined above, in the trivial special case when all automata have a single state.
Remember the duality by which (global) states of machines are interfaces, and inputs
(local states) are “the” states in Turing automata. In our language, literally, Hines’
observation constructs an individual Turing graph machine for each input string w ∈ Σ
∗
,
which consists of a linear array of cell vertices. Each cell is labeled by a letter of w in
a left-to-right way. The interpretation of any such a cell is then a single-state Turing
automaton, since the machine does not overwrite the symbol stored in the cell.
Moving on to Mealy automata and Turing machines, Hines did not observe the category
of monoidal automata with the cascade product as composition. Rather, he introduced his
own Comp construction, which produces a graded monoidal category from a monoidal
M. Bartha 38
category V; that is, Comp(V) is a functor from a monoidal category to the monoid
(N, +, 0), considered as a one-object category. See (Hines 2003) for the details. Intuitively,
the Comp construction aims at reconstructing the behavior of a Mealy/Turing machine
on an (n + m)-long tape from its behavior on an n-long and an m-long tape. Again,
the idea is similar to what we have accomplished with the algebra T , but the structure
involved is much less sophisticated. In his summary Hines himself admits: “ In particular,
we would like to be able to say that Int(Comp(V)) is isomorphic to Comp(Int(V)), but
giving this a precise meaning (or indeed, any meaning) appears to require substantial
algebraic input.” Indeed, Comp(V ) is not even a category, let alone monoidal or traced,
therefore Int(Comp(V)) is not meaningful.
8. Conclusion
We have provided a theoretical foundation for the study of Turing machines. In the
first half of the paper we established the categorical framework for this foundation. The
basic underlying structure, called indexed monoidal algebra, was introduced through a
small number of equational axioms, and it was shown that the category of such algebras is
equivalent to that of self-dual compact closed categories. A graphical language for indexed
monoidal algebras was worked out and a coherence theorem was proved to justify this
graphical language.
In the second half of the paper we defined the indexed monoidal algebra of Turing
automata, and showed how the classical Turing machine concept can be recaptured by
this definition. Turing graph machines were defined as interpretations of graphs in the
algebra of Turing automata. These machines have been generalized to D-flow Turing
graph machines, in which data from a set D are passed along with each transition. We
have pointed out that, at least at the high level of design, a von Neumann computer
architecture is a simple D-flow Turing graph machine. Finally we have presented the
soliton automaton example for switching at the molecular level by fully reversible binary
Turing graph machines.
9. Acknowledgment
The author is indebted to the anonymous referee, whose helpful comments have greatly
contributed to the improvement of the presentation.
References
Abramsky, S., Haghverdi, E. and Scott, P. (2002) Geometry of Interaction and Linear Combinatory Algebras. Mathematical Structures in Computer Science 12 625–665.
Abramsky, S. and Coecke, B. (2004) A categorical semantics of quantum protocols. in: Proceedings of the 19th Annual Symposium on Logic in Computer Science, LICS 2004, 415–425.
IEEE Computer Society Press.
Abramsky, S. (2005a) A structural approach to reversible computation. Thoret. Comput. Sci.
347, 441–464.
Structure of Turing machines 39
Abramsky, S. (2005b) Abstract Scalars, Loops, and Free Traced and Strongly Compact Closed
Categories. in: CALCO 2005, Springer Lecture Notes in Computer Science 3629 1–31.
Arnold, A. and Dauchet, M. (1978-79) Th´eorie des magmo¨ıdes. RAIRO Inform. Th´eor. Appl.
12 235–257, and 13 135–154.
Bartha, M. (1987a) A finite axiomatization of flowchart schemes. Acta Cybernetica 8 (2), 203–
217. Also available online at http://www.cs.mun.ca/∼bartha/linked/flow.pdf.
Bartha, M. (1987b) An equational axiomatization of systolic systems. Theoret. Comput. Sci. 55
265–289.
Bartha, M. and J¨urgensen, H. (1989) Characterizing finite undirected multigraphs
as indexed algebras. Technical Report No. 252, Department of Computer Science,
The University of Western Ontario, London, Ontario, Canada. Available online at
http://www.cs.mun.ca/∼bartha/linked/ind.pdf.
Bartha, M. and Gomb´as, E. (1991) The Gallai-Edmonds algebra of graphs. Technical Report ´
No. 9105, Department of Computer Science, Memorial University of Newfoundland, St. John’s,
NL, Canada. Available online at http://www.cs.mun.ca/∼bartha/linked/g-e.pdf.
Bartha, M. (1992) An algebraic model of synchronous systems. Information and Computation
97 97–131.
Bartha, M. and Kr´esz, M. (2003) Structuring the elementary components of graphs having a
perfect internal matching. Theoret. Comput. Sci. 299 179–210.
Bartha, M. and Kr´esz, M. (2006) Deterministic soliton graphs. Informatica 30 281–288.
Bartha, M. (2008) Simulation equivalence of automata and circuits. in: 12th International Conference on Automata and Formal Languages, Balatonf¨ured, Hungary 2008, Local Proceedings,
pp. 86–99. Available online at http://www.cs.mun.ca/∼bartha/linked/afl08.pdf.
Bartha, M. (2009) Equivalence relations of Mealy automata. in: First Workshop on Non-Classical
Models of Automata and Applications, Wroclaw, Poland, September, 2009, Local Proceedings,
pp. 31–45. Available online at http://www.cs.mun.ca/∼bartha/linked/wroc.pdf.
Bartha, M. and Kr´esz, M. (2010) Soliton circuits and network-based automata: review and
perspectives. In: C. Mart´ın-Vide, Ed. Mathemetics, Computing, Language, and Life: Frontiers
in Mathematical Linguistics and Language Theory, Vol. 2, Scientific Applications of Language
Methods, Imperial College Press, pp. 585–631.
Bartha, M. and Kr´esz, M. (2011) Molecular switching by Turing automata. in: Third Workshop
on Non-Classical Models of Automata and Applications, Milan, Italy, July, 2011, Local Proceedings, pp. 31–45. Available online at http://www.cs.mun.ca/∼bartha/linked/mol.pdf.
Bartha, M. (2011) Quantum Turing automata, submitted for publication. Available online at
http://www.cs.mun.ca/~bartha/linked/quant.pdf.
Bauderon, M. and Courcelle, B. (1987) Graph expressions and graph rewritings. Math. Systems.
Theory 20, 83–127.
Bennett, C.H. (1973) Logical reversibility of computation. IBM Journal of Research and Development 17 (6) 525–532.
Bloom, S. L. and Esik, Z. (1985) Axiomatizing schemes and their behaviors. ´ J. Comput. System
Sci. 31, 375–393.
Bloom, S. L. and Esik, Z. (1993) ´ Iteration Theories: The Equational Logic of Iterative Processes.
Springer-Verlag.
Burstall, R. M., Goguen, J. A. and Tarlecki, A. (1989) Some fundamental tools for the semantics of computation. Part 3: Indexed categories. Report ECS-LFCS-89-90, Laboratory for
Foundations of Computer Science, University of Edinburgh, Edinburgh, United Kingdom.
C˘az˘anescu, V. E. and S¸tef˘anescu, Gh. (1990) Towards a new algebraic foundation of flowchart
M. Bartha 40
scheme theory. Fundamenta Informaticae 13, 171–210. Also appeared as: INCREST Preprint
Series in Mathematics 43, Bucharest, Romania, 1987.
Dassow, J. and J¨urgensen, H. (1990) Soliton automata. J. Comput. System Sci. 40 154-181.
Davidov, A. S. (1985) Solitons in Molecular Systems. Reidel, Dordrecht.
D’Hondt, E. and Panangaden, P. (2006) Quantum weakest preconditions. Mathematical Structures in Computer Science 16 (3) 429–451.
Elgot, C. C. (1975) Monadic computations and iterative algebraic theories. In Logic Colloquium
1973, Studies in Logic and the Foundations of Mathematics 80 175–230. North Holland.
Engelfriet, J. and Vereijken, J. J. (1997) Context-free graph grammars and concatenation of
graphs. Acta Informatica 34 773–803.
Freyd, P. and Yetter, D. (1992) Coherence theorems via knot theory. J. Pure and Appl. Algebra
78 49–76.
Girard, J.-Y. (1987) Linear logic. Theoret. Comput. Sci. 50 1–102.
Girard, J.-Y. (1989) Geometry of Interaction I: Interpretation of System F. in: Logic Colloquium’88, ed. R. Ferro et al. North-Holland, pp. 221-260.
Girard, J.-Y. (1990) Geometry of Interaction II: Deadlock-free Algorithms. COLOG-88 (P.
Martin-Lof, G. Mints, eds.) Springer Lecture Notes in Computer Science 417, pp. 76–93.
Hines, P. (2003) A categorical framework for finite state machines. Mathematical Structures in
Computer Science 13 451–480.
Hyland, M. (1997) Game semantics. In A. M. Pitts and P. Dybjer (editors) Semantics and Logics
of Computation, 131–194. Cambridge University Press.
Joyal, A. and Street, R. (1991) The geometry of tensor calculus I. Advances in Mathematics 88
(1), 55–112.
Joyal, A., Street, R. and Verity, D. (1996) Traced monoidal categories. Math. Proc. Camb. Phil.
Soc. 119 447–468.
Katis, P., Sabadini, N. and Walters, R. F. C. (2002) Feedback, trace, and fixed-point semantics.
Theoret. Informatics Appl. 36 181–194.
Kelly, G. M. and Laplaza, M. L. (1980) Coherence for compact closed categories. J. Pure Appl.
Algebra 19 193–213.
Kelly, G.M. (1982) Basic Concepts of Enriched Category Theory. London Mathematical Society
Lecture Notes Series No. 64, Cambridge University Press.
Kleene, S. C. (1956) Representation of events in nerve nets and finite automata. In C. E. Shannon
and J. McCarthy (editors) Automata Studies, 3–42. Princeton University Press.
Kr´esz, M. (2007) Graph decomposition and descriptional complexity of soliton automata. Journal of Automata, Languages and Combinatorics 12 237–263.
Kr´esz, M. (2008) Soliton automata with constant external edges. Information and Computation
206 1126–1141.
Lafont, Y. (1989) Interaction nets. in: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pp. 95–108.
Landauer, R. (1961) Irreversibility and heat generation in the computing process. IBM J. Res.
Development 5 183–191.
Leiserson, C.E., and Saxe, J.B. (1983) Optimizing synchronous systems. J. VLSI Comput. Systems 1 41–67.
Lov´asz, L. and Plummer, M. D. (1986) Matching Theory. North Holland.
Lutz, C., and Derby, H. (1982) Janus: A Time-Reversible Language. Available online at
http://tinyurl.com/janus82.
Mac Lane, S. (1963) Natural associativity and commutativity. Rice Univ. Studies 49 28–46.
Mac Lane, S. (1971) Categories for the Working Mathematician. Springer-Verlag.
Structure of Turing machines 41
Mac Lane, S. and Par´e, R. (1985) Coherence for bicategories and indexed categories. J. Pure
and Appl. Algebra 37 59–80.
Milner, R. (2009) The Space and Motion of Communicating Agents. Cambridge University Press.
Roman, S. (2005) Advanced Linear Algebra. Springer-Verlag.
Selinger, P. (2009) A survey of graphical languages for monoidal categories. In B. Coecke (editor),
New Structures for Physics, Lecture Notes in Physics 813 Book chapter. Springer-Verlag.
EQUIVALENCE RELATIONS OF MEALY
AUTOMATA
Mikl´os Bartha
Memorial University of Newfoundland, St. John’s, NL, Canada
Email: bartha@mun.ca
Abstract
Two independent equivalence relations are considered for Mealy automata. Simulation equivalence, on the one hand, arises from a new multi-step simulation concept under the constraint
that the process of switching from one automaton to the other is reversible. Retiming equivalence, on the other hand, is the congruence induced by the so called sliding axiom in the
monoidal category of automata. It is shown that these two equivalence relations coincide, and a
characterization is given for simulation/retiming equivalence in terms of transition diagrams.
1. Introduction
Several definitions of simulation can be found in the literature, all of which are centered around
the algebraic concept of homomorphism between automata. Works devoting an extensive coverage to simulation and related issues include [2, 11, 13, 18], to name but a few major ones. In
this paper we take a new look at simulations between two Mealy automata in the light of some
recent results on monoidal categories with trace, feedback, and iteration. [1, 14, 15, 9, 8].
Let (U, α) and (V, β) be two Mealy automata with states U and V , respectively, and assume, for
simplicity, that the input A and output B are common for these automata. Then α : U × A →
U × B and β : V × A → V × B are functions or relations, depending on whether the automata
are deterministic or not. In the deterministic case, a homomorphism from (U, α) to (V, β) is a
function h : U → V such that α ◦ (h × idB) = (h × idA) ◦ β. The “monoidal” scheme of this
equation is shown in the diagram of Fig. 1a. The diagram is to be interpreted in the monoidal
category (Set, ×), that is, sets and functions with the tensor operation being cartesian product.
Interpreting the diagram in any symmetric monoidal category M yields the general notion of
simulation (machine morphism) between M-automata [11]. For example, if M = (Rel, ×),
that is, sets and relations with the tensor being product of sets and componentwise product of
relations, then we are looking at simulations between nondeterministic (relational) automata
[11]. For further examples of monoidal automata, see [11].
Simulations, as described above, are irreversible in nature. Once the switch has been made from
(U, α) to (V, β) using a simulation h, it is not possible to retrieve the original operation of (U, α)
by switching back from (V, β) using another simulation, unless h is an isomorphism. It is only
the input-output behavior of automata that is preserved by simulations. To be able to define
simulations that are reversible without being trivial, we introduce a look-back feature in the
2 Mikl´os Bartha
h
α
β
U A
V B
=
U A
V B
U
V
h
A
V B V B
s
U A U
U V
s
α
β
=
B
n
B
n
A
n
A
n
(a) (b)
Figure 1: Homomorphism and simulation between Mealy automata
process of switching. By this we mean that the switch from (U, α) to (V, β) is carried out by a
cross-transition function s : U ×An → V ×Bn
in n steps. For the monoidal scheme of switching
by s, see the diagram of Fig. 1b. Standing on this ground, one can already find simulations that
are reversible without being direct isomorphisms as machine (homo-)morphisms. The process
is as follows. Starting from (U, α), switch to (V, β) in n steps using simulation s. Then, after
at least m steps, switch back to (U, α) using an m-step simulation t. After the second switch,
run (U, α) for another fixed number of steps to allow full recovery. The operation of (U, α)
should then be exactly the same as if the switching back and forth did not happen. Automata
(U, α) and (V, β) are simulation equivalent if simulations s and t above exist, and the situation
is symmetric in t and s.
The idea of simulation equivalence originates from [16], where a similar concept was introduced
for synchronous systems. Synchronous systems are essentially structural Mealy automata [12],
which come with a scheme-like syntactical description in terms of registers and functional
elements, analogously to sequential circuits. Retiming a system means shifting its registers
around within the graph of the system in a certain regular way. Retiming is a fundamental
operation from the point of view of optimizing synchronous systems. If the number of registers
in the system is sufficiently large, then they can be rearranged by the help of retiming to produce
a systolic system, whereby at least one register can be found on the interconnection between any
two functional elements. The obvious advantage of having a systolic system is that the length
of the clock period for the system can be chosen minimum, i.e., as the maximal propagation
delay of the functional elements. See [16] or [7] for more details. An important observation in
[16] is that, whenever two systems are retiming equivalent, they can also simulate each other.
The exact relationship between retiming and simulation, however, was not addressed in [16].
Synchronous systems have since been studied as single-sorted symmetric monoidal categories
equipped with a feedback operation in a series of papers [3, 4, 5, 6, 7]. The main finding of [6]
is that two synchronous systems can simulate each other under the free interpretation of their
functional elements iff they are retiming equivalent. The present paper extends this result by
showing that retiming equivalence coincides with simulation equivalence for Mealy automata
in general, and provides a simple characterization of this equivalence in terms of transition
diagrams.
Equivalence of Mealy automata 3
On the basis of the synchronous system model, simulation equivalence was articulated in its
present form in [8], with the underlying structure being an arbitrary symmetric monoidal category M. The structure of simulation equivalent M-automata, Sim(M), has been constructed
as a quotient of the category Circ(M) [15] of isomorphism classes of M-automata in a suitable
2-categorical setting. We shall elaborate on this construction to some extent in Section 3 in
the special case M = (Set, ×).
Retiming a synchronous system is analogous to the concept of sliding [14, 15] in monoidal categories with feedback. This fact was pointed out in [8, 10]. Furthermore, parallel to the results
in [16, 6], retiming (sliding) equivalence always implies simulation equivalence, regardless of the
underlying category M. It is therefore of a significant theoretical importance to see if simulation equivalence coincides with retiming equivalence for any particular category M. A general
sufficient condition for this coincidence was worked out in [8], and the main result of the present
paper could in fact be interpreted as a test for that condition in the category (Set, ×). To
keep the paper self-contained, however, we shall not rely on the technically rather complicated
Sim construction in [8]. We shall not even assume much expertise in category theory, other
than familiarity with the basic definitions and understanding some simple monoidal diagrams
like the ones in Fig. 1, which should be straightforward. The reader might, however, learn a
lot about monoidal categories through the interpretation of these simple diagrams.
2. Simulation equivalence of Mealy automata
For the rest of the paper we shall assume that our Mealy automata are finite and deterministic. Even though the main result, namely that simulation equivalence coincides with retiming
equivalence, holds true for all Mealy automata, the proof becomes much more complicated
when the automata are infinite and/or nondeterministic. Therefore these cases will be dealt
with in a separate paper.
We shall use the notation (U, α) : A → B to identify an automaton with states U, input
A, output B, and transition function (including the output component) α : U × A → U × B.
Composition of functions, denoted by ◦, will follow the left-to-right application order in writing,
that is, (f ◦g)(x) = g(f(x)). Identity on set A will be denoted by idA, and πA,B : A×B → B×A
will stand for the symmetry [17] (a, b) 7→ (b, a).
Definition 2.1. For functions s : X ×A → Y ×C and t : Y ×B → Z × D, the cascade product
of s and t is the function casX,Y,Z(s,t) : X × A × B → Z × C × D defined as:
casX,Y,Z(s,t) = (s × idB) ◦ (idY × πC,B) ◦ (t × idC) ◦ (idZ × πD,C).
See Fig. 2a. When X, Y and Z are clear from the context, the subscript X, Y, Z will be omitted
from the notation. We shall also need the “twisted” version of cas defined by
sac(s,t) = (idX × πB,A) ◦ cas(s,t) ◦ (idZ × πC,D).
See Fig. 2b. It is easy to see that the binary operation of cascade product is associative. That
is,
casW,Y,Z((casW,X,Y (r, s),t) = casW,X,Z(r, casX,Y,Z(s,t))
4 Mikl´os Bartha
(a) (b)
A B
C D
X
Y
Z
s
t
B A
D C
X
Y
Z
s
t
Figure 2: The operations cas and sac
for all appropriate functions r, s,t. See Fig. 3 for a monoidal proof. Therefore, for any automaton (U, α) : A → B and integer k ≥ 1, α
k = cas(α, . . . , α) : U × Ak → U × Bk
is meaningful,
and it identifies the k-step transition function of (U, α).
W
X
Y
Z
2
B
s
t
r r
s
t
W
X
Y
Z
3
B
1
B
A
1
A 2
A 3
A 3
A 2
A
1
1
B 2
B
3
B
Figure 3: The associativity of cas
Definition 2.2. Let (U, α) and (V, β) be automata A → B. A simulation from (U, α) to (V, β)
is a function s : U × An → V × Bn
, n ≥ 0, such that casU,U,V (α, s) = casU,V,V (s, β). If n = 0,
then s is called immediate.
See again Fig. 1b. We shall write s : (U, α) → (V, β) to underline the intuition that simulations
are in fact 2-cells [17] in the category of automata. At this point the reader might wonder why
the output is included in the cross-transition s by Definition 2.2. Indeed, since the definition
implies cas(α
k
, s) = cas(s, β
k
) for any k ≥ 1 (e.g. for k = n), the outputs of s must be the same
as those of (U, α) in the first n steps. Thus, s is completely determined by its state transition
component U × An → V . The reason for still keeping the output component in s is to be in
line with [8, Definition 4.1], where × is just tensor, not necessarily product. This argument
highlights a different philosophy, compared to [11], regarding the interpretation of × in the
expression U × A → U × B. While we treat both occurrences of × in this expression simply
as tensor, Ehrig makes a distinction by separating the output function in the very definition of
automata. The two approaches can be synchronized, so that they both become special cases
Equivalence of Mealy automata 5
of machines in a general category by [18], but only if × is product in the underlying monoidal
category. For the details of this argument, see [4]. Also notice that our simulation concept is
quite different from weak simulation as defined in [2], whereby a move of the one automaton on
input a is simulated by the move of the other on a string f(a) determined by a fixed function
f : A → A∗
.
An immediate consequence of Definition 2.2 is the fact that, for every l ≥ 1, cas(α
l
, s) is also
a simulation (U, α) → (V, β). Intuitively, the only difference between s and cas(α
l
, s) is that
the latter “realizes” the same simulation l steps later than s. We say that s and cas(α
l
, s) are
indistinguishable. In particular, the identity simulation 1(U,α) = idU is indistinguishable from
α
l
for every l ≥ 0 (assuming the natural definition α
0 = idU ).
Definition 2.3. Simulations s, s
0
: (U, α) → (V, β) are indistinguishable, in notation s ≡ s
0
, if
there exist integers k, l ≥ 0 such that cas(α
k
, s) = cas(α
l
, s
0
).
Simulations can be composed using the cascade product. It is easy to see that, if s : (U, α) →
(V, β) and t : (V, β) → (W, γ) are simulations between automata A → B (with A and B fixed),
then cas(s,t) is a simulation (U, α) → (W, γ). Indeed:
cas(α, cas(s,t)) = cas(cas(α, s),t) = cas(cas(s, β),t) =
= cas(s, cas(β,t)) = cas(s, cas(t, γ)) = cas(cas(s,t), γ).
Composition of simulations is therefore associative. This composition, together with the identity simulations 1(U,α)
, defines the so called vertical category of Mealy automata. We shall
specify the horizontal structure of automata as a monoidal category in Section 3.
Definition 2.4. Automata (U, α) and (V, β) are simulation equivalent, in notation (U, α) ∼
(V, β), if there exist simulations s : (U, α) → (V, β) and t : (V, β) → (U, α) such that cas(s,t) ≡
1(U,α) and cas(t, s) ≡ 1(V,β)
.
Notice that the connection cas(s,t) ≡ 1(U,α) also allows for a recovery period in the operation
of the automaton (U, α) after the two subsequent switches s and t, as explained informally in
the Introduction.
Example Let A = {0, 1}, and consider the unit delay automaton ∇A = (A, πA,A) : A → A
with one of its variants 3A = (A0
, α) : A → A in Fig. 4, where A0 = {0, 0
0
, 1}. Since ∇A is
a homomorphic image of 3A, there exists an immediate simulation s : 3A → ∇A determined
by s(0) = s(00
) = 0 and s(1) = 1. Now let t(0, i) = (i, 0) for i = 0, 1, t(1, 0) = (00
, 1) and
t(1, 1) = (1, 1). It is easy to check that t is a simulation ∇A → 3A such that cas(s,t) = α and
cas(t, s) = πA,A. Thus, ∇A and 3A are simulation equivalent.
Heuristically, when switching from ∇A to 3A by simulation t, one looks back on the input. In
state 0, if the last input symbol was 0, then switch to state 0, otherwise switch to 0
0
. In state
1 always switch to state 1.
Example With I = {∅}, consider the trivial one-state automaton 1I = (I, idI ) : I → I
6 Mikl´os Bartha
0/0
1/1
0
1
1/0 0/1
0/0 0/0 0’ 0
1/1
1
0/1 1/0
1/0
Figure 4: The automaton ∇A and its variant 3A
and the two-state automaton 2I = (A, idA) : I → I, where A = {0, 1}. Even though these
two automata can trivially simulate each other, they are not simulation equivalent. Indeed,
Definition 2.4 would require an isomorphism between I and A, which is impossible.
Notice that 1I and 2I are not simulation equivalent as nondeterministic automata either, where
the simulations s and t in Definition 2.2 would come as relations, rather than functions. This
is rightfully so, because it is not possible to switch from 2I to 1I and back in any number of
steps without altering the original operation of 2I in an irreversible way.
3. Retiming equivalence of Mealy automata
In order to introduce the concept of retiming, we make a short digression towards the general
theory of monoidal categories with feedback, using Mealy automata as an illustrative example.
The reader is referred to [15, 8] for the precise definitions.
The collection of Mealy automata can be given the structure of a category Aut equipped with a
tensor operation ⊗ as follows. Objects are sets, and tensor of them is cartesian product of sets.
Composition and tensor of morphisms in Aut is the well-known serial and parallel composition
of automata [13]:
(U, α) · (V, β) = (U × V,(πU,V × idA) ◦ (idV × α) ◦ (πV,U × idB) ◦ (idU × β));
(U, α) ⊗ (V, β) = (U × V,(idU × πV,A × idC) ◦ (α × β) ◦ (idU × πB,V × idD)).
See Fig. 5. Identity on object A is the automaton 1A = (I, idA). The category Set is embedded
U V A
U V A C
B
U V C
U V B D
a
a
b
b
Figure 5: Composition and tensor in Aut
into Aut by the functor A 7→ A, α 7→ (I, α). (Recall that I = {∅} is the unit object of the
monoidal category (Set, ×)). Feedback is defined in Aut as follows:
↑
V
(U, α) = (U × V, α), where α : U × V × A → U × V × B.
Equivalence of Mealy automata 7
Isomorphism classes of automata are called circuits in [15], and Circ is the quotient of Aut
by isomorphism. As it was proved in [15], tensor (of isomorphism classes of automata) makes
Circ a symmetric monoidal category with the symmetry adopted from Set. We shall denote
isomorphism of automata by ∼=.
As it was proved in [8], simulation equivalence is compatible with the operations composition,
tensor, and feedback. Consequently, since ∼=⊆∼, the quotient of Aut by ∼ is in fact a quotient
of Circ, so that it is a monoidal category (with feedback). This category is denoted by Sim.
Building on the vertical structure of simulations described in Section 2, Sim can be enriched
to a 2-category [17]. In order for this, one needs a suitable horizontal composition and tensor
of simulations. Let s : (U, α) → (V, β) be a simulation between automata A → B and t :
(U
0
, α
0
) → (V
0
, β
0
) be a simulation between automata B → C, so that s and t take the same n
number of steps. Define
s • t = (idU × πU0
,An ) ◦ (s × idU0) ◦ (idV × πBn,U0 ◦ t).
See Fig. 6. According to [8, Proposition 3.8], s • t : (U, α) · (U
0
, α
0
) → (V, β) · (V
0
, β
0
) is a
simulation, which is called the horizontal composite of s and t. If s and t take different number
of steps, then one must choose indistinguishable representations for them that take the same
number of steps before composing these representations horizontally. It is easy to see that
in this way horizontal composition is well-defined on classes of indistinguishable simulations,
and it is associative. The identities for horizontal composition are determined by the trivial
function idI as an immediate simulation 1A → 1A for each set A. (Recall that 1A = (I, idA).)
Moreover, the interchange law
cas(s, s
0
) • cas(t,t
0
) ≡ cas(s • t, s
0
• t
0
)
holds for all appropriate simulations s,t, s
0
,t
0
.
B
n
U U'
V V'
A
n
C
n
B
n
s
t
Figure 6: Horizontal composition of simulations
Tensor of simulations can be worked out in the same way, and it comes with an analogous
interchange law with respect to the vertical composition cas of simulations. See [8] for the
details. Thus, groups of indistinguishable simulations as 2-cells extend Sim to a monoidal
2-category.
The sliding axiom [14] in a monoidal category with feedback is the following identity:
8 Mikl´os Bartha
g
f g
f
U
U B V B
A V A
V
U
=
Figure 7: The sliding axiom
↑
U
((g ⊗ 1A) · f) =↑
V
(f · (g ⊗ 1B)), where f : V ⊗ A → U ⊗ B, g : U → V.
See Fig. 7. We wish to emphasize that, unlike in our earlier monoidal diagrams, the boxes
in Fig. 7 represent automata rather than just functions. The sliding axiom does not hold in
Circ, but the equivalence that it stipulates is what we call retiming equivalence. More explicitly,
retiming equivalence is the quotient of Circ induced by the sliding axiom (identity) in the usual
algebraic sense. To explain the terminology, imagine that there is a register (delay element) on
the two round feedback lines in Fig. 7. In case the boxes are just functions, it is this assumption
that renders a sequential behavior to the cycles in the graphs at hand, turning functions into
real automata. These registers are de facto present in the graphs (schemes) of synchronous
systems as weights on the interconnections between functional elements, eliminating the need
to distinguish between “feedback” and “composition” lines. An elementary retiming step in a
synchronous system is shifting a layer of registers from the input side of a functional element
(box) to the output side, or vice versa. Shifting these registers is topologically the same as
sliding the box from one side of the registers to the other. Therefore the phenomenon sliding
is the exact replica of retiming. This is one of the major observations in [10], where the sliding
axiom was first used to model retiming of synchronous systems. The maneuver of shifting
registers through boxes is yet more conspicuous in Lemma 3.2 below.
Definition 3.1. Let (U, α) and (V, β) be automata A → B, and assume that there are functions
δ : V × A → U × B and η : U → V such that α = casU,V,U (η, δ) and β = casV,U,V (δ, η). Then η
is a retiming from (U, α) to (V, β), in notation, (U, α) →η (V, β).
See again Fig. 7, interpreting the boxes f and g as functions δ and η. It is routine to check that,
if automaton (V, β) results form (U, α) by general sliding (i.e., by applying the sliding axiom in
a left-to-right manner), then (V, β) can also be obtained from (U, α) by an appropriate retiming
η. Therefore no generality is lost with respect to retiming when using Definition 3.1 instead of
the sliding axiom. It is also easy to see that retiming is compatible with the operations serial
and parallel composition, and feedback. More precisely, if (U, α), (U
0
, α
0
), (V, β), and (V
0
, β
0
)
are appropriate automata such that (U, α) →η (V, β) and (U
0
, α
0
) →η
0 (V
0
, β
0
), then
(U, α) · (U
0
, α
0
) →η×η
0 (V, β) · (V
0
, β
0
),
(U, α) ⊗ (U
0
, α
0
) →η×η
0 (V, β) ⊗ (V
0
, β
0
),
and ↑
W (U, α) →η×idW ↑
W (V, β). Thus, the smallest equivalence relation containing retiming
is the congruence induced by the sliding axiom in Circ, that is, retiming equivalence. For this
Equivalence of Mealy automata 9
reason we shall also refer to the sliding axiom as the retiming axiom. Retiming equivalence will
be denoted by ∼r.
The following generalization of the retiming axiom will be crucial in Section 4.
Lemma 3.2. For any functions α : U × A → V × C and β : V × B → U × D,
(∇A ⊗ 1B) · (U, cas(α, β)) →πA,U ◦α (V, sac(β, α)) · (∇C ⊗ 1D).
Proof. Monoidal computation, sketched in Fig. 8. Recall that ∇A =↑
A πA,A = (A, πA,A) is the
unit delay automaton (register) A → A, and sac is the “twisted” version of cas. The proof
stands in all monoidal categories with feedback.
=
β
α
A XU
α
β
=
A XU
β
α α
β
A B
C D
V
U
U
β
α
β
α
β
α
β
α
A XU
β
α
η
V XC
V XC V XC
= =
= = = =
= =
β
α
Figure 8: The proof of Lemma 3.2
Evidently, if (U, α) →η (V, β), then η is a homomorphism, or, in our language, an immediate
simulation. One disturbing property of such retiming homomorphisms is that they are not composable. The composite of two retiming homomorphisms is just an ordinary homomorphism.
10 Mikl´os Bartha
Retiming homomorphisms, on the other hand, are trivially decomposable. If (U, α) →η (V, β),
and η = η1 ◦ η2 with η1 : U → W and η2 : W → V , then there exists an automaton (W, γ) such
that (U, α) →η1
(W, γ) →η2
(V, β). Clearly, γ = cas(η2, δ, η1), where δ : V × A → U × B is the
function specified in Definition 3.1. In particular, retiming homomorphisms have an epi-mono
factorization. It is therefore possible to study the impact of surjective and injective retimings
separately, and then synthesize the results.
The connection between the automata 3A and ∇A in Fig. 4 is a typical example of a surjective
retiming η : A0 → A, where η is the homomorphism s defined there. The effect of η is called
state fusion, described as follow. Two states u1 and u2 of automaton (U, α) can be picked for
fusion if α(u1, a) = α(u2, a) for all a ∈ A. In this case, u1 and u2 are merged into one state u,
so that all transitions arriving at u1 or u2 are redirected to u. The inverse operation of state
fusion is called state splitting. Now the effect of a general surjective retiming η is merging all
states of U that are mapped to the same element in V into one state by state fusion.
The following example illustrates the effect of an injective retiming.
Example Consider the automata 1A = (I, idA), A = (A,), and 
0
A = (A,
0
) A → A, where
A = {0, 1}, (0, i) = (1, i) = (0, i), 
0
(0, i) = (0, i), and 
0
(1, i) = (0, 0) for i = 0, 1. Clearly, A
can be obtained from 1A by state splitting. At the same time, 1A can be retimed into both A
and 
0
A by the injective retiming ∅ 7→ 0 : I → A.
On the analogy of the above example, observe that the effect of an injective retiming is that
of an isomorphism, together with the introduction of a number of inaccessible states (i.e.,
states having no transitions arriving at them), which come with arbitrary transitions not in
contradiction with these states being inaccessible. We shall say that a Mealy automaton is
accessible if it does not have inaccessible states. In general, a state u of a Mealy automaton
(U, α) is called run-out if it cannot be reached by any sufficiently long input string starting
from any state; otherwise u is permanent. Since our automata are finite, (U, α) is accessible
iff U does not contain run-out states. The restriction of (U, α) to its permanent states will be
denoted by (Up, αp).
Two states u, u
0 ∈ U are said to be retiming equivalent (in notation u ∼r u
0
) if u and u
0
are equivalent in the usual sense, and, furthermore, u and u
0 are taken to the same state by
(U, α) on every sufficiently long input string w. (Recall e.g. from [13] that state-equivalence of
automaton (U, α) is the largest equivalence ≈ on U such that whenever u ≈ u
0
, δ(u, a) ≈ δ(u
0
, a)
and λ(u, a) = λ(u
0
, a) hold for every input a ∈ A, where α = (δ, λ) is the decomposition of the
transition function to state transition and output.) Equivalently, using the n-step transition
function α
n
, α
n
(u, w) = α
n
(u
0
, w) for every sufficiently large n and input w ∈ An
. Automaton
(U, α) is called reduced if u ∼r u
0
implies u = u
0
for all states u, u
0 ∈ U. A minimal automaton
is one that is both accessible and reduced.
According to the characterization of retiming homomorphisms above, every automaton can be
transformed into a reduced one by a sequence of surjective retimings. Also, every automaton
can be made accessible by a sequence of inverse injective retimings. Moreover, a straightforward
Equivalence of Mealy automata 11
induction shows that if (V, β) can be obtained from (U, α) by a sequence of direct or inverse
retimings, then (Up, αp) and (Vp, βp) reduce to the same minimal automaton, which is unique
up to isomorphism. In this way we have proved the following characterization theorem.
Theorem 3.3. Two Mealy automata (U, α) and (V, β) are retiming equivalent iff the automata
(Up, αp) and (Vp, βp) reduce to the same minimal automaton.
For (U, α) : A → B, consider the automaton (U, α
k
) : A
k → B
k
. The following statement will
be used in Section 4. Its proof should be evident.
Proposition 3.4. Automaton (U, α) is accessible (reduced, minimal) iff (U, α
k
) is such for all
k ≥ 1.
4. Simulation equivalence coincides with retiming equivalence
We begin with the simple statement that retiming equivalence implies simulation equivalence.
Proposition 4.1. Let (U, α) and (V, β) be Mealy automata A → B. If (U, α) ∼r (V, β), then
(U, α) ∼ (V, β).
Proof. We can assume, without loss of generality, that (U, α) →η (V, β) for some retiming
η : U → V . Then α = cas(η, δ) and β = cas(δ, η) for an appropriate function δ : V ×A → U×B.
Consequently, cas(α, η) = cas(η, β) and cas(β, δ) = cas(δ, α), so that η and δ are simulations of
0 and 1 step, respectively. Moreover, cas(η, δ) = α ≡ 1(U,α) and cas(δ, η) = β ≡ 1(V,β)
, showing
that (U, α) ∼ (V, β).
Now we turn to the more challenging proof of ∼⊆∼r. The proof is preceded by two lemmas.
In these lemmas, the state and output components of the transition function α are denoted by
αstate and αout, respectively.
Lemma 4.2. Let (U, α) and (V, β) be accessible automata A → B. If (U, α
k
) ∼= (V, β
k
) for all
sufficiently large k by the same isomorphism ξ : U → V , then ξ is also an isomorphism from
(U, α) to (V, β).
Proof. We need to show that, for every u ∈ U and a ∈ A, ξ(αstate(u, a)) = βstate(ξ(u), a) and
αout(u, a) = βout(ξ(u), a). The latter equation being trivial, we deal with state transitions only.
Let v = ξ(u). Since (U, α
k
) and (V, β
k
) are accessible and isomorphic for any sufficiently large
k, there exist states uk ∈ U, vk ∈ V and input w ∈ Ak
such that ξ(uk) = vk, α
k
state(uk, w) = u
and β
k
state(vk, w) = v. Thus, for the input wa ∈ Ak+1
,
ξ(αstate(u, a)) = ξ(α
k+1
state(uk, wa)) = β
k+1
state(vk, wa) = βstate(v, a),
for ξ is an isomorphism between (U, α
k+1) and (V, β
k+1) as well.
Lemma 4.3. Let (U, α) and (V, β) be minimal automata A → B, and assume that there exists
an automaton (W, γ) with mappings η : A × U → W and χ : A × V → W such that
∇A · (U, α) →η (W, γ) ←χ ∇A · (V, β).
Then (U, α) ∼= (V, β) via an isomorphism ξ that is completely determined by the mappings η
and χ.
12 Mikl´os Bartha
Proof. By definition, ∇A · (U, α) = (A × U, αˆ), where for all a, a
0 ∈ A and u ∈ U,
αˆ((a
0
, u), a) = ((a, αstate(u, a
0
)), αout(u, a
0
)).
Similarly, ∇A · (V, β) = (A × V, βˆ). It follows that the automata ∇A · (U, α) and ∇A · (V, β)
are accessible, even though they are not minimal in general. Without loss of generality we
can assume that the mappings η and χ are onto. Indeed, accessible states are mapped into
accessible ones by any homomorphism. Furthermore, if w ∈ W is missed by either η or χ, then
w is inaccessible in (W, γ). Thus, we can concentrate on the surjective part of the epi-mono
factorizations of η and χ, which will have the same range.
We claim that for every u ∈ U there exists a unique v ∈ V such that
∀a ∈ A : η(a, u) = χ(a, v).
For, let u = αstate(u1, au) for some u1 ∈ U and au ∈ A. (Remember that (U, α) is accessible.)
Then there exists a pair (av, v1) ∈ A × V such that w = η(au, u1) = χ(av, v1). Set v =
βstate(v1, av). Now,
η(a, u) = η(αˆstate((au, u1), a)) = γstate(w, a) = χ(βˆ
state((av, v1), a)) = χ(a, v).
With regard to the uniqueness of v, assume that ∀a ∈ A : η(a, u) = χ(a, v¯) holds for some v¯ 6= v.
Then ∀a ∈ A : χ(a, v) = χ(a, v¯), so that for any a
0 ∈ A, βˆ
state((a, v), a
0
) = βˆ
state((a, v¯), a
0
).
(Remember that χ is a retiming.) This last equation implies that ∀a ∈ A : βstate(v, a) =
βstate(v¯, a), which contradicts (V, β) being reduced. The correspondence u 7→ v established
above is therefore a mapping ξ : U → V . A symmetric argument shows that ξ is a bijection.
Moreover, if u and v are corresponding states, then for all a, a
0 ∈ A:
αout(u, a) = αˆout((a, u), a
0
) = βˆ
out((a, v), a
0
) = βout(v, a), and
η(a
0
, αstate(u, a))= η(αˆstate((a, u), a
0
))= χ(βˆ
state((a, v), a
0
))= χ(a
0
, βstate(v, a)),
proving that ξ is an isomorphism. This isomorphism is completely determined by η and χ, as
required.
Corollary 4.4. Let (U, α) and (V, β) be minimal automata A×C → B, and assume that there
exists an automaton (W, γ) with mappings η : A × U → W and χ : A × V → W such that
(∇A ⊗ 1C) · (U, α) →η (W, γ) ←χ (∇A ⊗ 1C) · (V, β).
Then (U, α) ∼= (V, β) via an isomorphism ξ that is completely determined by the mappings η
and χ.
Proof. Consider the automata f · (U, α) and f · (V, β), where
f = (1A ⊗ ∇C) · (∇A ⊗ 1C) : A × C → A × C.
Observe that f ∼= ∇A×C by the isomorphism πA,C. (The category Circ is monoidal.) Furthermore, since retiming is compatible with the operations in Circ,
Equivalence of Mealy automata 13
f · (U, α) →η
0 (1A ⊗ ∇C) · (W, γ) ←χ0 f · (V, β)
holds with the retimings η
0 = idC × η and χ
0 = idC × χ. Consequently,
∇A×C · (U, α) →η
00 (1A ⊗ ∇C ) · (W, γ) ←χ00 ∇A×C · (V, β),
where η
00 = (πA,C ×idU )◦η
0 and χ
00 = (πA,C ×idV )◦χ
0
. The result now follows from Lemma 4.3,
because the condition
∀a ∈ A∀c ∈ C : η
00((a, c), u) = χ
00((a, c), v)
for all states u and v is equivalent to ∀a ∈ A : η(a, u) = χ(a, v).
We are now ready to prove the main result of the paper.
Theorem 4.5. Simulation equivalence of finite state Mealy automata coincides with retiming
equivalence.
Proof. Let (U, α) and (V, β) be simulation equivalent automata A → B. By Proposition 4.1
we need only show that (U, α) ∼r (V, β). In the light of Proposition 4.1 and Theorem 3.3 we
can assume, without loss of generality, that (U, α) and (V, β) are minimal. Consequently, the
automata (U, α
k
) and (V, β
k
) are also minimal for all k ≥ 1 by Proposition 3.4. According to
Lemma 4.2, it is therefore sufficient to prove that (U, α
k
) ∼= (V, β
k
) for all sufficiently large k
via a fixed isomorphism ξ.
By definition, there exist simulations s : (U, α) → (V, β) and t : (V, β) → (U, α) such that
cas(s,t) ≡ 1(U,α) and cas(t, s) ≡ 1(V,β)
. Spelling this out, we have:
(i) cas(α, s) = cas(s, β) and cas(β,t) = cas(t, α);
(ii) cas(α
k
, s,t) = α
n+m+k and cas(β
k
,t, s) = β
k+m+n
,
where s : U × An → V × Bn
, t : V × Am → U × Bm, and k is any sufficiently large integer.
Furthermore, cas(α
k
, s,t) = cas(s,t, α
k
) follows from a repeated application of (i) above. Using
Lemma 3.2, a short computation yields:
(∇An ⊗ 1Am+k ) · (U, α
n+m+k
) →η (V, sac(β
k+m, β
n
)) · (∇Bn ⊗ 1Bk+m ),
where η = πAn,U ◦ s. See Fig. 9 for the case n = m = k = 1. On the other hand, directly from
Lemma 3.2:
(∇An ⊗ 1Am+k ) · (V, β
n+m+k
) →χ (V, sac(β
m+k
, β
n
)) · (∇Bn ⊗ 1Bm+k ),
where χ = πAn,U ◦ α
n
. Observe that η and χ do not depend on k. Therefore, according to
Corollary 4.4, (U, α
k
) ∼= (V, β
k
) holds for all sufficiently large k via a fixed isomorphism ξ, which
completes the proof.
14 Mikl´os Bartha
U U V V V = = =
U U
U
V
V V
V V
U V
α
α
α α
s
t α
t
s
β
t
s
β
β
β
η
Figure 9: The proof of Theorem 4.5
5. Conclusions
We have given two different characterizations of the congruence relation induced by the sliding/retiming axiom in the monoidal category Circ of Mealy automata. First we introduced
simulation as an additional vertical structure in Circ, and defined simulation equivalence to
be a pair of isomorphisms in this vertical category. Operating with surjective and injective
retiming homomorphisms, we then showed that two automata are retiming equivalent iff their
restriction to permanent states can be reduced to the same minimal automaton. On the basis
of this result we proved that retiming equivalence coincides with simulation equivalence. In
this way we have also provided a semantics to the sliding axiom in certain monoidal categories
with feedback.
References
[1] ABRAMSKY, S., Retracing some paths in process algebras, in: U. Montanari, V. Sassone (Eds.),
Proc. CONCUR’96, Pisa, 1996, Lecture Notes in Computer Science 1119 (1996), 1–17.
[2] ARBIB, M.A., Theories of Abstract Automata, Prentice-Hall, Englewood Cliffs, N.J. 1969.
[3] BARTHA, M., An equational axiomatization of systolic systems, Theoretical Computer Science
55 (1987), 265–289.
[4] BARTHA, M., An algebraic model of synchronous systems, Information and Computation 97
(1992), 97–131.
[5] BARTHA, M., Foundations of a theory of synchronous systems, Theoretical Computer Science
100 (1992), 325–346.
[6] BARTHA, M., CIR ˇ OVIC, ˇ B., On some equivalence notions of synchronous systems, in: Z. Esik, ´
Z. Ful¨ ¨ op (Eds.), Proc. 11th International Conference on Automata and Formal Languages, Dogog´ok˝o, Hungary, 2005, 69–82.
[7] BARTHA, M., Strong retiming equivalence of synchronous schemes, in: J. Farr´e, I. Litovsky,
S. Schmitz (Eds.), Proc. 10th International Conference, CIAA, Sophia Antipolis, 2005, Lecture
Notes in Computer Science 3845 (2006), 66–77.
[8] BARTHA, M., Simulation equivalence of automata and circuits, in: E. Csuhaj-Varju, ´ Z. Esik ´
(Eds.), Proc. 12th International Conference on Automata and Formal Languages, Balatonfured, ¨
Hungary, 2008, 86–99.
Equivalence of Mealy automata 15
[9] BLOOM, S.L., ESIK, ´ Z., Iteration Theories: The Equational Logic of Iterative Processes,
Springer-Verlag, Berlin 1993.
[10] CIR ˇ OVIC, ˇ B., Equivalence relations of synchronous systems, Ph.D. Dissertation, Memorial University of Newfoundland, 2000.
[11] EHRIG, H., Universal Theory of Automata, B.G. Teubner, Stuttgart 1974.
[12] GECSEG, ´ F., PEAK, ´ I., Algebraic Theory of Automata, Akad´emiai Kiad´o, Budapest 1972.
[13] HARTMANIS, J., STEARNS, R.E., Algebraic Structure Theory of Sequential Machines,
Prentice-Hall, Englewood Cliffs, N.J. 1966.
[14] JOYAL, A., STREET, R., VERITY, D., Traced monoidal categories, Mathematical Proceedings
of the Cambridge Philosophical Society 119 (1996), 447–468.
[15] KATIS, P., SABADINI, N., WALTERS, R.F.C., Feedback, trace, and fixed-point semantics,
Theoretical Informatics and Applications 36 (2002), 181–194.
[16] LEISERSON, C.E., SAXE, J.B., Optimizing synchronous systems, Journal of VLSI and Computer Systems 1 (1983), 41–67.
[17] MACLANE, S., Categories for the Working Mathematician, Springer-Verlag, Berlin 1971.
[18] MANES, E.G., Algebraic Theories, Springer-Verlag, Berlin 1976.
Simulation equivalence of automata and circuits
Mikl´os Bartha ∗
Department of Computer Science, Memorial University of Newfoundland
Abstract
Automata over a symmetric monoidal category M are introduced, and a
multi-step simulation is defined among such automata. The collection of Mautomata is given the structure of a 2-category on the same objects as M, in
which the vertical structure is determined by groups of indistinguishable simulations. Two M-automata are called simulation equivalent if they are connected
by an isomorphism of 2-cells in this 2-category. It is shown that the category of
simulation equivalent M-automata is monoidal, and it satisfies all the axioms
of traced monoidal categories, except the one that explicitly kills the delay.
1 Introduction
Simulation equivalence is a fundamental relationship between two synchronous systems. As introduced in [18], a synchronous system is a deterministic Mealy automaton in which states are so called configurations – determined by the values stored
in the memory elements (registers) of the system – and the transition function is
“wired in” through certain functional elements (e.g. logical gates). By definition,
system (automaton) G can simulate system F if the following condition is satisfied.
To every sufficiently old state u of F there corresponds a state v of G
such that F and G exhibit the exact same input-output behavior when
started from states u and v, respectively.
Systems F and G are simulation equivalent if they can simulate each other. The
term “sufficiently old” in the definition above means that, starting from an arbitrary
initial state u0, F might be run for a number of steps (until state u) before setting
the simulating state v of G. The state v then depends on u0 and also on the inputs
that have arrived at F during the delay period.
As pointed out in [18], the relevance of simulation equivalence in synchronous
systems manifests itself in the ability to transform an arbitrary synchronous system
into a systolic one, in which at least one register stops data on each interconnection
between the functional elements. If the number of registers present in the system is
sufficiently large, then the systolic transformation is achieved by shifting the registers
around within the graph of the system in a suitable way. This process is called
retiming in [18]. It is then proved that retiming in synchronous systems respects
simulation equivalence. Thus, every synchronous system having a sufficiently large
number of registers can be transformed into a simulation equivalent systolic one. The
numerous advantages of dealing with a systolic, rather than an ordinary synchronous
system are well-known in computer architecture design. (See again [18] as a source
of reference.)
∗Partially supported by Natural Science and Engineering Research Council of Canada, Discovery
Grant #170493-03
1
It was later proved in [7] with a much more sophisticated formalism that retiming equivalence essentially coincides with simulation equivalence, at least when the
functional elements are interpreted in the free algebra determined by them. Thus,
the semantical concept simulation equivalence can be characterized in syntactical
terms by retiming. Obviously, this result has important consequences regarding the
decidability of simulation equivalence in synchronous systems [8].
The present paper takes a much more general look at simulation equivalence.
Starting from an arbitrary monoidal category M, we first consider the category
Circ(M) constructed in [17]. The morphisms of Circ(M) represent synchronous
systems (automata, circuits, etc.). Then we define a quotient category Sim(M) of
Circ(M), which reflects simulation equivalence of systems. Finally, we prove that
Sim(M) is a category with feedback satisfying the so called circular feedback axiom,
which corresponds directly to retiming in synchronous systems.
As a motivating example, consider the monoidal category (Set, ×), where ×
is product of sets. A Set-automaton A → B is a pair (U, α), where U is a set
and α : U × A → U × B is a function. Clearly, (U, α) is a standard deterministic
Mealy automaton with states U, input alphabet A, and output alphabet B. The
automaton has no final states, and it lacks a specified initial state, too. The intuitive
notion of simulation between automata (U, α) and (V, β) has been explained above.
The formal definition of simulation is a bit more complex and requires more than
the intuitive concept. Nevertheless, this simple heuristics as an underlying idea can
easily be retrieved in Definition 4.1 below.
Unfortunately, due to space constraints, we can only include the above example in
our presentation here, and ask the reader to follow the steps of the Sim construction
on this particular model. A number of relating examples can, however, be found
in [17] in connection with the Circ construction, which the reader may want to
analyze from the simulation point of view. Also for space restrictions, we must
assume familiarity with basic category theory, automata theory, and some elements
of computer hardware design.
2 Feedback vs. trace and iteration
Feedback, trace, and iteration in monoidal categories are strongly related concepts,
which have been studied intensively through various models in mathematics and
computer science. While iteration [9, 13] is primarily used as a fixed-point operation
in (single-sorted) algebraic theories, feedback and trace [1, 5, 15, 17] have a more
general scope covering all symmetric (or braided) monoidal categories. If, however,
the underlying monoidal category is a theory, then iteration, feedback, and trace
have the same expressive power and they are interchangeable using simple syntactic
rules. The only significant difference between iteration theories and traced monoidal
categories over an algebraic theory is the presence of the commutative axiom [9, 10],
which is required in iteration theories, but not in traced monoidal categories. Thus,
a traced monoidal category over an algebraic theory is a Conway theory in the sense
of [9], and vice versa. This fact was essentially proved back in 1987, cf. [4, Theorem
2], where the traced monoidal category axioms were introduced on magmoids in an
algebraic context. These axioms were called scheme identities in [4], and they were
used to axiomatize flowchart schemes in the sense of Elgot [12]. The same identities,
2
with the exception of yanking (called X in [4]), were used in [5] to axiomatize
synchronous schemes. See also [14] for the connection of traced monoidal categories
and theories with a dagger (fixed-point, iteration). Connections with other areas of
mathematics (algebra, geometry) and physics can be found in [15]. Further recent
papers on this topic include [1, 2, 16, 20].
In [17], categories with feedback were defined by relaxing the traced monoidal
category axioms, and the free category with feedback Circ(M) generated by an
arbitrary symmetric strict monoidal category M was constructed from so called
circuits in M. Our concern in this paper is with monoidal categories having a
feedback operation that satisfies all of the traced monoidal category axioms, except
for yanking. This axiom is the “milestone” which separates feedback (with delay)
from trace, i.e., feedback with no delay, therefore it must not hold in our delayoriented models.
In our approach we shall be working with monoidal automata and simulations
among them. These concepts have their origins in [6], where automata over a singlesorted algebraic theory T, called T-automata, and simulations of them with surjective and injective mappings were used to construct the free strong feedback theory
Fs(T) generated by T. The very same idea underlies the Circ construction of [17],
which is more general in its scope, but very restricted in the sense that simulations
in it are isomorphisms only. Reasoning in terms of Mealy automata, the Circ construction does not go beyond the observation of isomorphism between automata,
so that it carries very little semantical information. Our Sim construction, on the
other hand, does have a significant semantical contents, which will be pointed out
in Sections 4 and 5.
Another novelty of our Sim construction compared to the Circ construction is
that it captures the stepwise behavior of monoidal automata and introduces a multistep simulation between them in the way it was originally done for synchronous
systems in [18]. The result is a monoidal 2-category, which leads to a category with
circular feedback in a natural way.
3 Monoidal categories with trace and feedback
Recall from [19] that a strict monoidal category is a category M equipped with an
associative bifunctor, called tensor, which has a unit element. In the spirit of the
author’s earlier works on the subject, composition (left-to-right) and tensor will be
denoted by · and +, respectively, and I will stand for the tensor unit object. A
symmetry in M is a family of isomorphisms πA,B : A + B → B + A, natural in A
and B, such that
πA,B · πB,A = 1A+B and (πA,B + 1C) · (1B + πA,C) = πA,B+C.
The notation πA,B for symmetries is adopted from [17]. Morphisms built up solely
from symmetries using composition and tensor in a regular fashion are called permutations.
We say that a monoidal category M is single-sorted if the set of its objects is
generated by a single object 1. Such structures are also known as magmoids [3]. In
the sequel, by monoidal category we shall always mean a strict symmetric monoidal
category. This will simplify the presentation of the Sim construction a great deal,
although the construction itself works for non-strict symmetric monoidal categories
3
as well with obvious modifications. In particular, our example category (Set, ×) is
not strict, but it is perhaps the best suited to help the reader’s understanding of
simulation equivalence.
The following definition of traced monoidal categories originates from [15]. Trace
in a category M will be introduced below as an operation M(U + A, U + B) →
M(A, B), rather than M(A + U, B + U) → M(A, B) as it appears in [15], to be in
accordance with the author’s earlier works. In addition, the fact that M is strict
and symmetric will also be taken into account to simplify the definition.
Definition 3.1 A trace for a monoidal category M is a natural family of functions
T rU
A,B : M(U + A, U + B) → M(A, B)
satisfying the following three axioms:
vanishing:
T rI
A,B(f) = f , T rU+V
A,B (g) = T rV
A,B(T rU
V +A,V +B(g));
superposing:
T rU
A,B(f) + g = T rU
A+C,B+D(f + g), where g : C → D;
yanking:
T rU
U,U (πU,U ) = 1U .
Naturality of trace is meant in all three “variables” A, B, U. While naturality in A
and B is fairly obvious, we wish to spell out the meaning of naturality in U, because
the resulting axiom is our main concern in this paper.
sliding:
T rU
A,B((g + 1A) · f) = T rV
A,B(f · (g + 1B)), where f : V + A → U + B, g : U → V.
The objects A, B in T rU
A,B will usually be clear from the context, in which case they
will be omitted from the notation.
According to [17], a category with feedback is a monoidal category M equipped
with a feedback operation ↑
U
A,B: M(U + A, U + B) → M(A, B), which satisfies
all the requirements for T rU
A,B in Definition 3.1, except for the yanking axiom.
Furthermore, naturality of ↑
U
A,B in U is only required in a weaker form, when the
morphism g : U → V in the sliding axiom is an isomorphism. In the context of
categories with feedback we shall refer to sliding as the axiom of circular feedback,
by the name it appears first in [21] regarding the single-sorted case. A category
with circular feedback is then a category with feedback which satisfies the circular
feedback axiom.
Let M be a monoidal category, fixed for the rest of the paper. An M-automaton
A → B is a pair (U, α), where U is an object and α : U + A → U + B is a
morphism in M. Reflecting a digital circuit interpretation, the object U is called
the state component, while α is the combinational logic of (U, α). The collection of
M-automata can be given the structure of a category AutM equipped with a tensor
as follows. Objects, and tensor of them are as in M. Furthermore:
1A = (I, 1A);
(U, α) · (V, β) = (U + V,(πU,V + 1A) · (1V + α) · (πV,U + 1B) · (1U + β));
4
(U, α) + (V, β) = (U + V,(1U + πV,A + 1C) · (α + β) · (1U + πB,V + 1D)).
See Fig. 1. The category M is embedded into AutM by the functor A 7→ A, α 7→
(I, α). Feedback is defined in AutM as follows:
↑
V
A,B (U, α) = (U + V, α), where α : U + V + A → U + V + B.
U V A
U V A C
B
U V C
U V B D
a
a
b
b
Figure 1: Composition and tensor in AutM
Notice that tensor is not a bifunctor in AutM, therefore this category is not
monoidal. Thus, in a strict sense, AutM – with our ↑ in it – does not qualify as a
category with feedback. Of course, this does not mean that feedback is ill-defined
in AutM, rather, the name “category with feedback” is ambiguously chosen.
Two M-automata (U, α),(V, β) : A → B are said to be isomorphic if there exists
an isomorphism γ : U → V in M such that (γ + 1A)· β = α ·(γ + 1B). Isomorphism
classes of M-automata are called circuits in [17], and Circ(M) is the quotient of
AutM by this isomorphism. As it was proved in [17], tensor (of isomorphism classes
of M-automata) already makes Circ(M) a monoidal category with the symmetry
adopted from M. Moreover, Circ(M) is a category with feedback.
4 The Sim construction
In this section we present a construction which, from a given monoidal category
M, produces a category Sim(M) with circular feedback. In order to make the
construction feasible, we need to make the following assumption on the nature of
composition in M.
Virtual trace:
For αi
: A → U + Ci
, βi
: U + Ci → B, i = 1, 2 :
(1U + α1) · (πU,U + 1C1
) · (1U + β1) = (1U + α2) · (πU,U + 1C2
) · (1U + β2)
implies α1 · β1 = α2 · β2. See Fig. 2.
a a a a
b b b b
1
1 1
1 2
2 2
2
U A U A
U B U B
A A
B B
C C 1 = 2 =
Figure 2: The virtual trace implication
Intuitively, the virtual trace implication enables the use of an imaginary trace
operation when the result is expressible in terms of composition. Indeed,
T rU
((1U + αi) · (πU,U + 1Ci
) · (1U + βi)) = αi
· βi
5
is trivially satisfied in all traced monoidal categories. Therefore the virtual trace
implication simply ensures that this imaginary partial trace operation is well-defined.
Notice that virtual trace is a necessary condition for any monoidal category to have
an extension to a traced monoidal category.
The virtual trace implication is quite natural and holds in surprisingly many
monoidal categories. For example, if tensor is product or coproduct in M, then this
implication is almost straightforward. It also holds in all scheme algebras [4, 5].
In general, it holds true whenever the morphisms of M express a kind of data-flow
semantics, either explicitly or implicitly. From this point on we shall assume that
the virtual trace implication holds in our category M.
For objects A, B in M, a transition chain (chain, for short) A → B is a nonempty sequence
s = ((si
, Xi
, Xi+1)| 0 ≤ i < n), (n ≥ 1), where si
: Xi + A → Xi+1 + B.
Define the cascade product of s recursively as follows. (See Fig. 3.)
cas0(s) = s0;
casi(s) = (casi−1(s) + 1A) · (1Xi + πB,A) · (si + 1iB);
cas(s) = casn−1(s) · (1Xn + π) : X0 + nA → Xn + nB,
where for any natural number m, mB stands for the object B + . . . + B, and π :
nB → nB is the permutation that reverses the sequence of n B-blocks.
X A A A
X
X
X B B B
s
s
s
0
1
2
2
3
0
1
Figure 3: Cascade product of a chain with 3 links
Chains s = ((si
, Xi
, Xi+1)| 0 ≤ i < n) and t = ((tj , Yj , Yj+1)| 0 ≤ j < m) A → B
are said to be linkable if Xn = Y0. Clearly, in this case, the concatenation s k t of
the two chains is also a chain A → B. With a slight ambiguity, if the sequence s or
t in s k t has a single item, that item will be identified with the sequence itself.
Definition 4.1 An n-step simulation (n ≥ 1) (U, α) → (V, β) in AutM(A, B) is a
chain s = ((si
, Xi
, Xi+1)| 0 ≤ i < n) A → B such that:
(i) X0 = U and Xn = V ;
(ii) cas(s k β) = cas(α k s).
An immediate (0-step) simulation (U, α) → (V, β) is just a morphism δ : U → V
such that (δ + 1A) · β = α · (δ + 1B).
Lemma 4.2 Let s = ((si
, Xi
, Xi+1)| 0 ≤ i < n) and t = ((tj , Yj , Yj+1)| 0 ≤ j < m)
be linkable chains A → B with V = Xn = Y0. Then
cas(s k t) · πnB,mB = (cas(s) + 1mA) · (1V + πnB,mA) · (cas(t) + 1nB).
6
Corollary 4.3 If s : (U, α) → (V, β) and t : (V, β) → (W, γ) are non-immediate
simulations in AutM(A, B), then s k t is a simulation (U, α) → (W, γ).
Corollary 4.4 If s : (U, α) → (V, β) is a non-immediate simulation, then so is
s k β.
Both Corollaries 4.3 and 4.4 can be generalized to all simulations by adopting the
following natural rules regarding the operation k. Let δ and δ
′ be immediate simulations and s be a non-immediate one between appropriate M-automata.
(i) δ k s and s k δ are formed by melting δ into the first link (respectively, last
link) of s using composition in M, i.e., by replacing s0 : V + A → X1 + B with
(δ + 1A) · s0 (respectively, sn−1 : Xn−1 + A → V + B with sn−1 · (δ + 1B)).
(ii) δ k δ
′ = δ · δ
′
.
Concerning cascade products, define cas(δ) = δ.
Proposition 4.5 Simulations as morphisms define a category on AutM(A, B) as
objects.
Proof. Identities in AutM(A, B) are the identity morphisms 1U in M as immediate
simulations. Composition of simulations is k. ✷
Intuitively, Corollary 4.4 says that simulations s and s k β are essentially the
same between the automata (U, α) and (V, β). For the very same reason, s and α k s
are the same, too. The only difference between s and s k β is that s k β “realizes”
the simulation one step later, even though it has already been established by s. We
shall say that s and s k β are indistinguishable. In order to put this relation as a
proper equivalence between simulations, we introduce the following notation. For an
n-step simulation s : (U, α) → (V, β) and integer m ≥ n, φn,m(s) is the simulation
s k β
m−n
(i.e., s k (β, . . . , β)).
Definition 4.6 Simulations s, s′
: (U, α) → (V, β), where s and s
′ are n-step and
n
′
-step, respectively, are indistinguishable if there exists l ≥ max(n, n′
) such that
cas(φn,l(s)) = cas(φn′
,l(s
′
)).
We shall use the symbol ≡ to denote indistinguishability of simulations.
Proposition 4.7 If s ≡ s
′
: (U, α) → (V, β) and t ≡ t
′
: (V, β) → (W, γ), then
s k t ≡ s
′ k t
′
: (U, α) → (W, γ).
By virtue of Proposition 4.7 we redefine our vertical categories AutM(A, B), so
that its morphisms be groups of indistinguishable simulations between automata.
For simplicity, however, we shall keep working with representatives rather than
equivalence groups of simulations.
We now turn to defining a horizontal composition on simulations of M-automata.
Let s = ((si
, Xi
, Xi+1)| 0 ≤ i < n) and t = ((ti
, Yi
, Yi+1)| 0 ≤ i < n) be chains A →
B and B → C, respectively. (See Fig. 4.) Define s • t = ((ui
, Zi
, Zi+1)| 0 ≤ i < n)
to be the chain A → C in which Zi = Xi + Yi
, and
ui = (1Xi + πYi,A) · (si + 1Yi
) · (1Xi+1 + πB,Yi
· ti).
Equivalently, by the help of the (postfix) virtual trace operation ⇑ in M:
ui = ((1Xi + πYi,A + 1B) · (si + ti) · (1Xi+1 + πB,Yi+1+C)) ⇑
B .
7
X
X Y A
B
X Y A
B
X Y
B
B
Y C
i+1 i+1
i+1 i+1
i i
i i
s
s
t
t
i i
i
i
C
Figure 4: Horizontal composition of transition chains
Proposition 4.8 For simulations s : (U, α) → (V, β) and t : (U
′
, α′
) → (V
′
, β′
)
between M-automata A → B and B → C, where s and t are chains as above with
X0 = U, Xn = V , Y0 = U
′
, Yn = V
′
, s • t : (U, α) · (U
′
, α′
) → (V, β) · (V
′
, β′
) is a
simulation. Moreover, for every m ≥ n, φn,m(s) • φn,m(t) = φn,m(s • t).
Proof. For simplicity, we only show a diagram that proves the first statement for
1-step simulations. The proof itself is highlighted by dotted lines in the diagram
of Fig. 5. Notice that the virtual trace implication need not be used in this proof,
although it clarifies the situation a great deal. ✷
U A U A
A U' A U'
V V' V V'
B B
B B
B B C C B B C C
=
s
s t
t a
b b'
a'
Figure 5: The proof of Proposition 4.8
The simulation s • t is called the horizontal composite of s and t. Using Corollary 4.4 it becomes possible to define the horizontal composite of two simulations
even when they take different number of steps. Thus, by the second statement of
Proposition 4.8, horizontal composition is properly established on groups of indistinguishable simulations. It is routine to check that • is associative, and the identities
for this composition are determined by the morphism 1I of M as an immediate simulation (I, 1A) → (I, 1A) for each object A. Furthermore, the horizontal composite
of two vertical identities is itself a vertical identity, which is obvious.
Theorem 4.9 Groups of indistinguishable simulations as 2-cells extend AutM to a
2-category.
Proof. All components of this 2-category have been checked earlier, hence we need
only show the interchange law
(s k s
′
) • (t k t
′
) ≡ (s • t) k (s
′
• t
′
)
for all appropriate simulations s, t, s′
, t′
. We leave this as an easy exercise. ✷
The next step in our construction is to show that in AutM, simulations are
compatible with tensor and feedback. Let s = ((si
, Xi
, Xi+1)| 0 ≤ i < n) and
8
t = ((ti
, Yi
, Yi+1)| 0 ≤ i < n) be chains A1 → B1 and A2 → B2, respectively, and
define s✷t = ((ui
, Zi
, Zi+1)| 0 ≤ i < n) to be the chain A1 + A2 → B1 + B2 in which
Zi = Xi + Yi and
ui = (1Xi + πYi,A1 + 1A2
) · (si + ti) · (1Xi+1 + πB1,Yi+1 + 1B2
).
Proposition 4.10 For simulations s : (U1, α1) → (V1, β1) and t : (U2, α2) →
(V2, β2) between M-automata A1 → B1 and A2 → B2, where s and t are as
above with X0 = U1, Xn = V1, Y0 = U2, and Yn = V2, s✷t is a simulation.
For every m ≥ n, φn,m(s)✷φn,m(t) = φn,m(s✷t). Moreover, the interchange law
(s k s
′
)✷(t k t
′
) ≡ (s✷t) k (s
′✷t
′
) holds for all appropriate simulations s, t, s′
, t′
.
Now let s = ((si
, Xi
, Xi+1)| 0 ≤ i < n) be a chain U + A → U + B, and define
↑
U s = ((si
, Xi + U, Xi+1 + U)| 0 ≤ i < n) as a chain A → B.
U U
A A
B B
V U U
W U U
A A
B B
V U U
W
W V
s
s
a
b
=
Figure 6: The proof of Proposition 4.11
Proposition 4.11 If s : (V, α) → (W, β) is a simulation between M-automata,
where s is a chain U + A → U + B as above with X0 = V and Xn = W, then
↑
U s : (V +U, α) → (W +U, β) is a simulation. Moreover, s ≡ s
′
implies ↑
U s ≡↑U s
′
,
and the interchange law ↑
U (s k t) = (↑
U s) k (↑
U t) holds for all appropriate
simulations s, t.
Proof. Again, we restrict the proof to the case n = 1, and provide a diagram
for justification. The dotted feedback line in the digram of Fig. 6 indicates the
main point of the proof. Observe that the virtual trace implication does play a
crucial role in this argument. Indeed, the U-to-U cross-connection in the diagram,
although expressible in terms of composition in M, cannot be made legal without
this implication, for another V-to-V (or W-to-W) connection already exists between
the same two boxes, which connection has been established by composition. ✷
Definition 4.12 Automata (U, α) and (V, β) in AutM(A, B) are simulation equivalent, in notation (U, α) ∼ (V, β), if they are isomorphic according to the vertical
structure of the 2-category AutM. The category of simulation equivalent M-automata
is defined as Sim(M) = AutM/ ∼.
Example. Let A = {0, 1}, and consider the Set-automaton ∇A =↑
A πA,A together
with one of its variants over the set of states A′ = {0, 0
′
, 1} in Fig. 7. As we
shall see in Section 5, these two automata are simulation equivalent. On the other
9
hand, the trivial one-state automaton 1I is not simulation equivalent with the twostate automaton the transition diagram of which consists of two identical copies of
1I . These automata can simulate each other, but Definition 4.12 would require an
isomorphism between I and a two-element set in Set.
0/0
1/1
0
1
1/0 0/1
0/0 0/0 0’ 0
1/1
1
0/1 1/0
1/0
Figure 7: The automaton ∇{0,1} and its variant
Theorem 4.13 Sim(M) is a category with circular feedback.
Proof. By the interchange laws (Theorem 4.9 and Propositions 4.10, 4.11) simulation
equivalence of M-automata is compatible with composition, tensor, and feedback.
Also, Sim(M) is a quotient of Circ(M). Indeed, what makes two M-automata
equivalent as circuits in Circ(M) is, in our language, an immediate simulation that
is also an isomorphism in M. Thus, Sim(M) is a category with feedback, so ∼ can
also be considered as a congruence in Circ(M). As to the circular feedback axiom,
let f = (Wf , α) : V +A → U+B and g = (Wg, β) : U → V be M-automata. We need
to show that the morphisms ↑
V
(f · (g + 1B)) and ↑
U ((g + 1A) · f) are simulation
equivalent in AutM(A, B). An immediate simulation s :↑
U ((g + 1A) · f) →↑V
(f · (g + 1B)) is easy to find:
s = (πWg,Wf + 1U ) · (1Wf + β).
W
W W W
W W
V
V A
B
U
U
f
f
g g
g
g
a
b
b
f
f
g
g
g g
a
b
b
W
W W
W W W
U
U A
B
V
V
=
Figure 8: Circularity of feedback
For a justification, see the diagram of Fig. 8. A similar diagram shows that
t = (πWf ,Wg + 1V + 1A) · (1Wg + α)
is a one-step simulation ↑
V
(f · (g + 1B)) →↑U ((g + 1A)· f). A simple computation
shows that s • t is exactly the combinational logic of ↑
U ((g + 1A) · f) and t • s is
that of ↑
V
(f ·(g + 1B)). The result now follows from Corollary 4.4 by the definition
of indistinguishability. ✷
10
α 1
α 2
α
α
U
U V
V
V U
A
B
1
1
A A A
B B B
1 2
1 2 2
2
=
1
2
Figure 9: The retiming identity
5 Universality of the Sim construction
We start out by a generalization of the circular feedback axiom. This generalization,
called the retiming identity, was first considered in [11] for synchronous systems.
The axiom, shown in Fig. 9, is the equation appearing in Lemma 5.1 below. The
notation ∇A =↑
A πA,A is adopted from [5], and it identifies a register of sort A.
For convenience, we also generalize our cascade product cas so that it applies for
sequences of morphisms si
: Xi+Ai → Xi+1+Bi without the restriction that Ai = A
and Bi = B for fixed objects A, B.
Lemma 5.1 For any morphisms α1 : U + A1 → V + B1 and α2 : V + A2 → U + B2
in a category with circular feedback,
(∇A1 + 1A2
)· ↑U
cas(α1 k α2) = πA1,A2
· (↑
V
cas(α2 k α1)) · πB2,B1
· (∇B1 + 1B2
).
Proof. We present a proof covering the special case U = V = I and α2 = 1I only,
which already features the trick of pulling a register through a box. The reader can
follow the steps of this simple proof on Fig. 10. ✷
= = =
=
= = =
Figure 10: The proof of Lemma 5.1.
The point of retiming in general is indeed shifting a layer of registers from one side
of a “box” to the other, as introduced originally in [18] for synchronous systems.
In Fig. 9, register ∇A1
is shifted explicitly from the input side of box α1 to the
output side, whereas the register ∇U is part of the U-feedback connection coming
from box α2 and is turned into a register ∇V on the V -feedback connection from
α1 to α2. Lemma 5.1 simply says that the retiming identity is a consequence of
the circular feedback identity in monoidal categories with feedback. For this reason,
the congruence relation induced by the circular feedback identity in any monoidal
category with feedback will be called retiming equivalence; denoted ∼r.
Now we introduce two new axioms, which are aimed at capturing the sequential
behavior of the feedback operation (with delay). Let α : U + A → U + B and
β : V + A → V + B be morphisms in M, and for every n ≥ 1 construct the chains
α
n = (α, . . . , α) and β
n = (β, . . . , β) A → B. The sequential feedback axioms are
the following two implications in Circ(M).
11
Speed-up:
If ↑
U cas(α
n
) ∼r↑
V
cas(β
n
) for every sufficiently large n, then ↑
U α ∼r↑
V β.
Delay:
If ∇A· ↑U α ∼r ∇A· ↑V β, then ↑
U α ∼r↑
V β.
The rationale for the speed-up axiom is the following. Let f = (U, α) and
g = (V, β) be the M-automata associated with α and β, respectively. Then f
n =
(U, cas(α
n
)) is essentially the same as f, except that it performs n steps of f in one
clock cycle. In other words, f
n
is the n-speed-up of f. Hence, if f
n and g
n are
retiming equivalent for all sufficiently large n, then it is natural to expect that f
and g be retiming equivalent as well.
Concerning the delay axiom, our philosophy is as follows. We know that the
input-output channels are not retimable in synchronous systems (see [7, 8]). Therefore it does not matter if we put an extra register (morphism ∇A) on the interconnections starting from the input channels, retimability of f to g should not be
affected.
Theorem 5.2 If Circ(M) satisfies the sequential feedback axioms, then Sim(M) is
the free monoidal category with circular feedback generated by M.
Proof. We show that ∼⊆∼r holds in Circ(M), provided that feedback is sequential in that category. This statement, together with Theorem 4.13, implies that
∼=∼r. Given that Circ(M) is freely generated by M as a category with feedback,
Sim(M) = Circ(M)/ ∼ will also be free as a category with circular feedback.
Let f = (U, α) and g = (V, β) be M-automata A → B such that f ∼ g. By
definition, there exist simulations s : f → g and t : g → f such that s · t ≡ 1f and
t · s ≡ 1g. Spelling this out, we have:
(i) cas(s k β) = cas(α k s) and cas(t k α) = cas(β k t);
(ii) cas(α
k k s k t) = cas(α
k+n+m) and cas(t k s k β
k
) = cas(β
m+n+k
),
where n and m denote the length of s and t, respectively, and k is any sufficiently
large integer. Using Lemma 5.1, a short computation shows that
(∇mA + 1(n+k)A) ↑
V
cas(β
m+n+k
) = π1 ↑
U cas(α
k+n+m) · π2 · (∇mB + 1(n+k)B),
where π1 = πmA,(n+k)A and π2 = π(n+k)B,mB. See Fig. 11 for the case n = m = k =
1. On the other hand, directly by Lemma 5.1,
(∇mA + 1(n+k)A) ↑
U cas(α
m+n+k
) ∼r π1 ↑
U cas(α
n+k+m) · π2 · (∇mB + 1(n+k)B).
Thus,
(∇mA + 1(n+k)A) ↑
V
cas(β
m+n+k
) ∼r (∇mA + 1(n+k)A) ↑
U cas(α
m+n+k
), so that
∇(m+n+k)A ↑
V
cas(β
m+n+k
) ∼r ∇(m+n+k)A ↑
U cas(α
m+n+k
).
The statement now follows from the sequential feedback axioms. ✷
Returning to our Example, we show that the two automata appearing in Fig. 7
are retiming equivalent, and therefore they are simulation equivalent as well. Let
δ : A × A → A × A be the transition function (with the output component) of
automaton ∇A on the left, and δ
′
: A′ × A → A′ × A be that of its variant on the
right. (Remember that A = {0, 1} and A′ = {0, 0
′
, 1}.) Define α : A × A → A′ × A
and κ : A′ → A by α(i, j) = δ
′
(i, j), κ(i) = i, and κ(0′
) = 0 for every i, j ∈ {0, 1}.
Then, clearly, δ = α · (κ × 1A) and δ
′ = (κ × 1A) · α.
12
V
t
s
β
V
V
U
s
β
t
U
U
U
α
t
s
U
U
U
α
α
α
U
U
V
β
β
β
V
V
~r
= = =
Figure 11: The proof of Theorem 5.2
In general, a state s of a finite state Mealy automaton f is called run-out if
no sufficiently long input can take f to s; otherwise s is called permanent. Denote
by P(f) the restriction of f to its permanent states, and let δ be the transition
function of P(f). Two states s and s
′ of P(f) are said to be simulation equivalent
(in notation s ∼ s
′
) if s and s
′ are bisimulation equivalent in the usual sense, and,
furthermore, for every sufficiently long input string w, δ(s, w) = δ(s
′
, w). (See again
the transition diagram on the right-hand side of Fig. 7 to verify that states 0 and
0
′ are simulation equivalent.) The equivalence ∼ gives rise to a minimal automaton
P(f)/∼, in terms of which retiming equivalence of finite state Mealy automata can
be characterized in the following way.
Theorem 5.3 Two finite state Mealy automata f and g are retiming equivalent iff
the automata P(f)/∼ and P(g)/∼ are isomorphic.
The proof of Theorem 5.3 will be presented in a forthcoming paper.
Using the above characterization, one can prove that feedback is sequential in
Circ(Setf ), where Setf is the restriction of Set to finite sets and functions. Thus,
by Theorem 5.2, we have the following important result.
Corollary 5.4 Sim(Setf ) is freely generated by Setf as a category with circular
feedback.
At present we do not know if Corollary 5.4 holds for infinite state Mealy automata or
not. Nor do we know if, in general, the sequential feedback axioms are necessary for
∼r to coincide with ∼, or if any condition at all is needed to ensure this coincidence.
We conjecture, however, that ∼r=∼ holds in Circ(M) for all monoidal categories
M in which tensor is product or coproduct.
References
[1] S. Abramsky, Retracing some paths in process algebras, CONCUR’96, U. Montanari and V. Sassone, eds., Springer-Verlag, Lecture Notes in Comput. Sci.
1119 (1996) 1–17.
[2] S. Abramsky, Abstract scalars, loops, and free traced and strongly compact
closed categories, CALCO 2005, J. Fiadeiro, N. Harman, M. Roggenbach, and
J. Rutten, eds., Springer-Verlag, Lecture Notes in Comp. Sci. 3629 (2005) 1–29.
[3] A. Arnold, M. Dauchet, Th´eorie des magmo¨ıdes, RAIRO Inform. Th´eor. 12
(1978), 235–257 and 13 (1979), 135–154.
13
[4] M. Bartha, A finite axiomatization of flowchart schemes, Acta Cybernet. 2
(1987), 203–217.
[5] M. Bartha, An equational axiomatization of systolic systems, Theoret. Comput.
Sci. 55 (1987), 265–289.
[6] M. Bartha, An algebraic model of synchronous systems, Information and Computation 97 (1992), 97–131.
[7] M. Bartha, B. Ciroviˇc, On some equivalence notions of synchronous syste ˇ ms,
Proceedings, 11th International Conference on Automata and Formal Languages, Dogog´ok˝o, Hungary (Z. Esik and Z. F¨ul¨op, eds.), 2005. ´
[8] M. Bartha Strong retiming equivalence of synchronous systems, Proceedings of
the International Conference CIAA’05, Sophia Antipolis, France, Lecture Notes
in Computer Science Vol. 3845/2006, pp. 66–77.
[9] S. L. Bloom, Z. Esik, ´ Iteration Theories: The Equational Logic of Iterative
Processes, Springer Verlag, Berlin, 1993.
[10] S. L. Bloom, Z. Esik, Axiomatizing schemes and their behaviors, ´ J. Comput.
System Sci. 31 (1985), 375–393.
[11] B. Ciroviˇc, Equivalence relations of synchronous systems, P ˇ h.D. Dissertation,
Memorial University of Newfoundland, 2000.
[12] C. C. Elgot, Monadic computations and iterative algebraic theories, in: H.E.
Rose, ed.,Logic Colloquium 73 (North-Holland, Amsterdam, 1975) 175–230.
[13] C. C. Elgot, Selected Papers (S. L. Bloom, ed.), Springer Verlag, New York,
1982.
[14] M. Hasegawa, Models of Sharing Graphs: A categorical semantics of let and
letrec, Ph.D. Thesis, Edinburgh (1997), Springer (1999).
[15] A. Joyal, R. Street, and D. Verity, Traced monoidal categories, Math. Proc.
Camb. Phil. Soc. 119 (1996), 447–468.
[16] P. Katis, N. Sabadini, and R. F. C. Walters, Bicategories of processes, J. Pure
Appl. Algebra 115 (1997) 141–178.
[17] P. Katis, N. Sabadini, and R. F. C. Walters, Feedback, trace, and fixed-point
semantics, Theoret. Informatics Appl. 36 (2002), 181–194.
[18] C. E. Leiserson, J. B. Saxe, Optimizing synchronous systems, J. VLSI Comput.
Systems 1 (1983), 41–67.
[19] S. MacLane, Categories for the Working Mathematician, Springer Verlag,
Berlin, 1971.
[20] A. Simpson and G. Plotkin, Complete axioms for categorical fixed-point operators, in Proc. 15th LICS (2000) 30–41.
[21] Gh. S¸tef˘anescu, “Feedback Theories (A Calculus for Isomorphism Classes of
Flowchart Schemes),” Research Report 24, National Institute for Scientific and
Technical Creation, Bucharest, 1986.
14
On some equivalence notions of synchronous
systems
Mikl´os Bartha ∗
Department of Computer Science
Memorial University of Newfoundland
St. John’s, NL, Canada
Branislav Cirovic
Department of Electronics Engineering
College of the North Atlantic
St. John’s, NL, Canada
Abstract
An important optimization tool in the design of synchronous systems is
retiming, which in many cases allows a significant reduction in the length of
the systems’ clock period. Even though the internal structure of systems
changes upon retiming, their input-output behavior remains essentially
the same. The original system and the one after the retiming can simulate
each other in a suitable way. The equivalence notion arising from this
kind of mutual simulation is called simulation equivalence, and the aim of
this paper is to characterize simulation equivalence in an algebraic setting.
It is shown that simulation equivalence is a congruence relation of the
algebra of synchronous schemes, and that this congruence is the smallest
one containing retiming equivalence and finitary strong equivalence. An
axiomatization of these equivalences is presented in the general framework
of strictly monoidal categories with feedback.
1 Introduction
The increasing demands of speed and performance in modern signal and image
processing applications necessitate a revolutionary super-computing technology.
In most real-time digital signal processing applications, general purpose parallel computers cannot offer satisfactory processing speed due to severe system
∗Partially supported by Natural Science and Engineering Research Council of Canada,
Discovery Grant #170493-03
overheads. Therefore, special purpose array processors will become the only
appealing alternative. Synchronous systems are such multiprocessor structures,
which provide a realistic model of computation capturing the concepts of pipelining, parallelism, and interconnection. They are single-purpose machines which
directly implement as low-cost hardware devices a wide variety of algorithms,
such as filtering, convolution, matrix operations, sorting, etc.
The concept of a synchronous systems was derived from that of a systolic
system, which has turned out to be one of the most attractive tools in massive
parallel computing. A large number of systolic systems have been designed,
many of them manufactured. Transformation methodologies for the design and
optimization of systolic systems have been developed, and yet, a rigorous mathematical foundation of systolic and synchronous systems has not been provided
until recently. Important equivalence relations of synchronous systems, such as
retiming equivalence, strong equivalence, and simulation equivalence still lack
a proper characterization and decision algorithms. The present paper aims at
providing a suitable characterization in a precise algebraic framework, which
characterization will easily lead to appropriate decision algorithms.
As introduced in [15], a synchronous system is partitioned into functional
elements (combinational logic) and registers (clocked memory). Such a system
can be described by an edge-weighted directed graph G, in which the vertices
represent functional elements and the edges correspond to interconnections between the functional elements. The weight of each edge in G is a non-negative
integer, which indicates the number of registers placed along the interconnection between the two functional elements that correspond to the endpoints of
the edge. The external interface is represented in G by a distinguished vertex,
called the host.
In a synchronous system, every functional element has a fixed primitive operation associated with it. These operations are designed to manipulate some
simple data (e.g. signals) in the usual algebraic sense. The registers and functional elements are organized by a common clock, which renders the following
stepwise behavior to the system. A state (also called configuration) is an assignment of data to all registers. With each clock tick, the current configuration is
mapped into a new configuration in such a way that every functional element
performs the primitive operation associated with it. The operands (result) of
the operation performed by each functional element are taken from (is forwarded
to) the nearest registers lying on the interconnections arriving at (going out of)
the functional element. At the same time, data are advanced one register in the
queue of registers along each interconnection. If there is no register along an
interconnection, then data are always propagated through that interconnection
during a single clock cycle. To avoid circular rippling of data within the system,
it is assumed that every oriented cycle in the graph of the system contains at
least one edge having strictly positive weight.
In the forthcoming sections we are going to present a formal algebraic model
for the study of synchronous systems, and characterize some of their basic equivalences as congruence relations in the algebra of synchronous schemes. The reader
2
is referred to [12] for the universal algebraic terminology used.
2 Schemes, flowcharts, and their algebras
As introduced in [3], a synchronous scheme over a ranked alphabet Σ = {Σn|n ≥
0} is a finite directed graph F having the following additional structure.
1. Each vertex v is labeled by either a symbol in Σ, or one of the symbols
in:
{icj | 1 ≤ j ≤ q} ∪ {oci
| 1 ≤ i ≤ p} ∪ {⊥},
where p and q are fixed non-negative integers. If the label of v is in Σ, then
v is called a box. Boxes represent functional elements in synchronous systems.
Vertices labeled by the symbols {icj | 1 ≤ j ≤ q} and {oci
| 1 ≤ i ≤ p} are called
input and output channels, respectively, and every vertex labeled by ⊥ is called
a loop vertex. We shall assume that each label, not only those in Σ, has a fixed
rank associated with it, so that rank(oci) = 1, rank(icj) = 0, and rank(⊥) = 1.
Then the in-degree of v (that is, the number of edges arriving at v) equals the
rank of the symbol labeling v. Moreover, the only edge arriving at each loop
vertex is a loop around that vertex.
2. The edges arriving at any vertex v labeled by a symbol of rank n are
ordered, which order is captured by saying that these edges enter v at the first,. . .,
n-th input port.
3. Each edge e is assigned a non-negative integer weight w(e), which specifies
the number of registers placed along the interconnection represented by e. It is
required that, in each oriented cycle of F, there exists at least one edge e with
w(e) > 0. This requirement will be referred to as the exclusion of rippling.
A synchronous system is a pair (F, I), where F is a synchronous Σ-scheme
(scheme, for short), and (Σ, I) is a Σ-algebra. If F is a scheme having p output
and q input channels, then we write F : p → q. Let F
R denote the directed
graph obtained from F by reversing the direction of each edge in it. When
forgetting the weight of the edges, F
R becomes a flowchart in the sense of [10].
This flowchart will be denoted by fl(F).
Vertex v in scheme F is said to be accessible if there exists a directed path
in F
R from some output channel leading to v. Scheme F is accessible if all of
its boxes and loop vertices are accessible. System S is accessible if the scheme
of S is such.
Now we turn to defining the algebra of synchronous schemes and some other
related algebras. Each of these algebras is sorted by the set N × N of all pairs
of non-negative integers, and we shall refer to any element a in an underlying
set of sort (p, q) as a : p → q. Our algebras use a common set of constants and
operations, which are listed below.
Constants. 1 : 1 → 1, 0 : 0 → 0, 01 : 0 → 1, x : 2 → 2, and ǫ : 2 → 1.
Sum. If F1 : p1 → q1 and F2 : p2 → q2, then F1 + F2 : p1 + p2 → q1 + q2.
3
Composition. If F : p → q and G : q → r, then F · G : p → r.
Feedback. If F : 1 + p → 1 + q, then ↑ F : p → q.
The algebra Syn(Σ) of synchronous schemes is defined as follows.
— The constants are “all-wire” schemes not containing boxes, registers, or
loop vertices. Each one of them, except for x, is uniquely determined by this
description as an appropriate mapping. The constant x is defined as the transposition mapping 2 → 2.
— The sum of schemes F1 : p1 → q1 and F2 : p2 → q2 is essentially their
disjoint union with an appropriate relabeling of the input-output channels of F2.
— The composite of schemes F : p → q and G : q → r is obtained by
gluing them together at the input (output) channels of F (respectively, G). The
weights of the edges that are joined during this procedure are added up, and
output wires of G “cut” by an isolated input channel of F are deleted. See [3]
for the details.
— The feedback of scheme F : 1 + p → 1 + q is obtained by joining the edge
e arriving at oc1 with each of the edges starting from ic1 (if any), adding up
their weights as in the case of composition, and incrementing each of these sums
by 1. The incrementation amounts to putting an extra register along each of
the newly created interconnections. Again the output wire e is deleted if ic1 is
isolated. In case e comes directly from ic1, a new loop vertex is created and the
weight of its loop is set to w(e) + 1.
Notice the different treatment of loop vertices in the definition of feedback
above compared to [2, 3] and [7]. In those papers there is exactly one loop vertex
in each scheme, regardless of whether this vertex has actually been generated by
the feedback operation or not. This is a minor change, however, and the axiomatization of schemes presented in those works can be adjusted in a staightforward
manner to accomodate multiple loop vertices. This issue will be dealt with in
Section 7.
Sometimes it is advantageous to make the presence of registers more explicit
in schemes. For this reason, augment Σ by a new symbol ∇ of rank 1, and
consider the graph F
R for any scheme F : p → q. Interpret the weight of
each edge e in F
R by subdividing e with a sequence of w(e) ∇-boxes to obtain
a flowchart fl∇(F) : p → q over the alphabet Σ∇ = Σ ∪ {∇}. Clearly, the
connection χ : F 7→ fl∇(F) is one-to-one, but not all Σ∇-flowcharts are covered
by χ because of the restriction imposed by the exclusion of rippling. As another
slight deficiency, the equation ǫ · ∇ = (∇ + ∇) · ǫ, which would be necessary to
exclude branches originating from ∇-boxes, is not true in the algebra of ordinary
Σ∇-flowcharts.
Let Fl(Σ∇) denote the algebra of Σ∇-flowcharts as introduced in [2], with
the modified treatment of loop vertices described above. Take the quotient
of Fl(Σ∇) determined by the equation ǫ · ∇ = (∇ + ∇) · ǫ, and restrict the
underlying sets of this algebra to flowcharts that are in the range of the function
4
χ. Let Fl∇(Σ) denote the resulting partial algebra. We shall make use of this
algebra in the proof of Theorem 4.2. To ensure a uniform treatment for flowchart
and synchronous schemes, flowcharts will also be considered as edge-weighted
schemes in which all edges have zero weight.
3 Simulation equivalence of synchronous systems
The following definition of simulation equivalence between two accessible synchronous systems is quoted from [15].
Definition 1 System S1 can simulate system S2 if, for every sufficiently old
configuration c2 of S2, there exists a configuration c1 of S1 such that S1 and
S2 exhibit the same input-output behavior when started from configurations c1
and c2, respectively. Systems S1 and S2 are simulation equivalent if they can
simulate each other. Schemes F1 and F2 are simulation equivalent, if the systems
S1 = (F1, I) and S2 = (F2, I) are such under all interpretations I.
To avoid ambiguity, we need to spell out the meaning of the term “sufficiently
old” in the definition of simulation equivalence above. It means that, starting
from an arbitrary initial configuration c of S2, there exists a non-negative integer
n(c) such that, no matter how the inputs to S2 are chosen in the first n(c) clock
cycles, the resulting configuration c2 has the property specified in Definition 1.
The corresponding configuration c1 of S1 will then depend on c and the inputs
to S2 in the first n(c) clock cycles. The important point here is that the choice
of c is arbitrary. This is to ensure transitivity of simulation, so that simulation equivalence indeed becomes an equivalence relation of synchronous systems
(schemes). Simulation equivalence of either systems or schemes will be denoted
by ∼.
Examples In all of the examples below, fix the interpretation I to be the free
Σ-algebra TΣ, and assume that Σ has at least two symbols, one of which is a
constant. The symbol g used in the examples has rank 1. For better understanding, the schemes appearing in Figures 1, 2, 3, and 4 have their registers
represented by ∇-boxes rather than integer weights.
Example 1. Consider the systems S1 and S2 in Fig. 1.
Clearly, S2 can simulate S1. We do not even have to bother making a configuration c of S1 sufficiently old, because the configuration f(c) will do for S2.
In order to simulate S2 with S1, however, we need to run S2 for one clock cycle
in order to obtain the value f(x1) in its sole register, where x1 is the input to
S2 in the first clock cycle. Then, by assigning the value x1 to the register of S1,
the simulation of S2 is properly established. Thus, S1 ∼ S2.
Example 2. See Fig. 2 for the systems S1 and S2.
Again, S2 can trivially simulate S1 by assigning the same value to its two
registers. When simulating S2 with S1, however, we cannot assume that these
values are the same, at least not initially. After just one clock cycle, however,
the difference between the initial contents of the two registers “flushes out”, so
5
g
g
ic 1
1
oc oc
ic 1
1
S S 1 2
Figure 1: The schemes of Example 1
g
g
g
oc1
S 2
oc 1
S 1
Figure 2: The schemes of Example 2
6
g
g
g
oc1
S 2
oc 1
S 1
Figure 3: The schemes of Example 3
that an appropriate configuration of S1 can be adjusted. Thus, S1 ∼ S2.
Example 3. See Fig. 3 for the systems S1 and S2.
Now S1 cannot simulate S2, because any possible difference between the
initial contents of the two registers in S2 “regenerates” itself after each clock
tick. The contents of these two registers are therefore inherently different, which
makes it impossible to set the simulating configuration of S1 in an appropriate
way. Thus, S1 6∼ S2.
Example 4. See Fig. 4 for the systems S1 and S2.
Observe that the two outputs of S1 are the same in each clock cycle, regardless
of the initial configuration. The two outputs of S2, however, will always be out
of pace compared to one another, the output at oc2 “lagging one g” behind the
output at oc1. Thus, S1 6∼ S2.
By definition, simulation equivalence is of a semantic nature. As stated in
[15]:
“Two synchronous systems may be equivalent even though their internal organizations are radically different. For example, one system might be a tree and the
other a mesh.”
Our goal is to demonstrate that this statement over-estimates the complexity of
simulation equivalence, which can in fact be characterized in syntactical terms.
This characterization even admits a decision algorithm for simulation equivalence, which will be presented in a forthcoming paper.
We start out by a theorem that reveals an important algebraic property of
simulation equivalence. The proof of the theorem is a routine check to verify that simulation equivalence is compatible with the scheme operations sum,
composition, and feedback.
Theorem 3.1 Simulation equivalence is a strong congruence relation of the par7
g g
g g
S 1
oc oc 1 2
S 2
oc oc 1 2
Figure 4: The schemes of Example 4
tial subalgebra Syna
(Σ) of Syn(Σ) determined by all accessible schemes.
Theorem 3.1 would remain true in the whole algebra Syn(Σ), should we want
to extend Definition 1 directly to all schemes. This is not our intention, however,
for reasons outlined in Section 7.
4 Retiming synchronous systems
Let F be scheme and u be a box in F labeled by σ ∈ Σn such that the edges
e1, . . . , en arriving at the input ports of u have positive weights. Retiming u then
means subtracting 1 from w(ei) for all 1 ≤ i ≤ n, and adding 1 to the weight
of each edge going out from u. Retiming will also be allowed on loop vertices,
in which case the label of u is ⊥, and n = 1. Elementary retiming is the binary
relation →r on Syn(Σ) by which F →r F
′
if F
′
results from F by retiming a
single box or loop vertex in it. The smallest equivalence relation containing →r
is called retiming equivalence, and is denoted by ∼r.
A retiming count vector for scheme F is an assignment R of integers to all of
its boxes and loop vertices. Extend R to all vertices of F by fixing R(v) = 0 for
each input/output channel. We say that R is legal if for every edge e : u → v
in F, w(e) + R(u) − R(v) ≥ 0. If R is legal, then it takes F into a scheme F
′
that has the same flowchart structure as F, but the weight w
′
(e) of each edge
e : u → v is w(e) + R(u) − R(v).
A characterization of ∼r by retiming count vectors was given in [15] as follows.
Proposition 4.1 Schemes F and F
′ are retiming equivalent iff there exists a
legal retiming count vector R taking F into F
′
.
8
It is clear by the definitions that if R is legal for F, then −R is legal for F
′
, and
−R takes F
′ back to F. It is also easy to see by Proposition 4.1 that retiming
equivalence is a congruence relation of Syn(Σ).
Based on Proposition 4.1, the following theorem was proved in [15] as the
“Retiming Lemma”, relying on the simple graph model of synchronous systems. Here we present an algebraic proof for the more sophisticated synchronous
scheme model.
Theorem 4.2 In Syna
(Σ), ∼r⊆∼.
Proof. It is sufficient to prove that elementary retiming in synchronous schemes
is simulation equivalent. That is, retiming a single box in an accessible scheme
F results in an accessible scheme F
′ ∼ F. On the analogy of Example 2, this
statement is obvious for retiming a loop vertex in F. If F consists of a box
σ ∈ Σn as a stand-alone scheme 1 → n having a single layer of registers on its
input side, then the statement is adequately reflected by Example 1. If the box σ
is being retimed in a context C, then try to model C as an algebraic expression
in Syn(Σ) over one variable 1 → n, so that F = C(S1) and F
′ = C(S2),
where S1 and S2 are single-box schemes before and after a suitable elementary
retiming, as in Example 1. If this was possible, then the statement of the theorem
would immediately follow from Theorem 3.1. Unfortunately, despite the fact
that Syn(Σ) is generated by Σ, the context C cannot always be modeled in
this way. The reason is that some of the registers present in S1 and S2 may
only be generated algebraically by applying the feedback operation as part of
the context C, which is illegal. In other words, ∼r cannot be specified as the
smallest congruence relation of Syn(Σ) containing single-box retiming.
On the other hand, it is straightforward to design C in the algebra Fl∇(Σ).
Even though Fl∇(Σ) is only a partial algebra, on the analogy of Theorem 3.1 it
is easy to establish ∼ as a strong congruence in it. With this observation, the
proof of Theorem 4.2 is now complete.
5 Strong equivalence of synchronous schemes
Let F be a synchronous scheme. The relation of having the same strong behavior
is defined on the vertices of F as the largest label-preserving equivalence µF
such that if uµF v, with the label of u and v having rank n ≥ 1, then for every
1 ≤ i ≤ n, uiµF vi holds for the vertices ui and vi that are connected to the
i-th input port of u and v by edges e
u
i
and e
v
i
; moreover, w(e
u
i
) = w(e
v
i
). The
equivalence µF gives rise to a minimal scheme F/µF in the usual way (cf. [10]),
and schemes F1, F2 are said to be strong equivalent if F1/µF1 = F2/µF2
. Strong
equivalence will be denoted by ∼s.
By the standard definition in graph theory, a directed walk in graph G
is an alternating sequence of vertices and edges, which starts and ends with
a vertex, and in which each edge points from the vertex immediately preceding it to the vertex immediately following it. Let F be a scheme, and
α = v0e1 . . . envn be a directed walk in F
R. By the pattern of α we mean
9
the sequence p(α) = σ0(i1, w1). . .(in, wn)σn, where σj , 0 ≤ j ≤ n, is the
label of vertex vj , ij identifies the input port of vj−1 to which the edge ej
is connected, and wj = w(ej ). In general, a pattern of walks is a sequence
p = σ0(i1, w1). . .(in, wn)σn such that σj ∈ Σ ∪ {ick|k ≥ 1} ∪ {ocl
|l ≥ 1},
1 ≤ ij ≤ rank(σj−1) and wj ≥ 0. We say that pattern p is feasible for vertex u
if there exists a directed walk α in F
R starting from u such that p = p(α). In
this case, end(u, p) denotes the last vertex of α.
It is easy to see that, for every two vertices u and v of F, uµF v is equivalent
to saying that an arbitrary pattern p is feasible for u iff p is feasible for v.
Now we introduce the relation of having the same finitary strong behavior on
the set of vertices of scheme F, denoted θF . For two vertices u and v, uθF v if for
every pattern p, p is feasible for u iff p is feasible for v, and, end(u, p) = end(v, p)
whenever p is feasible and sufficiently long. Clearly, θF is also an equivalence
relation, and θF ⊆ µF . This equivalence, too, gives rise to a minimal scheme
F/θF , and schemes F1, F2 are said to be finitary strong equivalent if F1/θF1 =
F2/θF2
. Finitary strong equivalence will be denoted by ∼f .
Consider the scheme equation
ǫ · σ = (σ + σ) · ǫn,
where σ ∈ Σn stands for a box scheme 1 → n with no registers, and ǫn is the
mapping 2n → n by which ǫn(in + j) = j for i = 0, 1 and 1 ≤ j ≤ n. The
mapping ǫn is essentially the branch ǫ : 2 → 1 performed on a “block” of size
n, which is easy to assemble from the constants 1, ǫ, and x using sum and
composition. (See [3] for the details.) The system of these equations for all
σ ∈ Σ determines the relation of box unfolding (reduction) on Syn(Σ), whereby
the left-hand side schemes unfold into the right-hand side ones and vice versa,
when reduction takes place.
The following two theorems characterize finitary strong equivalence, and relate it to simulation equivalence.
Theorem 5.1 Finitary strong equivalence is the smallest congruence relation of
Syn(Σ) containing box unfolding/reduction.
Strong equivalence and finitary strong equivalence have nothing to do with the
exclusion of rippling in synchronous schemes, therefore they are meaningful for
all Σ-flowcharts. Obviously, Theorem 5.1 also remains true in the algebra Fl(Σ)
of all Σ-flowcharts.
Theorem 5.2 In Syna
(Σ), ∼f⊆∼.
The reader should make note of the fact that finitary strong equivalence does
not coincide with the congruence relation of Syn(Σ) defined by the identity
ǫ · h = (h + h) · ǫn , where h : 1 → n.
Unfolding/reduction is only allowed for individual boxes according to ∼f , and
not for all schemes h : 1 → n as suggested by the identity above. Consequently,
10
∼f fails to be fully invariant. In contrast, ∼s is fully invariant, and it can be
captured by the system of feedback theory identities described in [4]. Nevertheless, ∼f is still powerful enough to unfold cycle-free accessible schemes into
finite trees, hence the name “finitary strong equivalence”.
6 The main result
As we have seen in Theorems 4.2 and 5.2, retiming equivalence and finitary
strong equivalence are both contained in simulation equivalence. It is natural to
ask if there are any other ingredients present in ∼, or it is completely determined
by the join of these two congruences. This question is answered by Theorem 6.1
below.
Theorem 6.1 Simulation equivalence is the smallest strong congruence relation
of Syna
(Σ) containing retiming equivalence and finitary strong equivalence.
In [6], the join of ∼r and ∼s has been introduced by the name strong retiming
equivalence. On this analogy, the join of ∼r and ∼f is called finitary strong
retiming equivalence.
Theorem 6.1 is a direct consequence of Theorems 4.2, 5.2, and Statements 1,
2, and 3 below.
Statement 1. If F1 ∼ F2, then F1 and F2 are strong retiming equivalent.
Statement 2. If F1 ∼ F2, then fl(F1) ∼f fl(F2). (Recall that fl(F) denotes
the Σ-flowchart determined by F.)
Statement 3. If F1 and F2 are strong retiming equivalent such that fl(F1) ∼f
fl(F2), then F1 and F2 are finitary strong retiming equivalent.
Statement 1 was proved in [9]. The proofs of Statements 2 and 3 both rely
on the following lemma, the first part of which was already shown in [6].
Lemma 6.2 If schemes F1 and F2 are strong retiming equivalent, then there
exist schemes G1 and G2 such that Fi ∼s Gi, i = 1, 2, and G1 ∼r G2. Moreover,
if fl(F1) ∼f fl(F2), then G1 and G2 can be chosen in such a way that Fi ∼f Gi.
In the light of Lemma 6.2, Statement 3 is straightforward. The proof of Statement 2 is based on Lemma 6.2, Theorem 5.1, and some other results found in
[9, 6]. This proof is challenging.
7 Axiomatizing simulation equivalence
In this section we present an equational axiomatization of the equivalence relations considered, and relate our work to the results obtained in [14] on strictly
monoidal categories with feedback. In order to make the connection in a smooth
way, we shall simplify the discussion by assuming that the underlying monoidal
categories are just magmoids [1] satisfying the block permutation axiom [2, 11].
Such magmoids will be called symmetric.
11
A magmoid with feedback is a symmetric magmoid M equipped with a feedback operation ↑: M(1 + p, 1 + q) → M(p, q) satisfying the following axioms.
S1: ↑ (f + g) =↑ f + g for f : 1 + p → 1 + q, g : r → s;
S2: ↑
2
((x + p) · f) =↑
2
(f · (x + q)) for f : 2 + p → 2 + q;
S3: ↑ (f · (1 + g)) = (↑ f) · g for f : 1 + p → 1 + q, g : q → r;
S4: ↑ ((1 + g) · f) = g · (↑ f) for f : 1 + q → 1 + r, g : p → q.
These axioms were introduced originally in [2] to axiomatize flowchart schemes,
but they coincide with a reduced version of the axioms naturality (S3, S4), weak
naturality (S2), and superposing (S1) of [14] in the present simplified context.
The following two axioms have been added in [13].
S: ↑
m ((g + p) · f) =↑
l
(f · (g + q)) for f : l + p → m + q, g : m → l;
X: ↑ x = 1.
Axioms S and X are called sliding and yanking, respectively. It was proved
in [2] that the set of axioms {S1, S2, S3, S4, S5, X}, where
S5: ↑ 1 = 0 and ǫ · ⊥ = ⊥ + ⊥,
provide an axiomatization of flowchart schemes, with the assumption that the
loop vertex is unique. Dropping this assumption simply eliminates S5 from the
required axioms. Since S is trivially satisfied in the algebra of flowchart schemes
(with the loop vertex being unique or not), sliding comes as a consequence of
{S1, S2, S3, S4, X}. See also [13, Lemma 2.1] in the general framework of
monoidal categories. Sliding has also been considered in [8] by the name circular
feedback as one of the basic axioms for feedback theories (with no delay).
Yanking was replaced in [3] by the axiom
S6: ↑
2
((ǫ + p) · f) =↑ (f · (ǫ + q)) for f : 1 + p → 2 + q,
and it was proved that {S1,. . .,S6}, together with the axiom
S7: 01 · ∇ = 01
and another axiom S8: ↑ (ǫ · ∇n) = ⊥, which is irrelevant in our present discussion, provide an axiomatization of synchronous schemes.
Now we turn to describing the Circ construction in [14]. If M is a symmetric
magmoid, then Circ(M) is the magmoid in which the morphisms p → q are
isomorphism classes of pairs (f, l), where f : l + p → l + q in M, and where
an isomorphism from (f, l) to (g, m) is an isomorphism γ : m → l such that
(γ + p) · f = g · (γ + q). Without loss of generality we can assume that m = l
and γ is a permutation l → l. Morphisms in Circ(M) are called circuits. The
identity circuit p → p is p in M, and composition of circuits is essentially cascade
product of automata. Feedback of circuits is defined by observing the obvious
rule ↑ (↑
l
f) =↑
l+1 f (vanishing). See [5, 14] for the details. It was proved in [14]
that for every symmetric magmoid M, Circ(M) is a magmoid with feedback.
Moreover, Circ(M) is freely generated by M.
Let us extend the definition of ∼f by incorporating the equations 01 · σ = 0n
12
for all σ ∈ Σn. This allows for eliminating certain inaccessible boxes in Syn(Σ).
Based on Theorem 6.1, redefine ∼ as finitary strong retiming equivalence, that
is, the join of ∼f and ∼r. Consider the symmetric magmoid Df(Σ) of data
flowchart schemes as introduced in [3], and let T (Σ) be the free algebraic theory
generated by Σ, also considered as a symmetric magmoid with the constants 01
and ǫ having their natural interpretation.
For an arbitrary collection E of identities in our algebraic language, let ηE
denote the congruence relation of magmoids with feedback determined by E.
Combining the axiomatization results of [2, 3] with the Circ construction, we
obtain the following results.
Theorem 7.1
1. Circ(T (Σ))/ηS6,S7 ∼= Syn(Σ)/∼f ;
2. Circ(T (Σ))/ηS
∼= Syn(Σ)/∼;
3. Circ(T (Σ))/ηX
∼= Fl(Σ)/∼f ;
4. Circ(Df(Σ))/ηS6,S7 ∼= Syn(Σ);
5. Circ(Df(Σ))/ηS
∼= Syn(Σ)/∼r;
6. Circ(Df(Σ))/ηX
∼= Fl(Σ).
Notice that the equation ↑ (01 + 1) = 01 cannot be proved from the axioms
S1,. . .,S4, therefore the axiom S7 must be considered in 1 and 4 above. Equation
S7, however, is provable from S, so that S6 and S7 need not be considered when
S is present. (Essentially, S6 and S7 are the special instances of S in which the
morphism g is chosen as the constant ǫ and 01, respectively.) Moreover, as we
have seen earlier, S comes free whenever X is present.
The axioms S5 and S8 are not valid in Syn(Σ) by our present model. One
can easily prove, however, that {S1,. . .,S4, S6, S7} is a proper axiomatization of
our current Syn(Σ), which is in accordance with 4 above.
8 Conclusion
We have provided an algebraic characterization of simulation equivalence of synchronous systems. First we showed that simulation equivalence is a congruence
relation of the algebra of synchronous schemes. Then we identified the two major components of this relation: retiming equivalence and finitary strong equivalence. We have found that both of these equivalences are easy to characterize in
syntactical terms as appropriate congruence relations of the algebra of schemes.
Finally, we have proved that simulation equivalence is the smallest congruence
relation containing these two components. We have also provided an axiomatization of our equivalences relying on the Circ construction in [14].
Acknowledgements. The authors thank a referee for pointing out the work
of Katis, Sabadini, and Walters. The first author is also grateful to Bob Walters
for a helpful communication on this matter.
13
References
[1] A. Arnold and M. Dauchet, Th´eorie des magmo¨ıdes, RAIRO Inform. Th´eor.
12 (1978), 235–257 and 13 (1979), 135–154.
[2] M. Bartha, A finite axiomatization of flowchart schemes, Acta Cybernet. 2
(1987), 203–217.
[3] M. Bartha, An equational axiomatization of systolic systems, Theoret. Comput. Sci. 55 (1987), 265–289.
[4] M. Bartha, Foundations of a theory of synchronous systems, Theoret. Comput. Sci. 100 (1992), 325–346.
[5] M. Bartha, An algebraic model of synchronous systems, Information and
Computation 97 (1992), 97–131.
[6] M. Bartha, Strong retiming equivalence of synchronous systems, submitted
for publication.
[7] S. L. Bloom and Z. Esik, Axiomatizing schemes and their behaviors, ´ J.
Comput. System Sci. 31 (1985), 375–393.
[8] V. E. Casanescu and Gh. Stefanescu, Feedback, iteration and repetition,
Research Report 42, National Institute for Scientific and Technical Creation,
Bucharest, 1988.
[9] B. Cirovic, Equivalence relations of synchronous systems, PhD Dissertation,
Memorial University of Newfoundland, 2000.
[10] C. C. Elgot, Monadic computations and iterative algebraic theories, in Logic
Colloquium ’73, Studies in Logic and the Foundations of Mathematics (H.
E. Rose and J. C. Shepherdson, eds.), pp. 175–230, North Holland, Amsterdam, 1975.
[11] C. C. Elgot and J. C. Shepherdson, An equational axiomatization of the
algebra of reducible flowchart schemes, IBM Research Report RC 8221.
[12] G. Gr¨atzer, Universal Algebra, Springer-Verlag, Berlin, 1968, 1979.
[13] A. Joyal, R. Street, and D. Verity, Traced monoidal categories, Math. Proc.
Camb. Phil. Soc. 119 (1996), 447–468.
[14] P. Katis, N. Sabadini, and R. F.C. Walters, Feedback, trace, and fixed-point
semantics, Theoret. Informatics Appl. 36 (2002) 181–194.
[15] C. E. Leiserson, J. B. Saxe, Optimizing synchronous systems, J. VLSI Comput. Systems 1 (1983), 41–67.
14
A depth-first algorithm to reduce graphs
in linear time
Mikl´os Bartha
Department of Computer Science
Memorial University of Newfoundland
St. John’s, NL, Canada, A1B 3X5
Email: bartha@mun.ca
Mikl´os Kr´esz
Faculty of Education
University of Szeged
6721 Szeged, Hungary
Email: kresz@jgypk.u-szeged.hu
Abstract—A redex in a graph G is a triple r = (u, c, v) of
distinct vertices that determine a 2-star. Shrinking r means
deleting the center c and merging u with v into one vertex.
Reduction of G entails shrinking all of its redexes in a
recursive way, and, at the same time, deleting all loops that are
created during this process. It is shown that reduction can be
implemented in O(m) time, where m is the number of edges
in G.
Keywords-graphs and matchings; depth-first trees; shrinking
edges in graphs;
I. INTRODUCTION
The problem dealt with in this paper is of an elementary
nature: given a graph G, shrink all 2-stars (redexes) in G
in a recursive way to obtain a reduced graph r(G) free
from subdividing vertices. During this process shrinking may
introduce loops in the graph. Such loops are eliminated
on the fly, giving rise to potential new redexes on which
shrinking continues.
As an example, consider the graph G0 of Fig. 1 containing
four redexes centered at vertices 4, 7, 11, and 14. Shrinking
the ones at 11 and 14 successively gives rise to a new redex
a in the resulting graph G2. In graphs G0 and G1 of Fig. 1,
small arrows indicate which two vertices are collapsed in one
step of the shrinking procedure. Shrinking at a and 7 in G2
yields G3, showing another new redex b. Finally, shrinking
at 4 and b in G3 results in a single edge, which is r(G0).
It is easy to come up with an O(n2) algorithm for the
above problem, where n denotes the number of vertices
in G. One would simply consider the adjacency matrix of
G, and keep track of the current set of redexes in a list.
The implementation of shrinking one redex in O(n) time is
then straightforward. See [2], [9] for more details. Finding
a linear-time algorithm for graph reduction is much harder.
The key idea is the following. First isolate groups of redexes
that are connected at one end. Such groups are called groves.
In our example, the redex groves are {11, 14}, {7}, and {4}.
Then find out, without performing any actual shrinking, if
a particular grove reduces to a new redex, like the grove
{11, 14} in our example. Such a grove is called an implied
redex. Locate and reduce implied redexes recursively, until
no more can be found. Finally, shrink the remaining groves
in an arbitrary order to obtain r(G).
3
Figure 1. A reduction example
Reduction is matching invariant, so it can be used as a
speed-up in matching algorithms. Due to its linear complexity, the gain can be quite substantial for sparse graphs.
The situation looks even better in some experimental greedy
matching algorithms, cf. [12], which have a linear time complexity to begin with. Moreover, reduction does not change
the number of maximum/perfect matchings in graphs, so
that it is helpful in the process of counting the number of
such matchings. In particular, if a connected graph reduces
to a single edge, like our example graph G0, then it has a
unique perfect matching. Even though the converse of this
statement is not true, reduction is still a helpful technique
in finding out if a graph has a unique perfect matching. The
currently fastest unique perfect matching algorithm [7] runs
in O(m log4
n) time on a graph with n vertices and m edges.
Reduction also plays an important role in the study of
certain matching-based molecular switching devices called
soliton automata [1], [2], [5]. As it was shown in [2], a
soliton automaton defined by an elementary graph G is
deterministic iff G reduces to a graph not containing evenlength cycles. Yet another application of reduction along this
line concerns finding a flexible maximum matching in graphs
[8], [3]. With the help of the present algorithm it becomes
possible to effectively find such a matching in linear time
(whenever it exists), provided that an arbitrary maximum
matching has previously been constructed for the graph.
Relating to other fundamental graph theory concepts,
reduction simplifies the problem of finding a minimum size
2-(edge) connected spanning subgraph of a 2-connected
graph [4], and that of finding a Hamiltonian cycle. Both
problems are known to be NP-complete. The argument is as
follows. The property of being 2-connected is preserved by
reduction, and the desired 2-connected spanning subgraph of
G can easily be retrieved from that of r(G). If both G and
r(G) are also 2-vertex-connected, then this process is trivial,
since each redex must be part of any spanning subgraph. For
a graph that is not 2-vertex-connected, its minimum size 2-
edge connected spanning subgraph is assembled from that
of its 2-vertex-connected components (blocks). Dynamically,
detaching such a component from G during reduction is
analogous to eliminating a loop, so that it might create a new
redex in the remainder graph and in the detached component
as well. Thus, the algorithm becomes recursive. This type of
recursion can be handled by a suitable modification of our
Algorithm 2 described in Section 5.
As to Hamiltonian cycles it is obvious that, in order for
G to have a Hamiltonian cycle, no redex grove can have a
branch in it. That is, each maximum independent redex grove
R of G must follow a path pR, or G itself must be covered
by an even-length cycle composed of individual redexes. In
the latter case G has a trivial Hamiltonian cycle. Otherwise
every Hamiltonian cycle of G must cover the whole path
pR for each grove R. Thus, G can be replaced by the graph
Gp in which the internal points of p are deleted and the two
endpoints are connected by a new redex, that is, a single
edge with a subdividing vertex on it.
Reduction might turn out to be useful in other graph
theory applications as well. We believe, however, that the
algorithm presented in this paper is interesting by itself. The
shrinking technique applied in our algorithm bears a close
resemblance to shrinking e.g. in the Edmonds matching
algorithm [6], [11], and, in general, it can be used to shrink
graphs along a designated set of edges, which are specified
simply by putting a subdividing vertex on them.
II. A GREEDY ALGORITHM TO SHRINK REDEX GROVES
Our notation and terminology on graphs will follow [11].
Let us fix a connected undirected graph G = (V (G), E(G))
for the rest of the paper. Loops and multiple edges are
allowed to occur in G. A vertex v ∈ V (G) is called
subdividing if its degree is 2, external if the degree of v is 1,
and internal if it is not external. For a subset X ⊆ V (G) of
vertices, G[X] (or just [X] if G is understood) will denote
the subgraph of G induced by X. A redex in G is a triple
r = (u, c, v) of distinct vertices such that c is subdividing
and adjacent to both u and v. The vertex c is called the center
of the redex, while u and v are the two focal vertices (or just
focuses) of r. The two edges incident with c are called focal,
too. Shrinking a redex means deleting its center and merging
its two focal vertices into one sink vertex. Graph G is called
reduced if it does not contain redexes or loops. A reduced
graph r(G) is obtained from G by shrinking all redexes and
removing all loops in it in a recursive fashion. As it was
proved in [3], r(G) is unique up to graph isomorphism.
Let R be a set of redexes in G. A vertex set S ⊆ V (G) is
covered by R if S ⊆ ∪R. The set R is a redex cover in G if
it covers every redex in G. Two redexes are connected if they
have at least one focal vertex in common. This relationship
defines a graph Rd(G) on the set of G’s redexes as vertices,
whereby an edge exists between r1 and r2 iff r1 is connected
to r2. A redex grove is a non-empty set R of redexes such
that Rd(G)[R] is connected. Two redex groves are said to
be equivalent if they cover the same set of vertices in G.
A redex grove R is independent if it has no equivalent
proper sub-grove, maximal independent if it is independent
and cannot further be extended without losing this property,
and maximum if there exists no other grove R such that
∪R ⊂ ∪R
. Clearly, every redex grove has an equivalent
independent sub-grove, which may not be unique.
Figure 2. Redex groves
Examples. A triangle graph is covered by a redex grove
consisting of three interconnected redexes, each of which is
independent by itself. The graph in Fig. 2a has two maximal
independent redex groves: R1 = {(u, c1, v1),(u, c2, v2)}
and R2 = {(v, v2, c2)}, both of which are maximum as
well. The set R1 ∪ R2 is not a redex grove, as it does not
define a connected subgraph in Rd(G). In the graph of Fig.
2b, R = {(u1, c, v2)} is maximal but not maximum, for
the grove S = {(v1, u1, c),(c, v2, u2)} covers more vertices
than R.
In order to describe our reduction algorithm we need a
few technical statements on redex groves.
Proposition 2.1. If R is an independent redex grove, then
no vertex v ∈ ∪R is the center of one redex and a focus of
another at the same time.
Proof. Let us assume, on the contrary, that there exist
redexes r0 = (u, c, v) and r1 = (c, x, y) in R. Clearly, x = u
or x = v. Say x = v. Then u cannot be the center of some
other redex in R, because this would make r0 dispensable.
Since R is a grove, there exists a chain of connected redexes
r1,...,rn (n ≥ 1) such that u is a focal vertex of rn. (Note
that n = 1 identifies a triangle situation.) Thus, R − r0
covers the same vertices as R, which contradicts R being
independent. ✷
Corollary 2.2. Let R be a maximal independent redex
grove and v ∈ ∪R be a focal vertex according to R. If
(v, c1, x1),(x1, c2, x2),...,(xn−1, cn, xn) is a sequence of
connected redexes in G (not necessarily contained in R),
then xn ∈ ∪R.
Proof. Let S denote the set of redexes {(xi−1, ci, xi) | 1 ≤
i<n} with x0 = v. Then R∪S is a redex grove, and, since
R is maximal, R ∪ S covers the exact same vertices as R.
Thus, xn ∈ R. It may happen, though, that xn is designated
as center in R. ✷
The following lemma characterizes the relationship between two overlapping maximal independent redex groves.
With a slight ambiguity, ∪R will also denote the union of
R’s redexes as subgraphs of G. If ambiguity does arise, this
meaning will be re-enforced by writing graph ∪R.
Lemma 2.3. Let R and S be two non-equivalent maximal
independent redex groves in G. Then the graph ∪R ∩ ∪S
consists of n vertex-disjoint paths running exclusively on
subdividing vertices such that one of the following three
conditions is met.
(i) n = 0, so that the graphs ∪R and ∪S are themselves
vertex-disjoint;
(ii) n = 1, ∪R ⊂ ∪S (∪S ⊂ ∪R), and the only overlapping path between ∪R and ∪S is ∪R (respectively,
∪S) itself, consisting of an odd number of subdividing
vertices;
(iii) n ≥ 1, ∪R and ∪S are not comparable, and each
overlapping path is running on an even number of
subdividing vertices.
Proof. By Proposition 2.1, each vertex v ∈ ∪R and u ∈
∪S is either focus or center in R and S, respectively. It
cannot happen that x ∈ ∪R ∩ ∪S is focus according to both
R and S, because Corollary 2.2 would then imply that R
and S are equivalent. Thus, every vertex in ∪R ∩ ∪S is
subdividing. Consequently, ∪R ∩ ∪S indeed consists of n
disjoint paths, as stated. (Even-length cycles are excluded,
for ∪R 	= ∪S). Assuming that n ≥ 1, let p be any of these
paths. Observe that p has at least two vertices. Consider the
vertices u(p) and v(p) outside p that are adjacent to p from
its two ends. See Fig. 3. Since R and S are maximal, the
degree of these vertices is different from 2, and each one is
focus according to either R or S. If they are focus according
to the same grove, say S, then p coincides with ∪R. See
Fig. 3a, and recall that the grove R is connected. Hence we
have (ii) above. Notice that in this case R is not maximum.
Figure 3. Overlapping maximal redex groves
Figure 4. A flower graph
If u(p) and v(p) are focus according to different groves,
then p consists of an even number of vertices. See Fig. 3b.
Moreover, u(p) 	= v(p), implying that ∪R and ∪S are not
comparable. The proof is now complete. ✷
An immediate consequence of Lemma 2.3 is that a
maximal independent redex grove R is not maximum only
if ∪R is a path running on an odd number of subdividing
vertices.
Definition 2.4. Graph G is internally redex-covered if there
exists a redex grove R such that, with the exception of some
external vertices, R covers all of V (G).
Definition 2.5. A flower graph is a loop-free graph G having
a central vertex v such that each edge e ∈ E(G) is incident
with v, and the multiplicity of e is at most 2. Edges with
multiplicity 1 and 2 are called sepals and petals, respectively,
and a flower with n sepals (n ≥ 0) and no petals is called
an n-star. See Fig. 4.
Proposition 2.6. If G is internally covered by an independent redex grove R, then r(G) is a flower graph, which is
a star iff the graph ∪R is a tree.
Proof. Clearly, R is maximal. Pick a redex r ∈ R arbitrarily.
Starting from r, shrink the redexes in R one-by-one in
such a way that redexes corresponding to two consecutive
shrinkings be always connected. As long as ∪R follows
a tree structure, all redexes collapse into one sink vertex.
When, however, a vertex comes up repeatedly as focus in
this sequence in a circular fashion, its underlying redex s
will no longer exist, because its two focuses have previously
been joined. In this case s contributes a petal to the reduced
graph, and we continue as if s did not exist. The statement
of the proposition is now obvious. ✷
Corollary 2.7. An internally redex-covered graph can be
reduced in linear time.
Proof. In the procedure outlined above, fix u to be either
focal vertex of the initial redex r, and perform shrinking
in such a way that u keeps serving as the sink vertex. The
shrinking of one redex then costs as much as the number
of edges incident with the “other” focus (i.e., other than u).
Thus, the overall cost of reduction is indeed linear. ✷
An important observation must be made regarding the
reduction procedure presented in the proof of Proposition 2.6
above. We say that G directly reduces to the graph d(G)
obtained from G after having considered all redexes in R
for shrinking. Clearly, d(G) is still a flower graph, which
has as many sepals as the number of external vertices left
uncovered by R. Moreover, d(G) coincides with r(G), unless d(G) is a 2-star, i.e., a redex. According to Lemma 2.3,
this can happen in two different ways:
1. The grove R is maximum.
2. The grove R is not maximum, ∪R is a path p running
on an odd number of subdividing vertices, and G is
obtained from p by attaching the vertices u(p) and
v(p) described in the proof of Lemma 2.3 to the two
endpoints of p.
In the latter case, the whole graph G can be covered by
a different maximum redex grove R
, in which both u(p)
and v(p) appear as focal vertices. Thus, in order to define
d(G) in an unambiguous way, one must assume that R is
maximum, not just maximal. Ensuring that R be maximum
is straightforward: include a redex from the “side”, that is,
one having a non-subdividing vertex. If this is not possible,
then G is a cycle, so that the problem does not even arise.
Let R be an arbitrary redex grove in (a general) graph
G, and construct the graph I(R) by augmenting [∪R] in
the following way. For each vertex v ∈ ∪R and edge e in
G − R incident with v, introduce a new external vertex ve
and connect it to v. It is obvious that I(R) is internally
covered by R. Moreover, if R is maximum in G, then it is
such in I(R).
Definition 2.8. An implied redex in G is a maximum
independent redex grove R such that the graph d(I(R)) is
a redex.
Theorem 2.9. If G does not contain implied redexes, then
r(G) can be constructed in linear time.
Proof. Without loss of generality we can assume that G is
not a cycle. Let R = {R1,...,Rm} be a system of pairwise
non-equivalent maximum independent redex groves such
that ∪R is a redex cover in G. Since R1 is not an implied
redex, its direct reduction inside G does not introduce
a new redex into the resulting graph G1. Moreover, by
Lemma 2.3, the groves Rj , j > 1 will either remain intact,
disappear, or be truncated to some maximum independent
groves R
j in G1 such that d(I(R
j )) = d(I(Rj )). Thus,
G1 does not contain implied redexes either, or at least there
is none participating in the covering system R1 inherited
from R. (See also Corollary 2.10 below.) In this way, the
reduction of G entails only the direct reduction of the
groves R1,...,Rm, or whatever remains of them during
this process.
As to the desired algorithm, choose an initial redex r in
such a way that it includes a non-subdividing vertex. (Recall
that G is not a cycle.) Locate a maximum independent redex
grove R1 containing r, and perform its direct reduction. By
Corollary 2.7, this takes linear time regarding the size of
[∪R1]. Repeat the process until no more redexes are found. It
should be evident that the time complexity of this algorithm
is indeed O(|E(G)|). ✷
Corollary 2.10. Let R = {R1,...,Rm} be a system of
pairwise non-equivalent maximum independent redex groves
such that ∪R is a redex cover in G. If none of Ri, 1 ≤ i ≤ m
is an implied redex, then G does not have implied redexes.
Proof. By way of contradiction, assume that G does have
an implied redex R. Join R to R, and notice that it is
not equivalent to any of the groves Ri. Repeat the direct
reduction process eliminating the groves Ri. According to
Lemma 2.3, the reduction of any Ri overlapping with R can
only shorten R without being able to completely knock it
out. Moreover, shortening R will still keep it as an impied
redex. Thus, after the reduction, r(G) will still contain an
implied redex, which is impossible. ✷
The condition that G does not contain implied redexes
is crucial in Theorem 2.9. If G does contain such redex
groves, then, following the procedure outlined above, we
may locate “good” groves (i.e., ones that are not implied
redexes) first, and implied redexes later. In this way we
have merely reproduced our original problem at the “macro”
level, where the vertices are essentially the centers of the
flower graphs d(I(Ri)), together with those old vertices
of G not covered by ∪R. Of course, the complexity of
reduction accumulates in a recursive way, resulting in a
non-linear-time algorithm. This argument also explains why
one cannot simply go ahead and shrink redexes in G in an
arbitrary order, still hoping to finish in linear time. Indeed,
even though the shrinking of one redex r costs only as much
as the number of edges incident with one focus of r, there is
no guarantee that each edge will contribute only a constant
number of times this way to the overall count.
Our task is therefore to find implied redexes (even recursively emerging ones) in linear time, replace each such
grove by a single redex, and then come back to the greedy
algorithm described in Theorem 2.9 to finish the job.
III. FLIPPING BRACKETS IN DEPTH-FIRST TREES
A depth-first tree (DFT, for short) is a rooted spanning
tree TG such that every edge e ∈ E(G) falls into one of the
following two groups.
(i) tree edges, which belong to the tree TG;
(ii) back edges, which connect vertices with their ancestors in TG.
It is well-known that for every vertex v ∈ V (G), G has a
DFT rooted at v, which can be constructed in linear time.
We shall assume that this construction deletes all loops that
G might originally have. Notice that, among a number of
parallel edges in G, at most one can be a tree edge. With a
slight ambiguity we shall, in principle, identify G with the
tree TG and say that G is a DFT with root(G) = v. A DFT
G is called open if its root is an external vertex. In this case
the vertex adjacent to root(G) is called the ground of the
tree, denoted gr(G). In the sequel we shall assume that our
DFT G is open. From the reduction point of view generality
is not jeopardized, for if G were not open, then attaching
an extra redex to root(G) will make it such. The resulting
graph also reduces to r(G), essentially in the same amount
of time.
Figure 5. Depth-first tree concepts
Redexes come in two distinctive forms in a DFT G. A tree
redex is one consisting of two tree edges, and a transversal
redex (or bracket, for short) is a redex having one tree and
one back edge. Notice that, since G is open, every tree redex
r is “vertical” in G, meaning that the center of r has only
one immediate descendant (son), namely a focus of r. Thus,
if r = (u, c, v) is a redex (tree or not), then u<v, where <
denotes the ancestor relationship in TG. We say that r spans
from u to v, or r is a redex u → v, with u being the bottom
and v being the top of r.
When identifying redexes in a DFT G, we shall follow a
bottom-up alignment. This means that, during a bottom-up
sweep, a subdividing vertex v always becomes the center
of a redex, provided that v is not the peak of a petal and
its son (if any) has not previously been chosen as center.
In particular, a subdividing leaf of G (i.e., that of TG)
always identifies a bracket (or petal), even when its father
is subdividing, too. See Fig. 5 for an illustration of the DFT
concepts introduced so far. It is easy to see that the rule
of bottom-up alignment identifies a redex cover R(G) in
G in a unique way. Moreover, each connected component
of Rd(G)[R(G)] is a maximum independent redex grove.
(Recall that Rd(G) is the redex graph of G.)
Let r and s be redexes in R(G) spanning from a common
bottom vertex u to v and x, respectively, such that v<x
(i.e., x is above v). Flipping s through r means replacing the
(necessarily transversal) redex s by a new bracket s : v →
x. Even though the resulting DFT G is not isomorphic to
G, shrinking both r and s in G is isomorphic to shrinking
both r and s in G
. Thus, flipping is reduction invariant.
Furthermore, s ∈ R(G
), and the maximum (independent)
redex grove in R(G
) containing s and r is obtained by
substituting s for s in the corresponding grove in R(G)
containing s and r. The union of this grove (as a subgraph
of G
) is a tree iff the union of the corresponding grove is
such in G.
A set R of redexes in a DFT G is said to be regular
if no vertex of G occurs more than once as the bottom of
some redex in R, except when the redexes sharing their
bottom share their top as well. Let flip (G) denote the graph
obtained from G by successive flips, which eliminate all
ties between bottom vertices of different redexes in R(G),
if at all possible. Clearly, flip (G) is uniquely determined
and R(flip (G)) is regular.
Proposition 3.1. The DFT G is internally redex-covered by
R(G) with d(G) being a star graph iff every internal vertex
of flip (G), with the possible exception of the ground vertex,
is the bottom or center of exactly one redex in R(flip (G)).
Proof. By the nature of flipping it is sufficient to prove that,
whenever R(G) is regular, G being internally redex-covered
by R(G) in such a way that d(G) is a star is equivalent to
having each non-center internal vertex of G different from
gr(G) appear as the bottom of a unique redex in R(G).
If R(G) is regular, then the graph ∪R(G) is cycle-free as
a subgraph of G iff no two redexes in R(G) have the same
span. Given this fact, the condition that each non-center
internal vertex, excluding gr(G), appears as the bottom of
some redex in R(G) is equivalent to saying that ∪R(G)
has one more vertices than edges, and covers all internal
vertices of G (including gr(G)). In turn, this is equivalent
to ∪R(G) being a tree, so that our statement follows from
Proposition 2.6. ✷
We use the following bottom-up algorithm to check if G
is internally redex-covered with d(G) being a star graph.
Algorithm 1. Execute Check vertex(root(G)), and see if
each internal vertex of G different from gr(G) is marked
either center or bottom, but no vertex is marked doomed.
In Algorithm 1, the pseudo-code for the two recursive
procedures Check vertex(v) and Flip(u, v) is given below.
In Flip(u, v), u.span is a pointer associated with the bottom
of a redex r, which is intended to point – beyond a cascade
of redexes to be flipped through – to the top v of r. As part
of the flip, the span of each redex in the cascade is extended
(in principle) up to v in order to speed up flips that are yet
to come.
Check vertex(v):
if leaf(v) then
{ if deg(v)=2 and not petal(v)
then mark center(v) }
else { for each son u of v:
Check vertex(u);
if deg(v)=2 and not marked center(son(v))
then { mark center(v);
Flip(son(v),father(v)) }
else for each back edge (x, v) from a leaf x:
if marked center(x) then
Flip(father(x),v) }
Flip(u, v):
if u = v then mark doomed(u)
else { if not marked bottom(u)
then mark bottom(u)
else Flip(u.span,v);
u.span:=v }
Theorem 3.2. Algorithm 1 correctly decides if G is an
internally redex-covered graph for which d(G) is a k-star.
If so, the number of vertices left unmarked by the algorithm
is k + 1. The algorithm runs in linear time.
Proof. The first statement, with regard to correctness and
the number k, is obvious by Proposition 3.1. As to linearity,
observe that the only concerning issue is the number of
times the pointers u.span must be assigned on the last line
of Flip(u, v). Not counting the very first assignment of these
pointers, let T (k, n) denote the total number of pointer
assignments needed to process a DFT G with |R(G)| = n,
such that the first k redexes follow a regular arrangement
without duplication. Clearly, T (n, n)=0. Let r be the
(k + 1)st redex. If r must be flipped through a cascade of
l redexes, then this maneuver costs l pointer assignments.
On the other hand, r will kill the whole cascade, so that the
situation after the adjustment is the same as if we had only
n − l redexes to begin with, out of which the first k − l + 1
follow a regular arrangement with no duplication. Thus,
T (k, n) ≤ max({l+T (k−l+1, n−l) | 1≤l ≤k}, T (k+1, n))
if k<n. From this formula, T (k, n) ≤ n is provable by a
trivial induction on n − k. ✷
IV. IDENTIFYING AND REDUCING IMPLIED REDEXES IN
DEPTH-FIRST TREES
Topologically, an implied redex R manifests itself in three
different forms in a DFT G.
1. The graph d(I(R)) is a tree redex. In this case [∪R] is
surrounded by a subtree S from the bottom and a supertree
H from the top, so that the two edges eb and et of the 2-
star d(I(R)) provide a handle to S and [∪R], respectively,
as tree edges in G. See Fig. 6a. Clearly, the endpoints of
eb and et incident with [∪R] must be focus in R, while the
lower endpoint of eb, as well as the higher endpoint of et,
must not be subdividing.
2. The graph d(I(R)) is a bracket. In this case [∪R] itself
is a subtree, connected to a supertree H from the top by the
tree edge et. The edge eb is a back edge, which originates
from a focus in R. See Fig. 6b.
3. The graph d(I(R)) is an “illegal” horizontal redex. In this
case [∪R], as a supertree, includes root(G), and the edges
e1, e2 of the 2-star d(I(R)) are both tree edges. Notice that
gr(G) is now center in R. See Fig. 6c.
Figure 6. Implied redexes
Once an implied redex R is identified during a bottom-up
sweep of G, it is reduced as follows.
Case 1: R (i.e., d(I(R))) is a tree redex. Each vertex of R,
except the lower endpoint z of et, is marked “done”, and
practically forgotten. The lower endpoint u of eb is marked
bottom, and u.span is set to the upper endpoint of et. Vertex
z is marked center, and the bottom-up sweep continues.
Case 2: R is a bracket. Again, each vertex of R, except
the lower endpoint z of et, is forgotten, and the back edge
eb is moved in such a way that its lower endpoint coincide
with z. If the upper endpoint v of et is the same as that of
eb, then v is marked doomed (petal), otherwise it is marked
bottom. Vertex z is classified as leaf, marked center (if v is
not doomed), and the bottom-up sweep continues.
Case 3: R is illegal. Now we do not attempt to replace
d(I(R)) with a single redex, since this would trigger a topdown process of detecting new illegal redexes. Rather, we
“exclude” R by deleting its root. Notice that this cannot be
done a priori, for gr(G) may not originally have been a
subdividing vertex, just popped up as an implied tree redex.
Let G denote the shortened graph. Since the bottom-up
sweep has ended, G will have no more implied redexes.
Construct r(G
) by the greedy algorithm of Theorem 2.9,
using the redex cover R(G
) broken down into a system
R1,...,Rm of maximum independent redex groves following the bottom-up alignment. (This system is actually
constructed in a natural way during the bottom-up sweep
detecting the implied redexes.) The vertex w = root(G
) is
not covered by R(G
), therefore it is still present in r(G
) as
an external vertex. Delete w, and reduce the resulting graph
– which may contain at most one redex – in linear time.
Clearly, the end-result is the graph r(G).
V. DETECTING IMPLIED REDEXES IN DEPTH-FIRST
TREES
We are now left with the most difficult question of how
to detect implied redexes in a systematic way during a
single bottom-up sweep of G. With this question answered,
recursion – as described in Section 4 – will roll up all
implied redexes and make G free of them.
Let u ≤ v be two vertices in V (G). The interval
determined by the pair (u, v) is the set I(u, v) = {x ∈
V (G)|u ≤ x ≤ v}. Again, if no confusion arises, I(u, v)
will also stand for the line [I(u, v)] as a subgraph of G
(rather, TG, mind the back edges). The set of principal
intervals in G, relative to R(G), is the set P(G) of intervals
spanned by all tree redexes and back edges of G. Thus, any
given principal interval might be classified in three different
ways: a tree redex interval, a bracket interval, or an ordinary
back-edge interval.
For a vertex v ∈ V (G), let P(v) denote the set of
principal intervals of G running entirely within the subtree
determined by v. If u is a vertex in this subtree not covered
by any interval in P(v), then add {u} to P(v) as a degenerate principal interval. If S is a set of principal intervals,
then ∪S is called a region, and a maximal connected subset
of ∪S (as a subgraph of TG) is called a domain of that
region. Clearly, every region is the union of pairwise disjoint
domains. For every vertex v ∈ V (G), let Dv denote the
domain of ∪P(v) containing v, and denote the restriction
of R(G) to Dv by R(G)/v. According to the description of
implied redexes in Section 4, the following statement is an
immediate consequence of Proposition 3.1.
Proposition 5.1. Let v be an arbitrary vertex in G, and
assume that R(G) is regular. In order for Dv to form an
implied redex in G it is necessary that each vertex of Dv −v
be the center or bottom of a unique redex in R(G)/v.
Our strategy to detect implied redexes during a bottom-up
sweep of G is based on Proposition 5.1. For every non-center
vertex v, we locate the domain Dv and calculate its size
size (Dv). At the same time, we also count the number of hits
hit(Dv) by center or bottom vertices of redexes inside Dv,
together with the numbers down (Dv) and up (Dv) of edges
leaving Dv downwards and upwards, respectively. During
this process, we impose regularity by flipping and mark
any duplications of redexes doomed as in Algorithm 1. The
condition that identifies an implied redex is then:
hit(Dv) = size (Dv) − 1, up (Dv)=1 and
down (Dv)=1 for a tree redex;
hit(Dv) = size (Dv) − 1, up (Dv)=2 and
down (Dv)=0 for a bracket; and
hit(Dv) = size (Dv) − 1, up (Dv)=0 and
down (Dv)=2 for an illegal redex.
Furthermore, the tree edge leaving Dv upwards must not
lead to a subdividing vertex (center), and, in case of a
tree redex, the tree edge leaving Dv downwards must not
be the focal tree edge of a bracket whose top is above
v. (Remember that an implied redex is a maximum redex
grove.) Also, no vertex in Dv can be marked doomed.
To describe our algorithm we use a straightforward attribute grammar terminology [10]. The attributes v.size ,
v.hit , v.down , and v.up – representing size (Dv), hit(Dv),
down (Dv), and up (Dv) – are assigned to each non-center
vertex v, and the tree G is decorated by the values of these
attributes calculated at each such vertex. The method of
calculation is the same for each attribute, therefore we only
deal with v.hit here, leaving the elaboration of the other
three attributes to the reader.
Calculating v.hit , assuming that R(G) is regular:
Step 1. Initialize v.hit := 0 and set
S := ∪(P(u)|u is a son of v).
Step 2. For each tree redex interval I(u, v):
increment v.hit by 2;
add I(u, v) to S.
Step 3. For each remaining non-degenerate principal interval
I(u, v) ∈ P(v):
Let Dv1 ,...,Dvn , n ≥ 0 be those domains (including
degenerate ones) of region ∪S that intersect I(u, v) but do
not contain v.
increment v.hit by n
i=1 vi.hit ;
increment v.hit by 2 if I(u, v) is a bracket interval;
add I(u, v) to S.
The major technical difficulty in implementing this algorithm in linear time is to keep track of the domains of the
region ∪P(v). The following code will accomplish this goal.
Process vertex(v):
{ if leaf(v) then
{ if deg(v)=2 and not petal(v)
then mark center(v) }
else { v.hit:=0;
for each son u of v:
Process vertex(u);
if deg(v)=2 and not marked center(son(v))
then { mark center(v);
Flip(son(v),father(v));
father(v).hit:= father(v).hit+2;
Interval(son(v),father(v)) }
else for each back edge (x,v):
{ if marked center(x) then
{ Flip(father(x),v);
v.hit:=v.hit+2 };
Interval(x, v) }
};
if implied redex(v) then reduce(v)
/* as described in Section 4 */ }
Interval(u, v):
do while not (u = v or u.jump=v)
if marked center(u) then u:=father(u)
else { x:=u;
/* x is a swap variable */
if not marked seen(u)
/* domain top is reached */
then { mark seen(u);
v.hit:=v.hit+u.hit;
u:=father(u) }
else u:=u.jump;
/* go find the top */
x.jump:=v
/* adjust old u.jump */ }
The procedure Flip(u, v) is adopted from Section 3.
Algorithm 2. Process G by executing the procedure Process vertex(root(G)), resulting in a graph G free from
implied redexes. Forget the DFT structure, and apply the
greedy algorithm of Theorem 2.9 on G to obtain r(G),
taking into account the adjustment described under Case 3
of Section 4.
Theorem 5.2. Algorithm 2 constructs r(G) in linear time.
Proof. The correctness of Algorithm 2 should be evident.
The pointers x.jump in the procedure Interval(u, v) are
used to locate the top of the domain in the current region
containing x. At every point in time, whenever x is already
marked seen, x.jump points to the top v of a principal
interval I(u, v) in the current range for which:
1. vertex x lies on I(u, v) − v;
2. there is no other interval I(u
, v
) also containing x
such that v<v and x is the closest common ancestor of u
and u
.
Thus, the top of the domain in question can always be
reached by a number of jumps through the jump pointers.
The top is recognized when an unseen non-center vertex x is
reached for which x.jump is not yet defined. It is at this point
where the attributes must be considered for addition. The
Boolean function implied redex(v) will check the attributes
at vertex v for the conditions that identify an implied redex,
as described in the paragraph following Proposition 5.1. As
part of the check, it will also see if Dv is a maximum
redex grove, that is, the two edges leaving Dv do not lead
to subdividing vertices. If an edge is leaving downwards,
then its other endpoint will have been marked center if it is
subdividing. For a tree edge leaving upwards to the father
of v, this condition is straightforward to check.
The question of linearity again boils down to how many
times the pointers x.jump must be assigned on the last line of
Interval(u, v). The situation is analogous to the one handled
in the proof of Theorem 3.2 in connection with the span
pointers. Let I1 = I(u1, v1) and I2 = I(u2, v2) be principal
intervals in P(G). We say that I2 overlaps I1 if v1 < v2 and
u2 ≤ x for some vertex x ∈ I1 − v1. The lowest possible x
is then called the split of I2 from I1.
In our argument we shall consider groups of principal
intervals according to the equivalence relation I(u, v) ∼
I(u
, v
) iff v = v
. The top of an interval group is the
common top vertex of its intervals. We say that two groups
are non-overlapping if their tops are not comparable in
TG, or they are but no interval from the higher group
overlaps any interval from the lower one. Not counting the
very first assignment of the pointers x.jump, let T (k, n)
denote the total number of assignments to these pointers
that is necessary to process a DFT G with n groups of
principal intervals, such that the first k groups are pairwise
non-overlapping. Observe that T (n, n)=0. (Remember
that x.jump always points to the top of the interval I1
containing x for which there is no overlap between I1 and
a higher interval I2 such that the split of I2 from I1 is
x, and we ignore the very first assignments.) Consider an
interval I(u, v) from the (k + 1)st group, and assume that
the processing of I(u, v) costs l ≥ 1 pointer assignments
that count (as second assignments). Then I(u, v) overlaps
l intervals from distinct groups among the first k ones, so
that the pointer assignments incur at the split of I(u, v) from
these intervals. Let I(u1, v1),...,I(ul, vl) be these intervals
such that v1 <...<vl < v. See Fig. 7a, where k = l = 2,
and the split vertices are x and v1.
From the point of view of continuation, the situation after
processing I(u, v) is equivalent to having only n − l + 1
interval groups to start with, out of which the first k − l + 1
are non-overlapping. See Fig. 7b. In essence, I(u, v) kills the
whole cascade of interval groups containing the overlapped
intervals, except for the lowest one containing I(u1, v1),
whose range is extended from v1 to vl. By killing I(ui, vi),
i > 1 and its group, we mean lifting the whole subtree
Txi below the split xi of I(u, v) from I(ui, vi) — deprived
from its branches starting out on I(u, v) — to vl. All back
edges originating from Txi , too, are lifted along with Txi .
Furthermore, every back edge currently pointing to vi is
switched to vl instead. The same lifting procedure applies
to the subtrees below vj , 1 ≤ j<l as well, since vj .jump
will also point to the new top v. In Fig. 7b, I(u2, v2) is
killed and I(u1, v1) is extended up to v2.
Finally, the subtree Tx below the split x of I(u, v) from
I(u1, v1) – deprived from the branches starting out on
I(u1, v1) – is also lifted to vl. See again Fig. 7. The reader
can easily verify that, from each seen vertex below v, the
number of jumps necessary to reach v is the same in Fig. 7b
as it is in Fig. 7a after processing the interval I(u, v). The
only exceptions are the abandoned split vertices and the
vertices vj ,j < l, but the role of these vertices has been
taken over by vl. Therefore, since the cost of processing a
number of non-overlapping groups is zero, we can simply
replace G by the modified graph for the rest of the algorithm
Figure 7. Resolving the overlap between intervals
dealing with the principal intervals that have not yet been
processed. It is on this basis that the two situations are
considered equivalent from the point of view of continuation.
Due to the fact that the first k interval groups are pairwise
non-overlapping, the DFT structure is not hurt by the lifting
of the overlapped groups, and the depth of the system of
groups has been reduced by l − 1.
Repeat the above process for each interval in the (k +
1)st group, and let Gk denote the DFT resulting from the
modifications involved. Graph Gk will have n − j interval
groups for some 0 ≤ j ≤ k out of which at least the first
k−j+1 are non-overlapping. (Observe that the old (k+1)st
group will no longer overlap the previous ones.) Meanwhile,
the total cost of processing the (k + 1)st group in G by
Algorithm 2 is not more than j + nk+1, where nk+1 is the
number of intervals in that group.
The conclusion of the argument above is that, for every
k<n:
T (k, n) ≤ max({j+nk+1+T (k−j+1, n−j) | 0 ≤ j ≤ k},
T (k + 1, n)).
From this formula we have:
T (1, n) ≤ j1 + nk1 + ... + jt + nkt ,
where 0 ≤ t<n and t
i=1 ji < n. Thus, the total number
of pointer assignments is O(|E(G)|), as stated. The proof
of Theorem 5.2 is now complete. ✷
VI. CONCLUSION
We have presented a linear-time algorithm to shrink
a graph G recursively along its 2-star subgraphs called
redexes. The starting point was a greedy algorithm, which
works in linear time only if G does not contain recursively
incurring (implied) redexes. Implied redexes have been detected and reduced during a single bottom-up sweep of the
depth-first tree of G, and the resulting graph was transferred
to the greedy algorithm to construct the desired graph r(G).
REFERENCES
[1] M. Bartha, M. Kr´esz, Structuring the elementary components
of graphs having a perfect internal matching, Theoretical
Computer Science 299 (2003), 179–210.
[2] M. Bartha, M. Kr´esz, Deterministic soliton graphs, Informatica 30 (2006), 281–288.
[3] M. Bartha, M. Kr´esz, Flexible matchings, in Proceedings of
the 32nd International Workshop on Graph-Theoretic Concepts in Computer Science, Bergen, Norway, June 22-24,
2006. Lecture Notes in Computer Science 4271, pp. 313 -
324.
[4] J. Cheriyan, A. Seb˝o, Z. Szigeti, Improving on the 1.5 approximation of a smallest 2-edge connected spanning subgraph,
SIAM J. Discret. Math, 14 (2001), 170–180.
[5] J. Dassow, H. J¨urgensen, Soliton automata, J. Comput. System
Sci. 40 (1990), 154–181.
[6] J. Edmonds, Paths, trees, and flowers, Canad. J. Math. 17
(1965), 449–467.
[7] Gabow, H. N., H. Kaplan, and R. E. Tarjan, Unique maximum
matching algorithms, Journal of Algorithms, 40 (2001), 159–
183.
[8] Golumbic, M. C., T. Hirst, and M. Lewenstein, Uniquely
restricted matchings, Algorithmica, 31 (2001), 139–154.
[9] M. M. Hossain, Some Graph Algorithms of Molecular
Switching Devices. Master’s thesis, Memorial University of
Newfoundland, Canada, 2007.
[10] D. E. Knuth, Semantics of context-free languages, Math.
Systems Theory 2 (1968), 127–145.
[11] L. Lov´asz, M. D. Plummer, Matching Theory, North Holland,
Amsterdam, 1986.
[12] J. Magun, Greedy Matching Algorithms, an Experimental
Study, J. of Experimental Algorithms 3 (1998), 1–13.